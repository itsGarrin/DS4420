{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:28.036855Z",
     "start_time": "2023-10-12T17:16:26.044515Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import MLutils as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "y = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:28.037104Z",
     "start_time": "2023-10-12T17:16:28.032451Z"
    }
   },
   "id": "8880148f815df2a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d09354f5fcbb3c74"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Hidden Layer: \n",
      "[[-4.51717921  3.97674842  2.76370784]\n",
      " [ 4.37042637 -4.68266456  1.4699174 ]\n",
      " [-0.86356733 -1.43901131 -1.11995057]\n",
      " [-0.18882909  4.80366357 -2.86219488]\n",
      " [-1.05866795 -1.64740541  4.90317215]\n",
      " [-1.52258544 -0.84531414 -0.95242475]\n",
      " [ 4.75092404  0.30864824 -3.96569433]\n",
      " [-1.0866874  -1.18964752 -1.3066696 ]]\n",
      "Output: \n",
      "[[9.10160024e-01 2.26332214e-07 1.48049002e-02 1.49732067e-02\n",
      "  3.05432949e-02 1.22501752e-01 1.02345515e-07 3.13275627e-02]\n",
      " [9.97955171e-07 9.11486870e-01 1.10988333e-01 2.59381264e-07\n",
      "  3.55344721e-02 1.68428175e-02 1.32213859e-02 5.45029518e-02]\n",
      " [2.53227732e-02 1.30252258e-01 2.45543459e-01 3.14714009e-02\n",
      "  6.09195411e-02 2.04281578e-01 9.44233123e-02 2.36574958e-01]\n",
      " [2.46019887e-02 7.02006940e-06 4.79178789e-02 9.21214225e-01\n",
      "  3.24917648e-06 1.03519882e-01 4.32412361e-02 8.91596296e-02]\n",
      " [3.58473301e-02 4.22209353e-02 9.07221542e-02 1.17559740e-05\n",
      "  9.19276130e-01 1.00119759e-01 1.29267256e-05 7.57871928e-02]\n",
      " [1.50125769e-01 1.90730943e-02 2.00217756e-01 8.79900476e-02\n",
      "  7.08809272e-02 2.45337002e-01 2.10287238e-02 2.20964430e-01]\n",
      " [4.73726475e-06 2.03573050e-02 9.99058667e-02 3.79001934e-02\n",
      "  1.42735599e-06 3.08305756e-02 9.15144235e-01 9.35487564e-02]\n",
      " [5.12175389e-02 6.99210698e-02 2.41370335e-01 7.39038087e-02\n",
      "  4.94583912e-02 2.29355042e-01 8.95303543e-02 2.46128226e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Implement the Neural Network\n",
    "nn = ml.NeuralNetwork()\n",
    "nn.train(X, y)\n",
    "nn_pred = nn.predict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:28.053615Z",
     "start_time": "2023-10-12T17:16:28.034196Z"
    }
   },
   "id": "1becf6db31023bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b2d0f6dd3250344"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 13:16:28.119135: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Input Data: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Predicted data based on trained weights: [[9.2626441e-01 1.7356340e-02 1.7617779e-02 2.4874823e-02 2.7059108e-02\n",
      "  1.4356785e-03 3.0300116e-02 8.7491184e-04]\n",
      " [4.9239728e-03 9.8689270e-01 2.1292002e-12 4.9195383e-25 8.2915910e-03\n",
      "  5.2049266e-21 5.4161162e-03 2.3729567e-13]\n",
      " [4.0700873e-03 6.7681272e-10 9.8667860e-01 8.1591737e-03 8.7431734e-03\n",
      "  8.0648604e-12 1.4066127e-19 4.0584240e-23]\n",
      " [1.5706982e-02 2.1165885e-11 1.2368001e-02 9.8257548e-01 2.1229911e-09\n",
      "  1.4497644e-02 5.5384855e-12 6.3432415e-10]\n",
      " [2.1151599e-02 8.5675893e-03 8.5377730e-03 4.9810248e-13 9.8488832e-01\n",
      "  5.2839160e-18 1.2340741e-12 2.9081499e-21]\n",
      " [3.1598647e-05 3.3188749e-17 1.0091705e-11 1.0618368e-02 5.3957106e-22\n",
      "  9.8367769e-01 1.4310019e-09 1.2561655e-02]\n",
      " [1.3148788e-02 1.1765053e-02 1.1711662e-13 3.0142485e-17 1.6328440e-09\n",
      "  2.2001326e-10 9.8125768e-01 1.3546146e-02]\n",
      " [1.6905398e-04 1.7509889e-11 1.5787457e-15 1.6110491e-10 9.5771650e-20\n",
      "  1.3570151e-02 1.4584284e-02 9.8317045e-01]]\n",
      "Weights: [<tf.Variable 'autoencoder_tf/dense/kernel:0' shape=(8, 3) dtype=float32, numpy=\n",
      "array([[ 2.3682055 , -1.1744769 , -2.473565  ],\n",
      "       [-1.8485548 , -3.351337  ,  4.432903  ],\n",
      "       [ 3.9116783 ,  3.9556327 ,  3.5873415 ],\n",
      "       [ 1.882246  ,  3.2162218 , -1.0895833 ],\n",
      "       [ 2.4759433 ,  0.16690359,  4.7523804 ],\n",
      "       [-2.75313   ,  4.1457524 , -2.5760953 ],\n",
      "       [-2.813749  , -2.581127  , -0.17882453],\n",
      "       [-4.2366853 ,  0.86100984, -2.369356  ]], dtype=float32)>, <tf.Variable 'autoencoder_tf/dense/bias:0' shape=(3,) dtype=float32, numpy=array([-2.6302123,  1.525018 ,  3.464939 ], dtype=float32)>, <tf.Variable 'autoencoder_tf/dense_1/kernel:0' shape=(3, 8) dtype=float32, numpy=\n",
      "array([[ 1.3313007 ,  1.6934805 ,  4.7086244 ,  3.8658087 ,  4.9697113 ,\n",
      "         0.23606317, -1.3069776 , -2.3311183 ],\n",
      "       [-1.1550947 , -4.728721  ,  0.5241554 ,  3.4792285 , -3.714915  ,\n",
      "         2.1314623 , -4.4819727 , -1.8410575 ],\n",
      "       [-0.6863247 ,  0.7536296 , -0.26903015, -4.1161013 ,  1.689336  ,\n",
      "        -4.998892  , -2.4636254 , -5.193168  ]], dtype=float32)>, <tf.Variable 'autoencoder_tf/dense_1/bias:0' shape=(8,) dtype=float32, numpy=\n",
      "array([ 3.9647963 , -2.682103  , -2.704409  ,  0.20514154, -2.652735  ,\n",
      "       -2.2742248 ,  0.20521796, -1.8575424 ], dtype=float32)>]\n",
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create an Autoencoder using Tensorflow\n",
    "autoencoder = ml.AutoencoderTF()\n",
    "autoencoder.train(X, y)\n",
    "ae_pred = autoencoder.test(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:32.767185Z",
     "start_time": "2023-10-12T17:16:28.052483Z"
    }
   },
   "id": "9f2f59b0676efa32"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Output: \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Autoencoder Output: \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the Autoencoder and the Neural Network using mean squared error (MSE)\n",
    "\n",
    "# use argmax to select the only maximum as 1 and convert the results to one-hot labels\n",
    "nn_pred_idx = np.argmax(nn_pred, axis=1)\n",
    "nn_pred_idx = np.eye(8)[nn_pred_idx]\n",
    "\n",
    "ae_pred_idx = np.argmax(ae_pred, axis=1)\n",
    "ae_pred_idx = np.eye(8)[ae_pred_idx]\n",
    "\n",
    "print(\"Neural Network Output: \\n\" + str(nn_pred_idx))\n",
    "print(\"Autoencoder Output: \\n\" + str(ae_pred_idx))\n",
    "\n",
    "accuracy = np.sum(nn_pred_idx == ae_pred_idx) / len(ae_pred_idx) / 8\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:32.783814Z",
     "start_time": "2023-10-12T17:16:32.765452Z"
    }
   },
   "id": "735da874ef6b998d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc899744cf7484b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "     1  14.23  1.71  2.43  15.6  127   2.8  3.06   .28  2.29  5.64  1.04  \\\n0    2  11.65  1.67  2.62  26.0   88  1.92  1.61  0.40  1.34  2.60  1.36   \n1    2  11.41  0.74  2.50  21.0   88  2.48  2.01  0.42  1.44  3.08  1.10   \n2    3  13.78  2.76  2.30  22.0   90  1.35  0.68  0.41  1.03  9.58  0.70   \n3    1  13.30  1.72  2.14  17.0   94  2.40  2.19  0.27  1.35  3.95  1.02   \n4    1  13.29  1.97  2.68  16.8  102  3.00  3.23  0.31  1.66  6.00  1.07   \n..  ..    ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   \n145  2  12.51  1.73  1.98  20.5   85  2.20  1.92  0.32  1.48  2.94  1.04   \n146  2  13.34  0.94  2.36  17.0  110  2.53  1.30  0.55  0.42  3.17  1.02   \n147  2  12.77  3.43  1.98  16.0   80  1.63  1.25  0.43  0.83  3.40  0.70   \n148  1  14.10  2.02  2.40  18.8  103  2.75  2.92  0.32  2.38  6.20  1.07   \n149  3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06  7.70  0.64   \n\n     3.92  1065  \n0    3.21   562  \n1    2.31   434  \n2    1.68   615  \n3    2.77  1285  \n4    2.84  1270  \n..    ...   ...  \n145  3.57   672  \n146  1.93   750  \n147  2.12   372  \n148  2.75  1060  \n149  1.74   740  \n\n[150 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>11.65</td>\n      <td>1.67</td>\n      <td>2.62</td>\n      <td>26.0</td>\n      <td>88</td>\n      <td>1.92</td>\n      <td>1.61</td>\n      <td>0.40</td>\n      <td>1.34</td>\n      <td>2.60</td>\n      <td>1.36</td>\n      <td>3.21</td>\n      <td>562</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>11.41</td>\n      <td>0.74</td>\n      <td>2.50</td>\n      <td>21.0</td>\n      <td>88</td>\n      <td>2.48</td>\n      <td>2.01</td>\n      <td>0.42</td>\n      <td>1.44</td>\n      <td>3.08</td>\n      <td>1.10</td>\n      <td>2.31</td>\n      <td>434</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>13.78</td>\n      <td>2.76</td>\n      <td>2.30</td>\n      <td>22.0</td>\n      <td>90</td>\n      <td>1.35</td>\n      <td>0.68</td>\n      <td>0.41</td>\n      <td>1.03</td>\n      <td>9.58</td>\n      <td>0.70</td>\n      <td>1.68</td>\n      <td>615</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>13.30</td>\n      <td>1.72</td>\n      <td>2.14</td>\n      <td>17.0</td>\n      <td>94</td>\n      <td>2.40</td>\n      <td>2.19</td>\n      <td>0.27</td>\n      <td>1.35</td>\n      <td>3.95</td>\n      <td>1.02</td>\n      <td>2.77</td>\n      <td>1285</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.29</td>\n      <td>1.97</td>\n      <td>2.68</td>\n      <td>16.8</td>\n      <td>102</td>\n      <td>3.00</td>\n      <td>3.23</td>\n      <td>0.31</td>\n      <td>1.66</td>\n      <td>6.00</td>\n      <td>1.07</td>\n      <td>2.84</td>\n      <td>1270</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2</td>\n      <td>12.51</td>\n      <td>1.73</td>\n      <td>1.98</td>\n      <td>20.5</td>\n      <td>85</td>\n      <td>2.20</td>\n      <td>1.92</td>\n      <td>0.32</td>\n      <td>1.48</td>\n      <td>2.94</td>\n      <td>1.04</td>\n      <td>3.57</td>\n      <td>672</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>2</td>\n      <td>13.34</td>\n      <td>0.94</td>\n      <td>2.36</td>\n      <td>17.0</td>\n      <td>110</td>\n      <td>2.53</td>\n      <td>1.30</td>\n      <td>0.55</td>\n      <td>0.42</td>\n      <td>3.17</td>\n      <td>1.02</td>\n      <td>1.93</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2</td>\n      <td>12.77</td>\n      <td>3.43</td>\n      <td>1.98</td>\n      <td>16.0</td>\n      <td>80</td>\n      <td>1.63</td>\n      <td>1.25</td>\n      <td>0.43</td>\n      <td>0.83</td>\n      <td>3.40</td>\n      <td>0.70</td>\n      <td>2.12</td>\n      <td>372</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>1</td>\n      <td>14.10</td>\n      <td>2.02</td>\n      <td>2.40</td>\n      <td>18.8</td>\n      <td>103</td>\n      <td>2.75</td>\n      <td>2.92</td>\n      <td>0.32</td>\n      <td>2.38</td>\n      <td>6.20</td>\n      <td>1.07</td>\n      <td>2.75</td>\n      <td>1060</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>3</td>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    1  14.23  1.71  2.43  15.6  127   2.8  3.06   .28  2.29   5.64  1.04  \\\n0   1  13.56  1.73  2.46  20.5  116  2.96  2.78  0.20  2.45   6.25  0.98   \n1   3  12.79  2.67  2.48  22.0  112  1.48  1.36  0.24  1.26  10.80  0.48   \n2   2  12.17  1.45  2.53  19.0  104  1.89  1.75  0.45  1.03   2.95  1.45   \n3   1  14.22  1.70  2.30  16.3  118  3.20  3.00  0.26  2.03   6.38  0.94   \n4   2  11.87  4.31  2.39  21.0   82  2.86  3.03  0.21  2.91   2.80  0.75   \n5   2  12.42  4.43  2.73  26.5  102  2.20  2.13  0.43  1.71   2.08  0.92   \n6   1  14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98   5.20  1.08   \n7   3  12.36  3.83  2.38  21.0   88  2.30  0.92  0.50  1.04   7.65  0.56   \n8   2  12.72  1.75  2.28  22.5   84  1.38  1.76  0.48  1.63   3.30  0.88   \n9   2  12.00  1.51  2.42  22.0   86  1.45  1.25  0.50  1.63   3.60  1.05   \n10  3  13.45  3.70  2.60  23.0  111  1.70  0.92  0.43  1.46  10.68  0.85   \n11  3  13.88  5.04  2.23  20.0   80  0.98  0.34  0.40  0.68   4.90  0.58   \n12  1  14.39  1.87  2.45  14.6   96  2.50  2.52  0.30  1.98   5.25  1.02   \n13  2  12.29  3.17  2.21  18.0   88  2.85  2.99  0.45  2.81   2.30  1.42   \n14  3  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54   8.66  0.74   \n15  2  12.72  1.81  2.20  18.8   86  2.20  2.53  0.26  1.77   3.90  1.16   \n16  1  13.51  1.80  2.65  19.0  110  2.35  2.53  0.29  1.54   4.20  1.10   \n17  1  13.64  3.10  2.56  15.2  116  2.70  3.03  0.17  1.66   5.10  0.96   \n18  2  13.67  1.25  1.92  18.0   94  2.10  1.79  0.32  0.73   3.80  1.23   \n19  1  13.82  1.75  2.42  14.0  111  3.88  3.74  0.32  1.87   7.05  1.01   \n20  2  12.37  1.17  1.92  19.6   78  2.11  2.00  0.27  1.04   4.68  1.12   \n21  1  13.07  1.50  2.10  15.5   98  2.40  2.64  0.28  1.37   3.70  1.18   \n22  1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97   6.75  1.05   \n23  2  12.42  2.55  2.27  22.0   90  1.68  1.84  0.66  1.42   2.70  0.86   \n24  2  12.64  1.36  2.02  16.8  100  2.02  1.41  0.53  0.62   5.75  0.98   \n25  2  11.76  2.68  2.92  20.0  103  1.75  2.03  0.60  1.05   3.80  1.23   \n26  2  11.79  2.13  2.78  28.5   92  2.13  2.24  0.58  1.76   3.00  0.97   \n\n    3.92  1065  \n0   3.03  1120  \n1   1.47   480  \n2   2.23   355  \n3   3.31   970  \n4   3.64   380  \n5   3.12   365  \n6   2.85  1045  \n7   1.58   520  \n8   2.42   488  \n9   2.65   450  \n10  1.56   695  \n11  1.33   415  \n12  3.58  1290  \n13  2.83   406  \n14  1.80   750  \n15  3.14   714  \n16  2.87  1095  \n17  3.36   845  \n18  2.46   630  \n19  3.26  1190  \n20  3.48   510  \n21  2.69  1020  \n22  2.85  1450  \n23  3.30   315  \n24  1.59   450  \n25  2.50   607  \n26  2.44   466  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>13.56</td>\n      <td>1.73</td>\n      <td>2.46</td>\n      <td>20.5</td>\n      <td>116</td>\n      <td>2.96</td>\n      <td>2.78</td>\n      <td>0.20</td>\n      <td>2.45</td>\n      <td>6.25</td>\n      <td>0.98</td>\n      <td>3.03</td>\n      <td>1120</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>12.79</td>\n      <td>2.67</td>\n      <td>2.48</td>\n      <td>22.0</td>\n      <td>112</td>\n      <td>1.48</td>\n      <td>1.36</td>\n      <td>0.24</td>\n      <td>1.26</td>\n      <td>10.80</td>\n      <td>0.48</td>\n      <td>1.47</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>12.17</td>\n      <td>1.45</td>\n      <td>2.53</td>\n      <td>19.0</td>\n      <td>104</td>\n      <td>1.89</td>\n      <td>1.75</td>\n      <td>0.45</td>\n      <td>1.03</td>\n      <td>2.95</td>\n      <td>1.45</td>\n      <td>2.23</td>\n      <td>355</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.22</td>\n      <td>1.70</td>\n      <td>2.30</td>\n      <td>16.3</td>\n      <td>118</td>\n      <td>3.20</td>\n      <td>3.00</td>\n      <td>0.26</td>\n      <td>2.03</td>\n      <td>6.38</td>\n      <td>0.94</td>\n      <td>3.31</td>\n      <td>970</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>11.87</td>\n      <td>4.31</td>\n      <td>2.39</td>\n      <td>21.0</td>\n      <td>82</td>\n      <td>2.86</td>\n      <td>3.03</td>\n      <td>0.21</td>\n      <td>2.91</td>\n      <td>2.80</td>\n      <td>0.75</td>\n      <td>3.64</td>\n      <td>380</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>12.42</td>\n      <td>4.43</td>\n      <td>2.73</td>\n      <td>26.5</td>\n      <td>102</td>\n      <td>2.20</td>\n      <td>2.13</td>\n      <td>0.43</td>\n      <td>1.71</td>\n      <td>2.08</td>\n      <td>0.92</td>\n      <td>3.12</td>\n      <td>365</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>14.83</td>\n      <td>1.64</td>\n      <td>2.17</td>\n      <td>14.0</td>\n      <td>97</td>\n      <td>2.80</td>\n      <td>2.98</td>\n      <td>0.29</td>\n      <td>1.98</td>\n      <td>5.20</td>\n      <td>1.08</td>\n      <td>2.85</td>\n      <td>1045</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>12.36</td>\n      <td>3.83</td>\n      <td>2.38</td>\n      <td>21.0</td>\n      <td>88</td>\n      <td>2.30</td>\n      <td>0.92</td>\n      <td>0.50</td>\n      <td>1.04</td>\n      <td>7.65</td>\n      <td>0.56</td>\n      <td>1.58</td>\n      <td>520</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>12.72</td>\n      <td>1.75</td>\n      <td>2.28</td>\n      <td>22.5</td>\n      <td>84</td>\n      <td>1.38</td>\n      <td>1.76</td>\n      <td>0.48</td>\n      <td>1.63</td>\n      <td>3.30</td>\n      <td>0.88</td>\n      <td>2.42</td>\n      <td>488</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>12.00</td>\n      <td>1.51</td>\n      <td>2.42</td>\n      <td>22.0</td>\n      <td>86</td>\n      <td>1.45</td>\n      <td>1.25</td>\n      <td>0.50</td>\n      <td>1.63</td>\n      <td>3.60</td>\n      <td>1.05</td>\n      <td>2.65</td>\n      <td>450</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>13.45</td>\n      <td>3.70</td>\n      <td>2.60</td>\n      <td>23.0</td>\n      <td>111</td>\n      <td>1.70</td>\n      <td>0.92</td>\n      <td>0.43</td>\n      <td>1.46</td>\n      <td>10.68</td>\n      <td>0.85</td>\n      <td>1.56</td>\n      <td>695</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>13.88</td>\n      <td>5.04</td>\n      <td>2.23</td>\n      <td>20.0</td>\n      <td>80</td>\n      <td>0.98</td>\n      <td>0.34</td>\n      <td>0.40</td>\n      <td>0.68</td>\n      <td>4.90</td>\n      <td>0.58</td>\n      <td>1.33</td>\n      <td>415</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>14.39</td>\n      <td>1.87</td>\n      <td>2.45</td>\n      <td>14.6</td>\n      <td>96</td>\n      <td>2.50</td>\n      <td>2.52</td>\n      <td>0.30</td>\n      <td>1.98</td>\n      <td>5.25</td>\n      <td>1.02</td>\n      <td>3.58</td>\n      <td>1290</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>12.29</td>\n      <td>3.17</td>\n      <td>2.21</td>\n      <td>18.0</td>\n      <td>88</td>\n      <td>2.85</td>\n      <td>2.99</td>\n      <td>0.45</td>\n      <td>2.81</td>\n      <td>2.30</td>\n      <td>1.42</td>\n      <td>2.83</td>\n      <td>406</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>13.58</td>\n      <td>2.58</td>\n      <td>2.69</td>\n      <td>24.5</td>\n      <td>105</td>\n      <td>1.55</td>\n      <td>0.84</td>\n      <td>0.39</td>\n      <td>1.54</td>\n      <td>8.66</td>\n      <td>0.74</td>\n      <td>1.80</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n      <td>12.72</td>\n      <td>1.81</td>\n      <td>2.20</td>\n      <td>18.8</td>\n      <td>86</td>\n      <td>2.20</td>\n      <td>2.53</td>\n      <td>0.26</td>\n      <td>1.77</td>\n      <td>3.90</td>\n      <td>1.16</td>\n      <td>3.14</td>\n      <td>714</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>13.51</td>\n      <td>1.80</td>\n      <td>2.65</td>\n      <td>19.0</td>\n      <td>110</td>\n      <td>2.35</td>\n      <td>2.53</td>\n      <td>0.29</td>\n      <td>1.54</td>\n      <td>4.20</td>\n      <td>1.10</td>\n      <td>2.87</td>\n      <td>1095</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>13.64</td>\n      <td>3.10</td>\n      <td>2.56</td>\n      <td>15.2</td>\n      <td>116</td>\n      <td>2.70</td>\n      <td>3.03</td>\n      <td>0.17</td>\n      <td>1.66</td>\n      <td>5.10</td>\n      <td>0.96</td>\n      <td>3.36</td>\n      <td>845</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>13.67</td>\n      <td>1.25</td>\n      <td>1.92</td>\n      <td>18.0</td>\n      <td>94</td>\n      <td>2.10</td>\n      <td>1.79</td>\n      <td>0.32</td>\n      <td>0.73</td>\n      <td>3.80</td>\n      <td>1.23</td>\n      <td>2.46</td>\n      <td>630</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>13.82</td>\n      <td>1.75</td>\n      <td>2.42</td>\n      <td>14.0</td>\n      <td>111</td>\n      <td>3.88</td>\n      <td>3.74</td>\n      <td>0.32</td>\n      <td>1.87</td>\n      <td>7.05</td>\n      <td>1.01</td>\n      <td>3.26</td>\n      <td>1190</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2</td>\n      <td>12.37</td>\n      <td>1.17</td>\n      <td>1.92</td>\n      <td>19.6</td>\n      <td>78</td>\n      <td>2.11</td>\n      <td>2.00</td>\n      <td>0.27</td>\n      <td>1.04</td>\n      <td>4.68</td>\n      <td>1.12</td>\n      <td>3.48</td>\n      <td>510</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n      <td>13.07</td>\n      <td>1.50</td>\n      <td>2.10</td>\n      <td>15.5</td>\n      <td>98</td>\n      <td>2.40</td>\n      <td>2.64</td>\n      <td>0.28</td>\n      <td>1.37</td>\n      <td>3.70</td>\n      <td>1.18</td>\n      <td>2.69</td>\n      <td>1020</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1</td>\n      <td>14.20</td>\n      <td>1.76</td>\n      <td>2.45</td>\n      <td>15.2</td>\n      <td>112</td>\n      <td>3.27</td>\n      <td>3.39</td>\n      <td>0.34</td>\n      <td>1.97</td>\n      <td>6.75</td>\n      <td>1.05</td>\n      <td>2.85</td>\n      <td>1450</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n      <td>12.42</td>\n      <td>2.55</td>\n      <td>2.27</td>\n      <td>22.0</td>\n      <td>90</td>\n      <td>1.68</td>\n      <td>1.84</td>\n      <td>0.66</td>\n      <td>1.42</td>\n      <td>2.70</td>\n      <td>0.86</td>\n      <td>3.30</td>\n      <td>315</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2</td>\n      <td>12.64</td>\n      <td>1.36</td>\n      <td>2.02</td>\n      <td>16.8</td>\n      <td>100</td>\n      <td>2.02</td>\n      <td>1.41</td>\n      <td>0.53</td>\n      <td>0.62</td>\n      <td>5.75</td>\n      <td>0.98</td>\n      <td>1.59</td>\n      <td>450</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2</td>\n      <td>11.76</td>\n      <td>2.68</td>\n      <td>2.92</td>\n      <td>20.0</td>\n      <td>103</td>\n      <td>1.75</td>\n      <td>2.03</td>\n      <td>0.60</td>\n      <td>1.05</td>\n      <td>3.80</td>\n      <td>1.23</td>\n      <td>2.50</td>\n      <td>607</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2</td>\n      <td>11.79</td>\n      <td>2.13</td>\n      <td>2.78</td>\n      <td>28.5</td>\n      <td>92</td>\n      <td>2.13</td>\n      <td>2.24</td>\n      <td>0.58</td>\n      <td>1.76</td>\n      <td>3.00</td>\n      <td>0.97</td>\n      <td>2.44</td>\n      <td>466</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the wine dataset\n",
    "train_wine = pd.read_csv('train_wine.csv')\n",
    "test_wine = pd.read_csv('test_wine.csv')\n",
    "\n",
    "display(train_wine)\n",
    "display(test_wine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:32.790Z",
     "start_time": "2023-10-12T17:16:32.767501Z"
    }
   },
   "id": "37196ef4493105a9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "     1     14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n0    2 -1.670125 -0.586254  0.923595  1.981185 -0.810781 -0.612897 -0.384151   \n1    2 -1.970387 -1.408850  0.494238  0.454064 -0.810781  0.289686  0.007108   \n2    3  0.994702  0.377863 -0.221358  0.759488 -0.673205 -1.531598 -1.293826   \n3    1  0.394178 -0.542029 -0.793834 -0.767633 -0.398053  0.160746  0.183174   \n4    1  0.381667 -0.320901  1.138274 -0.828718  0.152251  1.127799  1.200446   \n..  ..       ...       ...       ...       ...       ...       ...       ...   \n145  2 -0.594185 -0.533184 -1.366310  0.301352 -1.017145 -0.161605 -0.080925   \n146  2  0.444221 -1.231948 -0.006679 -0.767633  0.702554  0.370274 -0.687376   \n147  2 -0.268901  0.970486 -1.366310 -1.073057 -1.361085 -1.080306 -0.736283   \n148  1  1.395051 -0.276675  0.136440 -0.217869  0.221039  0.724860  0.897221   \n149  3  0.907125  2.934102  0.315339  0.301352 -0.329265 -0.999718 -1.362297   \n\n          .28      2.29      5.64      1.04      3.92      1065  \n0    0.324537 -0.430475 -1.052111  1.781424  0.864654 -0.608422  \n1    0.486267 -0.255722 -0.845966  0.645535 -0.405789 -1.017432  \n2    0.405402 -0.972209  1.945577 -1.101987 -1.295098 -0.439067  \n3   -0.726705 -0.413000 -0.472329  0.296030  0.243549  1.701840  \n4   -0.403246  0.128735  0.408081  0.514470  0.342361  1.653909  \n..        ...       ...       ...       ...       ...       ...  \n145 -0.322381 -0.185821 -0.906092  0.383406  1.372831 -0.256930  \n146  1.537509 -2.038203 -0.807314  0.296030 -0.942198 -0.007690  \n147  0.567132 -1.321715 -0.708537 -1.101987 -0.673993 -1.215545  \n148 -0.322381  1.386956  0.493974  0.514470  0.215316  0.982879  \n149  1.294914 -0.919783  1.138177 -1.364116 -1.210402 -0.039644  \n\n[150 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>-1.670125</td>\n      <td>-0.586254</td>\n      <td>0.923595</td>\n      <td>1.981185</td>\n      <td>-0.810781</td>\n      <td>-0.612897</td>\n      <td>-0.384151</td>\n      <td>0.324537</td>\n      <td>-0.430475</td>\n      <td>-1.052111</td>\n      <td>1.781424</td>\n      <td>0.864654</td>\n      <td>-0.608422</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>-1.970387</td>\n      <td>-1.408850</td>\n      <td>0.494238</td>\n      <td>0.454064</td>\n      <td>-0.810781</td>\n      <td>0.289686</td>\n      <td>0.007108</td>\n      <td>0.486267</td>\n      <td>-0.255722</td>\n      <td>-0.845966</td>\n      <td>0.645535</td>\n      <td>-0.405789</td>\n      <td>-1.017432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.994702</td>\n      <td>0.377863</td>\n      <td>-0.221358</td>\n      <td>0.759488</td>\n      <td>-0.673205</td>\n      <td>-1.531598</td>\n      <td>-1.293826</td>\n      <td>0.405402</td>\n      <td>-0.972209</td>\n      <td>1.945577</td>\n      <td>-1.101987</td>\n      <td>-1.295098</td>\n      <td>-0.439067</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.394178</td>\n      <td>-0.542029</td>\n      <td>-0.793834</td>\n      <td>-0.767633</td>\n      <td>-0.398053</td>\n      <td>0.160746</td>\n      <td>0.183174</td>\n      <td>-0.726705</td>\n      <td>-0.413000</td>\n      <td>-0.472329</td>\n      <td>0.296030</td>\n      <td>0.243549</td>\n      <td>1.701840</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.381667</td>\n      <td>-0.320901</td>\n      <td>1.138274</td>\n      <td>-0.828718</td>\n      <td>0.152251</td>\n      <td>1.127799</td>\n      <td>1.200446</td>\n      <td>-0.403246</td>\n      <td>0.128735</td>\n      <td>0.408081</td>\n      <td>0.514470</td>\n      <td>0.342361</td>\n      <td>1.653909</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2</td>\n      <td>-0.594185</td>\n      <td>-0.533184</td>\n      <td>-1.366310</td>\n      <td>0.301352</td>\n      <td>-1.017145</td>\n      <td>-0.161605</td>\n      <td>-0.080925</td>\n      <td>-0.322381</td>\n      <td>-0.185821</td>\n      <td>-0.906092</td>\n      <td>0.383406</td>\n      <td>1.372831</td>\n      <td>-0.256930</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>2</td>\n      <td>0.444221</td>\n      <td>-1.231948</td>\n      <td>-0.006679</td>\n      <td>-0.767633</td>\n      <td>0.702554</td>\n      <td>0.370274</td>\n      <td>-0.687376</td>\n      <td>1.537509</td>\n      <td>-2.038203</td>\n      <td>-0.807314</td>\n      <td>0.296030</td>\n      <td>-0.942198</td>\n      <td>-0.007690</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2</td>\n      <td>-0.268901</td>\n      <td>0.970486</td>\n      <td>-1.366310</td>\n      <td>-1.073057</td>\n      <td>-1.361085</td>\n      <td>-1.080306</td>\n      <td>-0.736283</td>\n      <td>0.567132</td>\n      <td>-1.321715</td>\n      <td>-0.708537</td>\n      <td>-1.101987</td>\n      <td>-0.673993</td>\n      <td>-1.215545</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>1</td>\n      <td>1.395051</td>\n      <td>-0.276675</td>\n      <td>0.136440</td>\n      <td>-0.217869</td>\n      <td>0.221039</td>\n      <td>0.724860</td>\n      <td>0.897221</td>\n      <td>-0.322381</td>\n      <td>1.386956</td>\n      <td>0.493974</td>\n      <td>0.514470</td>\n      <td>0.215316</td>\n      <td>0.982879</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>3</td>\n      <td>0.907125</td>\n      <td>2.934102</td>\n      <td>0.315339</td>\n      <td>0.301352</td>\n      <td>-0.329265</td>\n      <td>-0.999718</td>\n      <td>-1.362297</td>\n      <td>1.294914</td>\n      <td>-0.919783</td>\n      <td>1.138177</td>\n      <td>-1.364116</td>\n      <td>-1.210402</td>\n      <td>-0.039644</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    1     14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n0   1  0.719462 -0.533184  0.351119  0.301352  1.115282  1.063329  0.760280   \n1   3 -0.243880  0.298257  0.422678  0.759488  0.840130 -1.322069 -0.628687   \n2   2 -1.019557 -0.780847  0.601577 -0.156784  0.289827 -0.661250 -0.247210   \n3   1  1.545182 -0.559719 -0.221358 -0.981430  1.252858  1.450150  0.975472   \n4   2 -1.394885  1.748856  0.100660  0.454064 -1.223509  0.902153  1.004817   \n5   2 -0.706784  1.854998  1.317173  2.133897  0.152251 -0.161605  0.124485   \n6   1  2.308349 -0.612790 -0.686495 -1.683906 -0.191689  0.805448  0.955909   \n7   3 -0.781849  1.324291  0.064881  0.454064 -0.810781 -0.000430 -1.059071   \n8   2 -0.331456 -0.515493 -0.292917  0.912200 -1.085933 -1.483245 -0.237429   \n9   2 -1.232243 -0.727776  0.208000  0.759488 -0.948357 -1.370422 -0.736283   \n10  3  0.581841  1.209304  0.852036  1.064913  0.771342 -0.967483 -1.059071   \n11  3  1.119811  2.394550 -0.471816  0.148640 -1.361085 -2.127947 -1.626396   \n12  1  1.757868 -0.409352  0.315339 -1.500651 -0.260477  0.321921  0.505962   \n13  2 -0.869426  0.740513 -0.543375 -0.462209 -0.810781  0.886036  0.965691   \n14  3  0.744483  0.218651  1.174054  1.523049  0.358615 -1.209247 -1.137323   \n15  2 -0.331456 -0.462423 -0.579155 -0.217869 -0.948357 -0.161605  0.515744   \n16  1  0.656907 -0.471268  1.030935 -0.156784  0.702554  0.080158  0.515744   \n17  1  0.819549  0.678597  0.708917 -1.317397  1.115282  0.644272  1.004817   \n18  2  0.857082 -0.957749 -1.580989 -0.462209 -0.398053 -0.322781 -0.208084   \n19  1  1.044746 -0.515493  0.208000 -1.683906  0.771342  2.546144  1.699300   \n20  2 -0.769338 -1.028510 -1.580989  0.026470 -1.498661 -0.306663 -0.002674   \n21  1  0.106426 -0.736621 -0.936953 -1.225769 -0.122901  0.160746  0.623340   \n22  1  1.520161 -0.506648  0.315339 -1.317397  0.840130  1.562973  1.356949   \n23  2 -0.706784  0.192116 -0.328697  0.759488 -0.673205 -0.999718 -0.159177   \n24  2 -0.431543 -0.860453 -1.223191 -0.828718  0.014675 -0.451721 -0.579780   \n25  2 -1.532505  0.307102  1.996988  0.148640  0.221039 -0.886895  0.026671   \n26  2 -1.494972 -0.179379  1.496072  2.744746 -0.535629 -0.274428  0.232081   \n\n         .28      2.29      5.64      1.04      3.92      1065  \n0  -1.292758  1.509284  0.515448  0.121278  0.610565  1.174602  \n1  -0.969299 -0.570277  2.469528 -2.063124 -1.591535 -0.870444  \n2   0.728861 -0.972209 -0.901797  2.174616 -0.518717 -1.269867  \n3  -0.807569  0.775321  0.571279 -0.053474  1.005814  0.695294  \n4  -1.211893  2.313147 -0.966218 -0.883547  1.471643 -1.189982  \n5   0.567132  0.216111 -1.275435 -0.140850  0.737609 -1.237913  \n6  -0.564975  0.687944  0.064506  0.558158  0.356477  0.934948  \n7   1.133185 -0.954734  1.116703 -1.713620 -1.436259 -0.742629  \n8   0.971455  0.076309 -0.751483 -0.315602 -0.250512 -0.844881  \n9   1.133185  0.076309 -0.622643  0.427094  0.074156 -0.966305  \n10  0.567132 -0.220771  2.417992 -0.446667 -1.464491 -0.183436  \n11  0.324537 -1.583845 -0.064334 -1.626244 -1.789159 -1.078144  \n12 -0.484110  0.687944  0.085980  0.296030  1.386947  1.717817  \n13  0.728861  2.138394 -1.180952  2.043552  0.328245 -1.106902  \n14  0.243672 -0.080969  1.550466 -0.927235 -1.125706 -0.007690  \n15 -0.807569  0.320963 -0.493803  0.907663  0.765842 -0.122724  \n16 -0.564975 -0.080969 -0.364962  0.645535  0.384709  1.094717  \n17 -1.535352  0.128735  0.021559  0.033902  1.076394  0.295871  \n18 -0.322381 -1.496468 -0.536749  1.213479 -0.194048 -0.391136  \n19 -0.322381  0.495716  0.859022  0.252342  0.935234  1.398279  \n20 -0.726705 -0.954734 -0.158817  0.732911  1.245786 -0.774582  \n21 -0.645840 -0.378049 -0.579696  0.995039  0.130620  0.855063  \n22 -0.160651  0.670469  0.730182  0.427094  0.356477  2.229079  \n23  2.427021 -0.290673 -1.009164 -0.402979  0.991698 -1.397682  \n24  1.375779 -1.688697  0.300714  0.121278 -1.422143 -0.966305  \n25  1.941833 -0.937259 -0.536749  1.213479 -0.137584 -0.464630  \n26  1.780103  0.303488 -0.880324  0.077590 -0.222280 -0.915179  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.719462</td>\n      <td>-0.533184</td>\n      <td>0.351119</td>\n      <td>0.301352</td>\n      <td>1.115282</td>\n      <td>1.063329</td>\n      <td>0.760280</td>\n      <td>-1.292758</td>\n      <td>1.509284</td>\n      <td>0.515448</td>\n      <td>0.121278</td>\n      <td>0.610565</td>\n      <td>1.174602</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>-0.243880</td>\n      <td>0.298257</td>\n      <td>0.422678</td>\n      <td>0.759488</td>\n      <td>0.840130</td>\n      <td>-1.322069</td>\n      <td>-0.628687</td>\n      <td>-0.969299</td>\n      <td>-0.570277</td>\n      <td>2.469528</td>\n      <td>-2.063124</td>\n      <td>-1.591535</td>\n      <td>-0.870444</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-1.019557</td>\n      <td>-0.780847</td>\n      <td>0.601577</td>\n      <td>-0.156784</td>\n      <td>0.289827</td>\n      <td>-0.661250</td>\n      <td>-0.247210</td>\n      <td>0.728861</td>\n      <td>-0.972209</td>\n      <td>-0.901797</td>\n      <td>2.174616</td>\n      <td>-0.518717</td>\n      <td>-1.269867</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1.545182</td>\n      <td>-0.559719</td>\n      <td>-0.221358</td>\n      <td>-0.981430</td>\n      <td>1.252858</td>\n      <td>1.450150</td>\n      <td>0.975472</td>\n      <td>-0.807569</td>\n      <td>0.775321</td>\n      <td>0.571279</td>\n      <td>-0.053474</td>\n      <td>1.005814</td>\n      <td>0.695294</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>-1.394885</td>\n      <td>1.748856</td>\n      <td>0.100660</td>\n      <td>0.454064</td>\n      <td>-1.223509</td>\n      <td>0.902153</td>\n      <td>1.004817</td>\n      <td>-1.211893</td>\n      <td>2.313147</td>\n      <td>-0.966218</td>\n      <td>-0.883547</td>\n      <td>1.471643</td>\n      <td>-1.189982</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>-0.706784</td>\n      <td>1.854998</td>\n      <td>1.317173</td>\n      <td>2.133897</td>\n      <td>0.152251</td>\n      <td>-0.161605</td>\n      <td>0.124485</td>\n      <td>0.567132</td>\n      <td>0.216111</td>\n      <td>-1.275435</td>\n      <td>-0.140850</td>\n      <td>0.737609</td>\n      <td>-1.237913</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>2.308349</td>\n      <td>-0.612790</td>\n      <td>-0.686495</td>\n      <td>-1.683906</td>\n      <td>-0.191689</td>\n      <td>0.805448</td>\n      <td>0.955909</td>\n      <td>-0.564975</td>\n      <td>0.687944</td>\n      <td>0.064506</td>\n      <td>0.558158</td>\n      <td>0.356477</td>\n      <td>0.934948</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>-0.781849</td>\n      <td>1.324291</td>\n      <td>0.064881</td>\n      <td>0.454064</td>\n      <td>-0.810781</td>\n      <td>-0.000430</td>\n      <td>-1.059071</td>\n      <td>1.133185</td>\n      <td>-0.954734</td>\n      <td>1.116703</td>\n      <td>-1.713620</td>\n      <td>-1.436259</td>\n      <td>-0.742629</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>-0.331456</td>\n      <td>-0.515493</td>\n      <td>-0.292917</td>\n      <td>0.912200</td>\n      <td>-1.085933</td>\n      <td>-1.483245</td>\n      <td>-0.237429</td>\n      <td>0.971455</td>\n      <td>0.076309</td>\n      <td>-0.751483</td>\n      <td>-0.315602</td>\n      <td>-0.250512</td>\n      <td>-0.844881</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>-1.232243</td>\n      <td>-0.727776</td>\n      <td>0.208000</td>\n      <td>0.759488</td>\n      <td>-0.948357</td>\n      <td>-1.370422</td>\n      <td>-0.736283</td>\n      <td>1.133185</td>\n      <td>0.076309</td>\n      <td>-0.622643</td>\n      <td>0.427094</td>\n      <td>0.074156</td>\n      <td>-0.966305</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>0.581841</td>\n      <td>1.209304</td>\n      <td>0.852036</td>\n      <td>1.064913</td>\n      <td>0.771342</td>\n      <td>-0.967483</td>\n      <td>-1.059071</td>\n      <td>0.567132</td>\n      <td>-0.220771</td>\n      <td>2.417992</td>\n      <td>-0.446667</td>\n      <td>-1.464491</td>\n      <td>-0.183436</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>1.119811</td>\n      <td>2.394550</td>\n      <td>-0.471816</td>\n      <td>0.148640</td>\n      <td>-1.361085</td>\n      <td>-2.127947</td>\n      <td>-1.626396</td>\n      <td>0.324537</td>\n      <td>-1.583845</td>\n      <td>-0.064334</td>\n      <td>-1.626244</td>\n      <td>-1.789159</td>\n      <td>-1.078144</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>1.757868</td>\n      <td>-0.409352</td>\n      <td>0.315339</td>\n      <td>-1.500651</td>\n      <td>-0.260477</td>\n      <td>0.321921</td>\n      <td>0.505962</td>\n      <td>-0.484110</td>\n      <td>0.687944</td>\n      <td>0.085980</td>\n      <td>0.296030</td>\n      <td>1.386947</td>\n      <td>1.717817</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>-0.869426</td>\n      <td>0.740513</td>\n      <td>-0.543375</td>\n      <td>-0.462209</td>\n      <td>-0.810781</td>\n      <td>0.886036</td>\n      <td>0.965691</td>\n      <td>0.728861</td>\n      <td>2.138394</td>\n      <td>-1.180952</td>\n      <td>2.043552</td>\n      <td>0.328245</td>\n      <td>-1.106902</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>0.744483</td>\n      <td>0.218651</td>\n      <td>1.174054</td>\n      <td>1.523049</td>\n      <td>0.358615</td>\n      <td>-1.209247</td>\n      <td>-1.137323</td>\n      <td>0.243672</td>\n      <td>-0.080969</td>\n      <td>1.550466</td>\n      <td>-0.927235</td>\n      <td>-1.125706</td>\n      <td>-0.007690</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n      <td>-0.331456</td>\n      <td>-0.462423</td>\n      <td>-0.579155</td>\n      <td>-0.217869</td>\n      <td>-0.948357</td>\n      <td>-0.161605</td>\n      <td>0.515744</td>\n      <td>-0.807569</td>\n      <td>0.320963</td>\n      <td>-0.493803</td>\n      <td>0.907663</td>\n      <td>0.765842</td>\n      <td>-0.122724</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>0.656907</td>\n      <td>-0.471268</td>\n      <td>1.030935</td>\n      <td>-0.156784</td>\n      <td>0.702554</td>\n      <td>0.080158</td>\n      <td>0.515744</td>\n      <td>-0.564975</td>\n      <td>-0.080969</td>\n      <td>-0.364962</td>\n      <td>0.645535</td>\n      <td>0.384709</td>\n      <td>1.094717</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>0.819549</td>\n      <td>0.678597</td>\n      <td>0.708917</td>\n      <td>-1.317397</td>\n      <td>1.115282</td>\n      <td>0.644272</td>\n      <td>1.004817</td>\n      <td>-1.535352</td>\n      <td>0.128735</td>\n      <td>0.021559</td>\n      <td>0.033902</td>\n      <td>1.076394</td>\n      <td>0.295871</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>0.857082</td>\n      <td>-0.957749</td>\n      <td>-1.580989</td>\n      <td>-0.462209</td>\n      <td>-0.398053</td>\n      <td>-0.322781</td>\n      <td>-0.208084</td>\n      <td>-0.322381</td>\n      <td>-1.496468</td>\n      <td>-0.536749</td>\n      <td>1.213479</td>\n      <td>-0.194048</td>\n      <td>-0.391136</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>1.044746</td>\n      <td>-0.515493</td>\n      <td>0.208000</td>\n      <td>-1.683906</td>\n      <td>0.771342</td>\n      <td>2.546144</td>\n      <td>1.699300</td>\n      <td>-0.322381</td>\n      <td>0.495716</td>\n      <td>0.859022</td>\n      <td>0.252342</td>\n      <td>0.935234</td>\n      <td>1.398279</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2</td>\n      <td>-0.769338</td>\n      <td>-1.028510</td>\n      <td>-1.580989</td>\n      <td>0.026470</td>\n      <td>-1.498661</td>\n      <td>-0.306663</td>\n      <td>-0.002674</td>\n      <td>-0.726705</td>\n      <td>-0.954734</td>\n      <td>-0.158817</td>\n      <td>0.732911</td>\n      <td>1.245786</td>\n      <td>-0.774582</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n      <td>0.106426</td>\n      <td>-0.736621</td>\n      <td>-0.936953</td>\n      <td>-1.225769</td>\n      <td>-0.122901</td>\n      <td>0.160746</td>\n      <td>0.623340</td>\n      <td>-0.645840</td>\n      <td>-0.378049</td>\n      <td>-0.579696</td>\n      <td>0.995039</td>\n      <td>0.130620</td>\n      <td>0.855063</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1</td>\n      <td>1.520161</td>\n      <td>-0.506648</td>\n      <td>0.315339</td>\n      <td>-1.317397</td>\n      <td>0.840130</td>\n      <td>1.562973</td>\n      <td>1.356949</td>\n      <td>-0.160651</td>\n      <td>0.670469</td>\n      <td>0.730182</td>\n      <td>0.427094</td>\n      <td>0.356477</td>\n      <td>2.229079</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n      <td>-0.706784</td>\n      <td>0.192116</td>\n      <td>-0.328697</td>\n      <td>0.759488</td>\n      <td>-0.673205</td>\n      <td>-0.999718</td>\n      <td>-0.159177</td>\n      <td>2.427021</td>\n      <td>-0.290673</td>\n      <td>-1.009164</td>\n      <td>-0.402979</td>\n      <td>0.991698</td>\n      <td>-1.397682</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2</td>\n      <td>-0.431543</td>\n      <td>-0.860453</td>\n      <td>-1.223191</td>\n      <td>-0.828718</td>\n      <td>0.014675</td>\n      <td>-0.451721</td>\n      <td>-0.579780</td>\n      <td>1.375779</td>\n      <td>-1.688697</td>\n      <td>0.300714</td>\n      <td>0.121278</td>\n      <td>-1.422143</td>\n      <td>-0.966305</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2</td>\n      <td>-1.532505</td>\n      <td>0.307102</td>\n      <td>1.996988</td>\n      <td>0.148640</td>\n      <td>0.221039</td>\n      <td>-0.886895</td>\n      <td>0.026671</td>\n      <td>1.941833</td>\n      <td>-0.937259</td>\n      <td>-0.536749</td>\n      <td>1.213479</td>\n      <td>-0.137584</td>\n      <td>-0.464630</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2</td>\n      <td>-1.494972</td>\n      <td>-0.179379</td>\n      <td>1.496072</td>\n      <td>2.744746</td>\n      <td>-0.535629</td>\n      <td>-0.274428</td>\n      <td>0.232081</td>\n      <td>1.780103</td>\n      <td>0.303488</td>\n      <td>-0.880324</td>\n      <td>0.077590</td>\n      <td>-0.222280</td>\n      <td>-0.915179</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "train_wine.iloc[:, 1:], train_avg, train_stdev = ml.normalize(train_wine.iloc[:, 1:])\n",
    "test_wine.iloc[:, 1:] = (test_wine.iloc[:, 1:] - train_avg) / train_stdev\n",
    "\n",
    "display(train_wine)\n",
    "display(test_wine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:32.800467Z",
     "start_time": "2023-10-12T17:16:32.787064Z"
    }
   },
   "id": "23c8f7420ee23e5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: [[9.91107233e-01 9.47904603e-05 6.17151520e-03]\n",
      " [5.54448598e-04 9.49507916e-03 9.88033254e-01]\n",
      " [7.09619190e-05 9.99908066e-01 1.26473628e-03]\n",
      " [9.89553520e-01 6.25271048e-05 2.78322935e-03]\n",
      " [1.65008300e-03 9.84647154e-01 3.89037949e-03]\n",
      " [2.00009701e-04 9.90933267e-01 9.34436960e-03]\n",
      " [9.81396454e-01 2.10702185e-04 7.59193433e-04]\n",
      " [2.95391605e-03 1.70934774e-03 9.96905899e-01]\n",
      " [2.41153941e-04 9.98722943e-01 1.03781589e-02]\n",
      " [2.61218067e-04 9.99090752e-01 1.10109911e-02]\n",
      " [7.68849960e-03 1.00254146e-03 9.91202043e-01]\n",
      " [3.67425146e-03 8.77039368e-04 9.97494943e-01]\n",
      " [9.98712972e-01 4.10652018e-06 4.13902717e-02]\n",
      " [1.84628314e-02 9.98291048e-01 5.94510302e-04]\n",
      " [3.19943240e-03 1.58023270e-03 9.91190986e-01]\n",
      " [3.71810598e-01 9.62424377e-01 5.30919604e-06]\n",
      " [9.92098619e-01 5.75151264e-04 2.34226575e-03]\n",
      " [9.92417252e-01 2.89124485e-05 9.66372604e-03]\n",
      " [2.54747170e-03 6.82002210e-01 5.50433343e-04]\n",
      " [9.96730046e-01 2.45352022e-04 2.22444824e-03]\n",
      " [3.35354978e-04 9.99053207e-01 3.25179477e-03]\n",
      " [9.01033682e-01 4.63379047e-01 4.45735766e-06]\n",
      " [9.96039721e-01 7.95612964e-05 7.90556727e-03]\n",
      " [2.01048311e-04 9.98568806e-01 9.05079461e-03]\n",
      " [5.24102336e-05 9.92121768e-01 4.61715947e-02]\n",
      " [1.34806681e-02 9.62183355e-01 1.12973778e-02]\n",
      " [2.64727243e-04 9.99201751e-01 9.10638880e-03]]\n",
      "Weights:               0         1         2         3         4         5         6  \\\n",
      "14.23 -0.423943 -0.279321  2.083536  2.489735  1.299652  2.560405 -2.088438   \n",
      "1.71  -0.303911 -1.366980 -0.102249  2.169523  0.205120  0.761745  0.339546   \n",
      "2.43  -2.279590  0.002994  1.901199 -1.605658  0.862913  0.987339  0.295637   \n",
      "15.6   0.602285  0.011563 -1.026757  0.917099 -1.743219 -0.871425  0.861606   \n",
      "127    0.266163  1.217010 -0.105395  1.250515 -0.504911 -0.352939 -1.346302   \n",
      "2.8   -0.000601  0.466102  0.156845 -0.335575 -0.100680 -0.389011  0.739384   \n",
      "3.06   2.114341  0.207001 -1.985598 -0.698031  1.484397  1.472391 -0.818704   \n",
      ".28    1.509552 -2.946982 -0.090670 -0.944744  0.373534 -1.329378  0.167335   \n",
      "2.29  -0.518238 -0.388313 -1.943686 -0.088423 -0.676629 -0.138860 -1.220289   \n",
      "5.64  -2.107400 -0.776637  2.369793  0.169270 -0.476229  0.435455 -1.508464   \n",
      "1.04   0.919555  1.083226  0.260208 -1.908212 -0.233384  1.439387  3.332317   \n",
      "3.92   1.178972 -3.373783 -0.390470  0.796243  1.033459  1.454495  0.877754   \n",
      "1065  -0.507275  0.144229  2.199015 -0.195515  2.712315  2.912441 -1.488268   \n",
      "\n",
      "              7  \n",
      "14.23  0.571355  \n",
      "1.71  -0.559222  \n",
      "2.43  -0.366066  \n",
      "15.6   0.335635  \n",
      "127   -2.343082  \n",
      "2.8    0.190023  \n",
      "3.06  -0.792618  \n",
      ".28    0.520477  \n",
      "2.29  -1.545048  \n",
      "5.64  -0.223011  \n",
      "1.04  -1.498667  \n",
      "3.92  -0.979901  \n",
      "1065  -2.562540  \n",
      "[[-1.80581955  4.09275975 -5.17900251]\n",
      " [-2.04226993  2.66717256 -1.89054781]\n",
      " [-0.18215538 -5.41107133  3.87883564]\n",
      " [-1.33336302 -2.33035655  0.37380949]\n",
      " [ 4.12333075 -1.53150856 -2.52655625]\n",
      " [ 4.22988786 -4.47333683 -3.7421475 ]\n",
      " [-2.50926094  2.6311585  -1.58566777]\n",
      " [-4.01701222  0.45640352  2.05917267]]\n",
      "Input (scaled): \n",
      "        14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n",
      "0   -1.670125 -0.586254  0.923595  1.981185 -0.810781 -0.612897 -0.384151   \n",
      "1   -1.970387 -1.408850  0.494238  0.454064 -0.810781  0.289686  0.007108   \n",
      "2    0.994702  0.377863 -0.221358  0.759488 -0.673205 -1.531598 -1.293826   \n",
      "3    0.394178 -0.542029 -0.793834 -0.767633 -0.398053  0.160746  0.183174   \n",
      "4    0.381667 -0.320901  1.138274 -0.828718  0.152251  1.127799  1.200446   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "145 -0.594185 -0.533184 -1.366310  0.301352 -1.017145 -0.161605 -0.080925   \n",
      "146  0.444221 -1.231948 -0.006679 -0.767633  0.702554  0.370274 -0.687376   \n",
      "147 -0.268901  0.970486 -1.366310 -1.073057 -1.361085 -1.080306 -0.736283   \n",
      "148  1.395051 -0.276675  0.136440 -0.217869  0.221039  0.724860  0.897221   \n",
      "149  0.907125  2.934102  0.315339  0.301352 -0.329265 -0.999718 -1.362297   \n",
      "\n",
      "          .28      2.29      5.64      1.04      3.92      1065  \n",
      "0    0.324537 -0.430475 -1.052111  1.781424  0.864654 -0.608422  \n",
      "1    0.486267 -0.255722 -0.845966  0.645535 -0.405789 -1.017432  \n",
      "2    0.405402 -0.972209  1.945577 -1.101987 -1.295098 -0.439067  \n",
      "3   -0.726705 -0.413000 -0.472329  0.296030  0.243549  1.701840  \n",
      "4   -0.403246  0.128735  0.408081  0.514470  0.342361  1.653909  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "145 -0.322381 -0.185821 -0.906092  0.383406  1.372831 -0.256930  \n",
      "146  1.537509 -2.038203 -0.807314  0.296030 -0.942198 -0.007690  \n",
      "147  0.567132 -1.321715 -0.708537 -1.101987 -0.673993 -1.215545  \n",
      "148 -0.322381  1.386956  0.493974  0.514470  0.215316  0.982879  \n",
      "149  1.294914 -0.919783  1.138177 -1.364116 -1.210402 -0.039644  \n",
      "\n",
      "[150 rows x 13 columns]\n",
      "Hidden Layer: \n",
      "[[ -2.44257163   3.60694837   1.23255488   2.05446039   3.10072365\n",
      "    8.04921476  -6.30156198  -9.24163602]\n",
      " [-11.30213922   4.1156918    5.65665406   6.21481895  -7.11577061\n",
      "   -6.72614094 -10.37193573   4.78054556]\n",
      " [  4.29545119   4.19795258  -2.7206933   -9.65495844  -4.26786828\n",
      "   -5.23313938  12.57996706   1.55842853]\n",
      " [ -0.21721874   1.00820334   3.06438756   4.04536502   5.92211549\n",
      "    9.22224155  -8.01436846  -6.91848969]\n",
      " [  2.4364311   -5.52612085 -15.18187558   1.90921645  -3.19853508\n",
      "   -2.70312793   3.97003055  -0.47540091]\n",
      " [  3.12234013  -5.75831869  -8.17848338   2.59187045  -4.88657415\n",
      "   -4.83780405   7.6830922    1.2129638 ]\n",
      " [  0.84191357   1.39590165   4.47341858   2.31823843   8.71880156\n",
      "   12.12342021  -6.86165922  -3.76077423]\n",
      " [ -5.44078853  -3.74634037   2.96217464   2.43865059  -5.20385522\n",
      "  -10.2037836   -1.74440063   9.37486853]\n",
      " [  3.57778751  -3.18257337  -5.6376149   -1.81498591  -3.84195204\n",
      "   -6.62335285   2.96509844   6.40186887]\n",
      " [  2.87206687  -3.40933158  -5.29974049  -6.24616694  -5.34715594\n",
      "   -8.26201969   7.87068711   4.80117802]\n",
      " [-10.11787836  -0.56511746   9.64503201   5.3244287   -4.67811177\n",
      "   -2.10867142  -6.30587663   1.2215324 ]\n",
      " [ -5.44872437  -2.74301208   4.8789676   10.73622028  -3.40267218\n",
      "   -5.31333999  -3.24735229  13.18077674]\n",
      " [ -1.47575957  -3.02814113   7.14154362   1.43370318  11.27514515\n",
      "   14.55168808  -6.2383694   -6.65410926]\n",
      " [  8.23774554  -2.26481459 -13.19031715  -6.59412682  -2.63979116\n",
      "   -2.01737747  10.50353491  -2.77338667]\n",
      " [ -9.48320695   0.05322976   8.21885277   4.40165375  -3.93744089\n",
      "   -2.05795357  -6.53270631   2.63447215]\n",
      " [  3.76325106   0.61821169  -4.52373184  -3.03730798   0.5781798\n",
      "    2.45101145   5.00888728  -0.6116655 ]\n",
      " [ -0.85233543   3.00021819   4.2211084   -1.35475574   5.55646647\n",
      "    8.23811856  -1.25668302  -6.06807999]\n",
      " [ -1.82189563   1.60174627   2.50618234   3.82062723   6.21224289\n",
      "    9.80716665  -4.08353246  -6.7666893 ]\n",
      " [  5.2131463    4.24253225   0.95431302  -0.12109709  -0.11335407\n",
      "    0.99429453   4.52412872   4.12304572]\n",
      " [  0.09357221   0.28140224   5.17454486  -1.0756232   10.07887897\n",
      "   11.72409032  -6.6672852   -8.42953972]\n",
      " [  6.12242086  -1.23611496  -4.86939287  -2.86441339  -2.37194937\n",
      "   -2.05253498   8.57758167   4.97661837]\n",
      " [  3.93938027   4.27541063   0.08257581  -3.01207168   4.79924136\n",
      "    5.52428119   1.35405817  -3.20436399]\n",
      " [ -1.35755348   1.51297943   7.94900504  -0.2008643   11.32636687\n",
      "   14.12281251  -9.00286754 -10.1366031 ]\n",
      " [  8.38153514 -11.61525641  -8.27896941  -1.12003054  -3.37066109\n",
      "   -9.09135189   6.28684737   6.52869615]\n",
      " [  2.7552983    2.12861741   1.12514585  -3.43695416  -3.77641851\n",
      "   -9.10076039   2.08561982   7.68967967]\n",
      " [  1.9468426   -3.35299849  -0.07429587  -9.84081329  -0.51782148\n",
      "   -4.03312576   9.94084968  -0.3507227 ]\n",
      " [  3.93789432  -4.01084398  -8.26073565  -6.7396557   -6.68214111\n",
      "   -9.91051464   8.87758272   3.73928559]]\n",
      "Output: \n",
      "[[9.91107233e-01 9.47904603e-05 6.17151520e-03]\n",
      " [5.54448598e-04 9.49507916e-03 9.88033254e-01]\n",
      " [7.09619190e-05 9.99908066e-01 1.26473628e-03]\n",
      " [9.89553520e-01 6.25271048e-05 2.78322935e-03]\n",
      " [1.65008300e-03 9.84647154e-01 3.89037949e-03]\n",
      " [2.00009701e-04 9.90933267e-01 9.34436960e-03]\n",
      " [9.81396454e-01 2.10702185e-04 7.59193433e-04]\n",
      " [2.95391605e-03 1.70934774e-03 9.96905899e-01]\n",
      " [2.41153941e-04 9.98722943e-01 1.03781589e-02]\n",
      " [2.61218067e-04 9.99090752e-01 1.10109911e-02]\n",
      " [7.68849960e-03 1.00254146e-03 9.91202043e-01]\n",
      " [3.67425146e-03 8.77039368e-04 9.97494943e-01]\n",
      " [9.98712972e-01 4.10652018e-06 4.13902717e-02]\n",
      " [1.84628314e-02 9.98291048e-01 5.94510302e-04]\n",
      " [3.19943240e-03 1.58023270e-03 9.91190986e-01]\n",
      " [3.71810598e-01 9.62424377e-01 5.30919604e-06]\n",
      " [9.92098619e-01 5.75151264e-04 2.34226575e-03]\n",
      " [9.92417252e-01 2.89124485e-05 9.66372604e-03]\n",
      " [2.54747170e-03 6.82002210e-01 5.50433343e-04]\n",
      " [9.96730046e-01 2.45352022e-04 2.22444824e-03]\n",
      " [3.35354978e-04 9.99053207e-01 3.25179477e-03]\n",
      " [9.01033682e-01 4.63379047e-01 4.45735766e-06]\n",
      " [9.96039721e-01 7.95612964e-05 7.90556727e-03]\n",
      " [2.01048311e-04 9.98568806e-01 9.05079461e-03]\n",
      " [5.24102336e-05 9.92121768e-01 4.61715947e-02]\n",
      " [1.34806681e-02 9.62183355e-01 1.12973778e-02]\n",
      " [2.64727243e-04 9.99201751e-01 9.10638880e-03]]\n",
      "[[9.91107233e-01 9.47904603e-05 6.17151520e-03]\n",
      " [5.54448598e-04 9.49507916e-03 9.88033254e-01]\n",
      " [7.09619190e-05 9.99908066e-01 1.26473628e-03]\n",
      " [9.89553520e-01 6.25271048e-05 2.78322935e-03]\n",
      " [1.65008300e-03 9.84647154e-01 3.89037949e-03]\n",
      " [2.00009701e-04 9.90933267e-01 9.34436960e-03]\n",
      " [9.81396454e-01 2.10702185e-04 7.59193433e-04]\n",
      " [2.95391605e-03 1.70934774e-03 9.96905899e-01]\n",
      " [2.41153941e-04 9.98722943e-01 1.03781589e-02]\n",
      " [2.61218067e-04 9.99090752e-01 1.10109911e-02]\n",
      " [7.68849960e-03 1.00254146e-03 9.91202043e-01]\n",
      " [3.67425146e-03 8.77039368e-04 9.97494943e-01]\n",
      " [9.98712972e-01 4.10652018e-06 4.13902717e-02]\n",
      " [1.84628314e-02 9.98291048e-01 5.94510302e-04]\n",
      " [3.19943240e-03 1.58023270e-03 9.91190986e-01]\n",
      " [3.71810598e-01 9.62424377e-01 5.30919604e-06]\n",
      " [9.92098619e-01 5.75151264e-04 2.34226575e-03]\n",
      " [9.92417252e-01 2.89124485e-05 9.66372604e-03]\n",
      " [2.54747170e-03 6.82002210e-01 5.50433343e-04]\n",
      " [9.96730046e-01 2.45352022e-04 2.22444824e-03]\n",
      " [3.35354978e-04 9.99053207e-01 3.25179477e-03]\n",
      " [9.01033682e-01 4.63379047e-01 4.45735766e-06]\n",
      " [9.96039721e-01 7.95612964e-05 7.90556727e-03]\n",
      " [2.01048311e-04 9.98568806e-01 9.05079461e-03]\n",
      " [5.24102336e-05 9.92121768e-01 4.61715947e-02]\n",
      " [1.34806681e-02 9.62183355e-01 1.12973778e-02]\n",
      " [2.64727243e-04 9.99201751e-01 9.10638880e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Train the multi-class neural network on the wine dataset\n",
    "train_x = train_wine.iloc[:, 1:]\n",
    "train_y = train_wine.iloc[:, 0]\n",
    "\n",
    "# convert the labels to one-hot labels\n",
    "# subtract 1 from the labels to make them 0, 1, 2 instead of 1, 2, 3\n",
    "train_y = train_y - 1\n",
    "train_y = np.eye(3)[train_y]\n",
    "\n",
    "test_x = test_wine.iloc[:, 1:]\n",
    "test_y = test_wine.iloc[:, 0]\n",
    "\n",
    "mnn = ml.MultiNeuralNetwork()\n",
    "mnn.train(train_x, train_y)\n",
    "mnn_pred = mnn.predict(test_x)\n",
    "print(mnn_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:32.953447Z",
     "start_time": "2023-10-12T17:16:32.800820Z"
    }
   },
   "id": "bffdbc58851a8306"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Neural Network Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Test Labels: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the multi-class neural network and the test labels\n",
    "mnm_pred_idx = np.argmax(mnn_pred, axis=1)\n",
    "mnm_pred_idx = np.eye(3)[mnm_pred_idx]\n",
    "\n",
    "test_y_idx = test_y - 1\n",
    "test_y_idx = np.eye(3)[test_y_idx]\n",
    "\n",
    "print(\"Multi-class Neural Network Output: \\n\" + str(mnm_pred_idx))\n",
    "print(\"Test Labels: \\n\" + str(test_y_idx))\n",
    "\n",
    "accuracy = np.sum(mnm_pred_idx == test_y_idx) / len(test_y_idx) / 3\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:32.955962Z",
     "start_time": "2023-10-12T17:16:32.953344Z"
    }
   },
   "id": "42b3fb162df31812"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d900294fdf284d79"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Input Data:        14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n",
      "0   0.719462 -0.533184  0.351119  0.301352  1.115282  1.063329  0.760280   \n",
      "1  -0.243880  0.298257  0.422678  0.759488  0.840130 -1.322069 -0.628687   \n",
      "2  -1.019557 -0.780847  0.601577 -0.156784  0.289827 -0.661250 -0.247210   \n",
      "3   1.545182 -0.559719 -0.221358 -0.981430  1.252858  1.450150  0.975472   \n",
      "4  -1.394885  1.748856  0.100660  0.454064 -1.223509  0.902153  1.004817   \n",
      "5  -0.706784  1.854998  1.317173  2.133897  0.152251 -0.161605  0.124485   \n",
      "6   2.308349 -0.612790 -0.686495 -1.683906 -0.191689  0.805448  0.955909   \n",
      "7  -0.781849  1.324291  0.064881  0.454064 -0.810781 -0.000430 -1.059071   \n",
      "8  -0.331456 -0.515493 -0.292917  0.912200 -1.085933 -1.483245 -0.237429   \n",
      "9  -1.232243 -0.727776  0.208000  0.759488 -0.948357 -1.370422 -0.736283   \n",
      "10  0.581841  1.209304  0.852036  1.064913  0.771342 -0.967483 -1.059071   \n",
      "11  1.119811  2.394550 -0.471816  0.148640 -1.361085 -2.127947 -1.626396   \n",
      "12  1.757868 -0.409352  0.315339 -1.500651 -0.260477  0.321921  0.505962   \n",
      "13 -0.869426  0.740513 -0.543375 -0.462209 -0.810781  0.886036  0.965691   \n",
      "14  0.744483  0.218651  1.174054  1.523049  0.358615 -1.209247 -1.137323   \n",
      "15 -0.331456 -0.462423 -0.579155 -0.217869 -0.948357 -0.161605  0.515744   \n",
      "16  0.656907 -0.471268  1.030935 -0.156784  0.702554  0.080158  0.515744   \n",
      "17  0.819549  0.678597  0.708917 -1.317397  1.115282  0.644272  1.004817   \n",
      "18  0.857082 -0.957749 -1.580989 -0.462209 -0.398053 -0.322781 -0.208084   \n",
      "19  1.044746 -0.515493  0.208000 -1.683906  0.771342  2.546144  1.699300   \n",
      "20 -0.769338 -1.028510 -1.580989  0.026470 -1.498661 -0.306663 -0.002674   \n",
      "21  0.106426 -0.736621 -0.936953 -1.225769 -0.122901  0.160746  0.623340   \n",
      "22  1.520161 -0.506648  0.315339 -1.317397  0.840130  1.562973  1.356949   \n",
      "23 -0.706784  0.192116 -0.328697  0.759488 -0.673205 -0.999718 -0.159177   \n",
      "24 -0.431543 -0.860453 -1.223191 -0.828718  0.014675 -0.451721 -0.579780   \n",
      "25 -1.532505  0.307102  1.996988  0.148640  0.221039 -0.886895  0.026671   \n",
      "26 -1.494972 -0.179379  1.496072  2.744746 -0.535629 -0.274428  0.232081   \n",
      "\n",
      "         .28      2.29      5.64      1.04      3.92      1065  \n",
      "0  -1.292758  1.509284  0.515448  0.121278  0.610565  1.174602  \n",
      "1  -0.969299 -0.570277  2.469528 -2.063124 -1.591535 -0.870444  \n",
      "2   0.728861 -0.972209 -0.901797  2.174616 -0.518717 -1.269867  \n",
      "3  -0.807569  0.775321  0.571279 -0.053474  1.005814  0.695294  \n",
      "4  -1.211893  2.313147 -0.966218 -0.883547  1.471643 -1.189982  \n",
      "5   0.567132  0.216111 -1.275435 -0.140850  0.737609 -1.237913  \n",
      "6  -0.564975  0.687944  0.064506  0.558158  0.356477  0.934948  \n",
      "7   1.133185 -0.954734  1.116703 -1.713620 -1.436259 -0.742629  \n",
      "8   0.971455  0.076309 -0.751483 -0.315602 -0.250512 -0.844881  \n",
      "9   1.133185  0.076309 -0.622643  0.427094  0.074156 -0.966305  \n",
      "10  0.567132 -0.220771  2.417992 -0.446667 -1.464491 -0.183436  \n",
      "11  0.324537 -1.583845 -0.064334 -1.626244 -1.789159 -1.078144  \n",
      "12 -0.484110  0.687944  0.085980  0.296030  1.386947  1.717817  \n",
      "13  0.728861  2.138394 -1.180952  2.043552  0.328245 -1.106902  \n",
      "14  0.243672 -0.080969  1.550466 -0.927235 -1.125706 -0.007690  \n",
      "15 -0.807569  0.320963 -0.493803  0.907663  0.765842 -0.122724  \n",
      "16 -0.564975 -0.080969 -0.364962  0.645535  0.384709  1.094717  \n",
      "17 -1.535352  0.128735  0.021559  0.033902  1.076394  0.295871  \n",
      "18 -0.322381 -1.496468 -0.536749  1.213479 -0.194048 -0.391136  \n",
      "19 -0.322381  0.495716  0.859022  0.252342  0.935234  1.398279  \n",
      "20 -0.726705 -0.954734 -0.158817  0.732911  1.245786 -0.774582  \n",
      "21 -0.645840 -0.378049 -0.579696  0.995039  0.130620  0.855063  \n",
      "22 -0.160651  0.670469  0.730182  0.427094  0.356477  2.229079  \n",
      "23  2.427021 -0.290673 -1.009164 -0.402979  0.991698 -1.397682  \n",
      "24  1.375779 -1.688697  0.300714  0.121278 -1.422143 -0.966305  \n",
      "25  1.941833 -0.937259 -0.536749  1.213479 -0.137584 -0.464630  \n",
      "26  1.780103  0.303488 -0.880324  0.077590 -0.222280 -0.915179  \n",
      "Predicted data based on trained weights: [[1.0000000e+00 1.8845347e-25 6.3079206e-08]\n",
      " [7.6128280e-15 1.9829003e-34 1.0000000e+00]\n",
      " [9.3742004e-14 1.0000000e+00 1.0484628e-15]\n",
      " [1.0000000e+00 8.4481550e-25 9.7195385e-10]\n",
      " [3.6814177e-07 1.0000000e+00 1.9041063e-22]\n",
      " [1.7971846e-12 1.0000000e+00 6.6647909e-16]\n",
      " [1.0000000e+00 3.4616084e-23 3.5579897e-11]\n",
      " [1.1456088e-16 4.1629608e-17 1.0000000e+00]\n",
      " [4.7343041e-18 1.0000000e+00 1.6047970e-12]\n",
      " [3.6400375e-19 1.0000000e+00 6.2522660e-13]\n",
      " [1.8211487e-09 0.0000000e+00 1.0000000e+00]\n",
      " [7.0679783e-14 2.6788249e-15 1.0000000e+00]\n",
      " [1.0000000e+00 7.1298108e-34 1.9019074e-10]\n",
      " [1.5428379e-09 1.0000000e+00 1.0471324e-29]\n",
      " [4.2021694e-10 7.9892405e-35 1.0000000e+00]\n",
      " [3.5455535e-04 1.0000000e+00 8.9226864e-18]\n",
      " [1.0000000e+00 2.1210163e-16 4.5732856e-10]\n",
      " [1.0000000e+00 2.7840096e-21 1.5190046e-10]\n",
      " [1.2531729e-07 1.0000000e+00 1.6494772e-10]\n",
      " [1.0000000e+00 7.5914225e-31 2.8526228e-11]\n",
      " [5.4042461e-12 1.0000000e+00 3.8800565e-16]\n",
      " [9.9978507e-01 8.7666130e-01 1.4591100e-15]\n",
      " [1.0000000e+00 2.1061011e-38 5.0163905e-08]\n",
      " [6.4184954e-16 1.0000000e+00 1.1970914e-20]\n",
      " [8.4614767e-17 1.0000000e+00 3.9347097e-02]\n",
      " [7.1855919e-07 1.0000000e+00 2.1351123e-13]\n",
      " [5.1670765e-23 1.0000000e+00 1.5324979e-15]]\n",
      "Weights: [<tf.Variable 'autoencoder_tf_1/dense_2/kernel:0' shape=(13, 8) dtype=float32, numpy=\n",
      "array([[ 7.86084950e-01, -6.30134493e-02, -4.82862204e-01,\n",
      "        -4.36095685e-01, -1.36161232e+00,  1.37184501e+00,\n",
      "        -2.17767692e+00,  2.77148902e-01],\n",
      "       [ 6.28607035e-01,  1.23138670e-02, -5.51594257e-01,\n",
      "        -7.36285686e-01, -3.06281507e-01,  1.11885786e-01,\n",
      "        -1.47074506e-01,  3.19144964e-01],\n",
      "       [ 1.00059652e+00, -8.19076374e-02, -3.50615680e-01,\n",
      "        -1.03054798e+00, -1.43271470e+00,  1.51566255e+00,\n",
      "        -1.34493959e+00,  3.73008139e-02],\n",
      "       [-1.04619265e+00, -8.92414808e-01,  8.98350477e-01,\n",
      "         1.01238108e+00,  2.89801896e-01, -8.13882589e-01,\n",
      "         1.54639816e+00, -1.42934591e-01],\n",
      "       [ 3.06186646e-01,  4.50378507e-01, -1.69158936e-01,\n",
      "        -3.31469066e-02,  3.92659344e-02, -5.08138120e-01,\n",
      "         1.62945390e-01,  4.05368991e-02],\n",
      "       [ 9.43562910e-02, -1.74987897e-01,  1.83975533e-01,\n",
      "        -4.15629148e-01, -5.53265736e-02,  5.12079477e-01,\n",
      "        -3.24356258e-01,  6.92258477e-02],\n",
      "       [ 5.42994365e-02,  1.64624751e+00, -4.65641767e-01,\n",
      "         9.44730401e-01,  1.94279027e+00, -3.75304490e-01,\n",
      "        -4.78356063e-01, -9.12068009e-01],\n",
      "       [-5.86687624e-01,  7.22679794e-01,  3.52805585e-01,\n",
      "         1.48028791e-01,  7.52644777e-01, -9.57593508e-03,\n",
      "         8.43977273e-01, -2.14536324e-01],\n",
      "       [ 5.82998455e-01, -4.11521643e-01, -5.79717934e-01,\n",
      "         5.20414174e-01,  2.98169076e-01,  3.94528210e-01,\n",
      "         4.23514962e-01, -5.26413143e-01],\n",
      "       [ 1.50433123e+00, -1.56041956e+00, -7.65364051e-01,\n",
      "        -7.91795492e-01, -2.79214215e+00,  1.84107757e+00,\n",
      "         9.20419097e-02,  1.42749751e+00],\n",
      "       [-6.05553091e-01,  8.95175755e-01,  2.58545399e-01,\n",
      "         2.61026114e-01,  1.16988385e+00, -3.65025073e-01,\n",
      "         1.67868108e-01, -1.96037069e-01],\n",
      "       [ 1.50143340e-01,  1.74966824e+00, -5.85299551e-01,\n",
      "        -3.21111262e-01,  2.68138498e-01, -5.86424023e-02,\n",
      "        -1.13968074e+00, -1.09142351e+00],\n",
      "       [ 1.16636741e+00, -1.78613767e-01, -1.36655939e+00,\n",
      "        -1.10439932e+00, -1.59338284e+00,  2.07972145e+00,\n",
      "        -1.75334311e+00,  4.76294721e-04]], dtype=float32)>, <tf.Variable 'autoencoder_tf_1/dense_2/bias:0' shape=(8,) dtype=float32, numpy=\n",
      "array([-0.60057867, -0.47735763, -2.4209244 ,  1.2776073 , -0.83591896,\n",
      "        1.6689357 ,  1.5683988 , -1.8406677 ], dtype=float32)>, <tf.Variable 'autoencoder_tf_1/dense_3/kernel:0' shape=(8, 3) dtype=float32, numpy=\n",
      "array([[ 2.0670998 , -1.5567961 ,  0.71072763],\n",
      "       [ 2.485759  ,  1.8154634 , -2.2615035 ],\n",
      "       [-0.16229744,  2.4637928 ,  2.2643068 ],\n",
      "       [-1.751948  ,  1.1037596 , -1.7257618 ],\n",
      "       [ 0.08278155,  3.3513837 , -1.6405458 ],\n",
      "       [ 0.47960111, -2.7770681 , -0.59142995],\n",
      "       [-2.2908027 ,  0.40251622, -0.36690468],\n",
      "       [-0.09745229, -0.08790528,  3.1927004 ]], dtype=float32)>, <tf.Variable 'autoencoder_tf_1/dense_3/bias:0' shape=(3,) dtype=float32, numpy=array([-0.89099354, -1.2059406 , -1.1139405 ], dtype=float32)>]\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "[[1.0000000e+00 1.8845347e-25 6.3079206e-08]\n",
      " [7.6128280e-15 1.9829003e-34 1.0000000e+00]\n",
      " [9.3742004e-14 1.0000000e+00 1.0484628e-15]\n",
      " [1.0000000e+00 8.4481550e-25 9.7195385e-10]\n",
      " [3.6814177e-07 1.0000000e+00 1.9041063e-22]\n",
      " [1.7971846e-12 1.0000000e+00 6.6647909e-16]\n",
      " [1.0000000e+00 3.4616084e-23 3.5579897e-11]\n",
      " [1.1456088e-16 4.1629608e-17 1.0000000e+00]\n",
      " [4.7343041e-18 1.0000000e+00 1.6047970e-12]\n",
      " [3.6400375e-19 1.0000000e+00 6.2522660e-13]\n",
      " [1.8211487e-09 0.0000000e+00 1.0000000e+00]\n",
      " [7.0679783e-14 2.6788249e-15 1.0000000e+00]\n",
      " [1.0000000e+00 7.1298108e-34 1.9019074e-10]\n",
      " [1.5428379e-09 1.0000000e+00 1.0471324e-29]\n",
      " [4.2021694e-10 7.9892405e-35 1.0000000e+00]\n",
      " [3.5455535e-04 1.0000000e+00 8.9226864e-18]\n",
      " [1.0000000e+00 2.1210163e-16 4.5732856e-10]\n",
      " [1.0000000e+00 2.7840096e-21 1.5190046e-10]\n",
      " [1.2531729e-07 1.0000000e+00 1.6494772e-10]\n",
      " [1.0000000e+00 7.5914225e-31 2.8526228e-11]\n",
      " [5.4042461e-12 1.0000000e+00 3.8800565e-16]\n",
      " [9.9978507e-01 8.7666130e-01 1.4591100e-15]\n",
      " [1.0000000e+00 2.1061011e-38 5.0163905e-08]\n",
      " [6.4184954e-16 1.0000000e+00 1.1970914e-20]\n",
      " [8.4614767e-17 1.0000000e+00 3.9347097e-02]\n",
      " [7.1855919e-07 1.0000000e+00 2.1351123e-13]\n",
      " [5.1670765e-23 1.0000000e+00 1.5324979e-15]]\n"
     ]
    }
   ],
   "source": [
    "# Train the multi-class neural network using Tensorflow on the wine dataset\n",
    "mnn_tf = ml.AutoencoderTF(8, 3)\n",
    "mnn_tf.train(train_x, train_y)\n",
    "mnn_tf_pred = mnn_tf.test(test_x)\n",
    "print(mnn_tf_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:45.599022Z",
     "start_time": "2023-10-12T17:16:32.956797Z"
    }
   },
   "id": "61245605bd0b5f91"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Neural Network Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Multi-class Neural Network using Tensorflow Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Test Labels: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the multi-class neural network, test labels, and the multi-class neural network using Tensorflow\n",
    "mnn_tf_pred_idx = np.argmax(mnn_tf_pred, axis=1)\n",
    "mnn_tf_pred_idx = np.eye(3)[mnn_tf_pred_idx]\n",
    "\n",
    "print(\"Multi-class Neural Network Output: \\n\" + str(mnm_pred_idx))\n",
    "print(\"Multi-class Neural Network using Tensorflow Output: \\n\" + str(mnn_tf_pred_idx))\n",
    "print(\"Test Labels: \\n\" + str(test_y_idx))\n",
    "\n",
    "accuracy = np.sum(mnn_tf_pred_idx == test_y_idx) / len(test_y_idx) / 3\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:45.622732Z",
     "start_time": "2023-10-12T17:16:45.592598Z"
    }
   },
   "id": "f958f869991e4d0c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:45.622888Z",
     "start_time": "2023-10-12T17:16:45.594453Z"
    }
   },
   "id": "51c82f7b6e07a277"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
