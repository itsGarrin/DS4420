{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:35:59.385944Z",
     "start_time": "2023-10-12T22:35:56.838799Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import MLutils as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "y = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:35:59.391615Z",
     "start_time": "2023-10-12T22:35:59.387411Z"
    }
   },
   "id": "8880148f815df2a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d09354f5fcbb3c74"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Hidden Layer: \n",
      "[[-4.61514556  4.53640579  2.87366823]\n",
      " [-1.31683754 -0.87301189  4.9512531 ]\n",
      " [-1.37888726 -1.39106612 -1.40118385]\n",
      " [-1.38470568 -1.39471074 -1.39155713]\n",
      " [ 4.92109748 -1.36347598 -0.94472491]\n",
      " [ 4.60982625  2.85535145 -4.8267931 ]\n",
      " [-0.88398735  4.84372191 -1.31565379]\n",
      " [ 3.03284209 -4.9449211   4.42194226]]\n",
      "Output: \n",
      "[[8.92789487e-01 6.92794892e-02 5.13709082e-02 5.23315102e-02\n",
      "  1.31348378e-08 4.67045985e-07 6.04957834e-02 1.62213436e-06]\n",
      " [8.25149338e-02 8.83073400e-01 9.49307228e-02 9.72234391e-02\n",
      "  2.96764401e-05 2.51547402e-07 5.90442277e-05 8.25208706e-02]\n",
      " [6.69211891e-02 7.91001472e-02 2.89551726e-01 2.89539576e-01\n",
      "  8.29543965e-02 6.51610891e-02 8.16836369e-02 6.63524281e-02]\n",
      " [6.77695198e-02 8.09422596e-02 2.89543413e-01 2.89548068e-01\n",
      "  8.18776461e-02 6.35799751e-02 8.08865678e-02 6.71129877e-02]\n",
      " [3.81503050e-07 5.55925683e-05 9.80107774e-02 9.67735008e-02\n",
      "  8.84683908e-01 7.99691148e-02 3.20510939e-05 8.05691898e-02]\n",
      " [1.73478629e-06 1.05707703e-08 5.16695785e-02 5.00405248e-02\n",
      "  5.70179827e-02 8.95386698e-01 6.93828401e-02 4.78791045e-07]\n",
      " [8.39078705e-02 2.86399742e-05 9.65341288e-02 9.54604270e-02\n",
      "  6.13316649e-05 8.32490310e-02 8.81002353e-01 3.29584869e-07]\n",
      " [5.42816443e-07 5.83037988e-02 5.03539594e-02 5.10450266e-02\n",
      "  6.53133732e-02 1.01236713e-06 1.08016604e-08 8.95143509e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Implement the Neural Network\n",
    "nn = ml.NeuralNetwork()\n",
    "nn.train(X, y, learning_rate=1)\n",
    "nn_pred = nn.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:35:59.406839Z",
     "start_time": "2023-10-12T22:35:59.390573Z"
    }
   },
   "id": "1becf6db31023bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b2d0f6dd3250344"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 18:35:59.473440: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "Input Data: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Predicted data based on trained weights: [[9.65946674e-01 7.01775305e-10 1.14057339e-05 1.92379998e-03\n",
      "  3.18451603e-05 3.09054572e-02 2.18386026e-06 3.30011733e-02]\n",
      " [4.47212579e-11 9.69834089e-01 6.99610464e-05 1.14604377e-03\n",
      "  3.74534540e-02 9.65636787e-07 2.71138325e-02 6.06935799e-08]\n",
      " [6.31995918e-03 2.77475026e-02 9.41536427e-01 3.43104228e-02\n",
      "  6.05240986e-02 3.04663263e-05 5.88419462e-05 5.22823520e-02]\n",
      " [3.11061405e-02 2.48022545e-02 3.04137990e-02 9.28406000e-01\n",
      "  1.25221990e-03 4.34941240e-02 5.32917604e-02 3.57653975e-04]\n",
      " [5.91255289e-13 1.02460384e-02 2.48233527e-02 7.88095370e-12\n",
      "  9.55881953e-01 4.09311392e-14 1.20970081e-10 1.54318567e-03]\n",
      " [1.70577131e-02 1.45734771e-10 1.56770782e-13 1.85918231e-02\n",
      "  5.03266335e-08 9.67800736e-01 1.75122824e-02 5.71741410e-08]\n",
      " [4.16380749e-11 1.88202821e-02 1.04654631e-12 2.22629756e-02\n",
      "  1.06602365e-05 1.70360301e-02 9.65997696e-01 3.49838626e-12]\n",
      " [2.21787170e-02 1.96383423e-10 1.12179257e-02 1.39785539e-09\n",
      "  2.55059563e-02 8.37269987e-09 2.94087281e-12 9.65336680e-01]]\n",
      "Weights: [<tf.Variable 'autoencoder_tf/dense/kernel:0' shape=(8, 3) dtype=float32, numpy=\n",
      "array([[ 0.1709581 ,  2.1981146 ,  1.9620411 ],\n",
      "       [-0.7057107 , -2.2098255 , -2.4349785 ],\n",
      "       [ 2.1449351 , -1.0758426 ,  0.43473336],\n",
      "       [ 0.55055475, -1.7114398 ,  1.4930755 ],\n",
      "       [ 1.8269682 ,  1.1474991 , -5.1890383 ],\n",
      "       [-3.7172172 ,  2.002671  ,  1.6529132 ],\n",
      "       [-4.300655  , -1.4023902 , -1.5225371 ],\n",
      "       [ 2.3524914 ,  3.8870049 , -0.8062188 ]], dtype=float32)>, <tf.Variable 'autoencoder_tf/dense/bias:0' shape=(3,) dtype=float32, numpy=array([-1.5183779,  1.9381768, -1.8887749], dtype=float32)>, <tf.Variable 'autoencoder_tf/dense_1/kernel:0' shape=(3, 8) dtype=float32, numpy=\n",
      "array([[ 1.4741951 ,  0.7362119 ,  4.7163444 , -0.68669903,  1.798276  ,\n",
      "        -2.00188   , -2.269079  ,  3.2731674 ],\n",
      "       [ 1.3324702 , -4.198158  , -1.5975511 , -2.6731772 , -0.60013837,\n",
      "        -0.70116603, -2.9721143 ,  1.3648705 ],\n",
      "       [ 4.550792  , -1.5210316 ,  0.2486586 ,  2.9347103 , -1.3734822 ,\n",
      "         3.4684246 ,  1.2817641 ,  0.9902213 ]], dtype=float32)>, <tf.Variable 'autoencoder_tf/dense_1/bias:0' shape=(8,) dtype=float32, numpy=\n",
      "array([-0.51336396, -2.6091769 ,  1.5632317 ,  3.6652255 , -5.3485837 ,\n",
      "       -3.4966931 , -3.892195  , -4.685376  ], dtype=float32)>]\n",
      "1/1 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create an Autoencoder using Tensorflow\n",
    "autoencoder = ml.AutoencoderTF()\n",
    "autoencoder.train(X, y, learning_rate=1)\n",
    "ae_pred = autoencoder.test(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:36:04.122089Z",
     "start_time": "2023-10-12T22:35:59.408145Z"
    }
   },
   "id": "9f2f59b0676efa32"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Output: \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Autoencoder Output: \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the Autoencoder and the Neural Network using mean squared error (MSE)\n",
    "\n",
    "# use argmax to select the only maximum as 1 and convert the results to one-hot labels\n",
    "nn_pred_idx = np.argmax(nn_pred, axis=1)\n",
    "nn_pred_idx = np.eye(8)[nn_pred_idx]\n",
    "\n",
    "ae_pred_idx = np.argmax(ae_pred, axis=1)\n",
    "ae_pred_idx = np.eye(8)[ae_pred_idx]\n",
    "\n",
    "print(\"Neural Network Output: \\n\" + str(nn_pred_idx))\n",
    "print(\"Autoencoder Output: \\n\" + str(ae_pred_idx))\n",
    "\n",
    "accuracy = np.sum(nn_pred_idx == ae_pred_idx) / len(ae_pred_idx) / 8\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:36:04.191240Z",
     "start_time": "2023-10-12T22:36:04.120607Z"
    }
   },
   "id": "735da874ef6b998d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc899744cf7484b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "     1  14.23  1.71  2.43  15.6  127   2.8  3.06   .28  2.29  5.64  1.04  \\\n0    2  11.65  1.67  2.62  26.0   88  1.92  1.61  0.40  1.34  2.60  1.36   \n1    2  11.41  0.74  2.50  21.0   88  2.48  2.01  0.42  1.44  3.08  1.10   \n2    3  13.78  2.76  2.30  22.0   90  1.35  0.68  0.41  1.03  9.58  0.70   \n3    1  13.30  1.72  2.14  17.0   94  2.40  2.19  0.27  1.35  3.95  1.02   \n4    1  13.29  1.97  2.68  16.8  102  3.00  3.23  0.31  1.66  6.00  1.07   \n..  ..    ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   \n145  2  12.51  1.73  1.98  20.5   85  2.20  1.92  0.32  1.48  2.94  1.04   \n146  2  13.34  0.94  2.36  17.0  110  2.53  1.30  0.55  0.42  3.17  1.02   \n147  2  12.77  3.43  1.98  16.0   80  1.63  1.25  0.43  0.83  3.40  0.70   \n148  1  14.10  2.02  2.40  18.8  103  2.75  2.92  0.32  2.38  6.20  1.07   \n149  3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06  7.70  0.64   \n\n     3.92  1065  \n0    3.21   562  \n1    2.31   434  \n2    1.68   615  \n3    2.77  1285  \n4    2.84  1270  \n..    ...   ...  \n145  3.57   672  \n146  1.93   750  \n147  2.12   372  \n148  2.75  1060  \n149  1.74   740  \n\n[150 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>11.65</td>\n      <td>1.67</td>\n      <td>2.62</td>\n      <td>26.0</td>\n      <td>88</td>\n      <td>1.92</td>\n      <td>1.61</td>\n      <td>0.40</td>\n      <td>1.34</td>\n      <td>2.60</td>\n      <td>1.36</td>\n      <td>3.21</td>\n      <td>562</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>11.41</td>\n      <td>0.74</td>\n      <td>2.50</td>\n      <td>21.0</td>\n      <td>88</td>\n      <td>2.48</td>\n      <td>2.01</td>\n      <td>0.42</td>\n      <td>1.44</td>\n      <td>3.08</td>\n      <td>1.10</td>\n      <td>2.31</td>\n      <td>434</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>13.78</td>\n      <td>2.76</td>\n      <td>2.30</td>\n      <td>22.0</td>\n      <td>90</td>\n      <td>1.35</td>\n      <td>0.68</td>\n      <td>0.41</td>\n      <td>1.03</td>\n      <td>9.58</td>\n      <td>0.70</td>\n      <td>1.68</td>\n      <td>615</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>13.30</td>\n      <td>1.72</td>\n      <td>2.14</td>\n      <td>17.0</td>\n      <td>94</td>\n      <td>2.40</td>\n      <td>2.19</td>\n      <td>0.27</td>\n      <td>1.35</td>\n      <td>3.95</td>\n      <td>1.02</td>\n      <td>2.77</td>\n      <td>1285</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.29</td>\n      <td>1.97</td>\n      <td>2.68</td>\n      <td>16.8</td>\n      <td>102</td>\n      <td>3.00</td>\n      <td>3.23</td>\n      <td>0.31</td>\n      <td>1.66</td>\n      <td>6.00</td>\n      <td>1.07</td>\n      <td>2.84</td>\n      <td>1270</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2</td>\n      <td>12.51</td>\n      <td>1.73</td>\n      <td>1.98</td>\n      <td>20.5</td>\n      <td>85</td>\n      <td>2.20</td>\n      <td>1.92</td>\n      <td>0.32</td>\n      <td>1.48</td>\n      <td>2.94</td>\n      <td>1.04</td>\n      <td>3.57</td>\n      <td>672</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>2</td>\n      <td>13.34</td>\n      <td>0.94</td>\n      <td>2.36</td>\n      <td>17.0</td>\n      <td>110</td>\n      <td>2.53</td>\n      <td>1.30</td>\n      <td>0.55</td>\n      <td>0.42</td>\n      <td>3.17</td>\n      <td>1.02</td>\n      <td>1.93</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2</td>\n      <td>12.77</td>\n      <td>3.43</td>\n      <td>1.98</td>\n      <td>16.0</td>\n      <td>80</td>\n      <td>1.63</td>\n      <td>1.25</td>\n      <td>0.43</td>\n      <td>0.83</td>\n      <td>3.40</td>\n      <td>0.70</td>\n      <td>2.12</td>\n      <td>372</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>1</td>\n      <td>14.10</td>\n      <td>2.02</td>\n      <td>2.40</td>\n      <td>18.8</td>\n      <td>103</td>\n      <td>2.75</td>\n      <td>2.92</td>\n      <td>0.32</td>\n      <td>2.38</td>\n      <td>6.20</td>\n      <td>1.07</td>\n      <td>2.75</td>\n      <td>1060</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>3</td>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    1  14.23  1.71  2.43  15.6  127   2.8  3.06   .28  2.29   5.64  1.04  \\\n0   1  13.56  1.73  2.46  20.5  116  2.96  2.78  0.20  2.45   6.25  0.98   \n1   3  12.79  2.67  2.48  22.0  112  1.48  1.36  0.24  1.26  10.80  0.48   \n2   2  12.17  1.45  2.53  19.0  104  1.89  1.75  0.45  1.03   2.95  1.45   \n3   1  14.22  1.70  2.30  16.3  118  3.20  3.00  0.26  2.03   6.38  0.94   \n4   2  11.87  4.31  2.39  21.0   82  2.86  3.03  0.21  2.91   2.80  0.75   \n5   2  12.42  4.43  2.73  26.5  102  2.20  2.13  0.43  1.71   2.08  0.92   \n6   1  14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98   5.20  1.08   \n7   3  12.36  3.83  2.38  21.0   88  2.30  0.92  0.50  1.04   7.65  0.56   \n8   2  12.72  1.75  2.28  22.5   84  1.38  1.76  0.48  1.63   3.30  0.88   \n9   2  12.00  1.51  2.42  22.0   86  1.45  1.25  0.50  1.63   3.60  1.05   \n10  3  13.45  3.70  2.60  23.0  111  1.70  0.92  0.43  1.46  10.68  0.85   \n11  3  13.88  5.04  2.23  20.0   80  0.98  0.34  0.40  0.68   4.90  0.58   \n12  1  14.39  1.87  2.45  14.6   96  2.50  2.52  0.30  1.98   5.25  1.02   \n13  2  12.29  3.17  2.21  18.0   88  2.85  2.99  0.45  2.81   2.30  1.42   \n14  3  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54   8.66  0.74   \n15  2  12.72  1.81  2.20  18.8   86  2.20  2.53  0.26  1.77   3.90  1.16   \n16  1  13.51  1.80  2.65  19.0  110  2.35  2.53  0.29  1.54   4.20  1.10   \n17  1  13.64  3.10  2.56  15.2  116  2.70  3.03  0.17  1.66   5.10  0.96   \n18  2  13.67  1.25  1.92  18.0   94  2.10  1.79  0.32  0.73   3.80  1.23   \n19  1  13.82  1.75  2.42  14.0  111  3.88  3.74  0.32  1.87   7.05  1.01   \n20  2  12.37  1.17  1.92  19.6   78  2.11  2.00  0.27  1.04   4.68  1.12   \n21  1  13.07  1.50  2.10  15.5   98  2.40  2.64  0.28  1.37   3.70  1.18   \n22  1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97   6.75  1.05   \n23  2  12.42  2.55  2.27  22.0   90  1.68  1.84  0.66  1.42   2.70  0.86   \n24  2  12.64  1.36  2.02  16.8  100  2.02  1.41  0.53  0.62   5.75  0.98   \n25  2  11.76  2.68  2.92  20.0  103  1.75  2.03  0.60  1.05   3.80  1.23   \n26  2  11.79  2.13  2.78  28.5   92  2.13  2.24  0.58  1.76   3.00  0.97   \n\n    3.92  1065  \n0   3.03  1120  \n1   1.47   480  \n2   2.23   355  \n3   3.31   970  \n4   3.64   380  \n5   3.12   365  \n6   2.85  1045  \n7   1.58   520  \n8   2.42   488  \n9   2.65   450  \n10  1.56   695  \n11  1.33   415  \n12  3.58  1290  \n13  2.83   406  \n14  1.80   750  \n15  3.14   714  \n16  2.87  1095  \n17  3.36   845  \n18  2.46   630  \n19  3.26  1190  \n20  3.48   510  \n21  2.69  1020  \n22  2.85  1450  \n23  3.30   315  \n24  1.59   450  \n25  2.50   607  \n26  2.44   466  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>13.56</td>\n      <td>1.73</td>\n      <td>2.46</td>\n      <td>20.5</td>\n      <td>116</td>\n      <td>2.96</td>\n      <td>2.78</td>\n      <td>0.20</td>\n      <td>2.45</td>\n      <td>6.25</td>\n      <td>0.98</td>\n      <td>3.03</td>\n      <td>1120</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>12.79</td>\n      <td>2.67</td>\n      <td>2.48</td>\n      <td>22.0</td>\n      <td>112</td>\n      <td>1.48</td>\n      <td>1.36</td>\n      <td>0.24</td>\n      <td>1.26</td>\n      <td>10.80</td>\n      <td>0.48</td>\n      <td>1.47</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>12.17</td>\n      <td>1.45</td>\n      <td>2.53</td>\n      <td>19.0</td>\n      <td>104</td>\n      <td>1.89</td>\n      <td>1.75</td>\n      <td>0.45</td>\n      <td>1.03</td>\n      <td>2.95</td>\n      <td>1.45</td>\n      <td>2.23</td>\n      <td>355</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.22</td>\n      <td>1.70</td>\n      <td>2.30</td>\n      <td>16.3</td>\n      <td>118</td>\n      <td>3.20</td>\n      <td>3.00</td>\n      <td>0.26</td>\n      <td>2.03</td>\n      <td>6.38</td>\n      <td>0.94</td>\n      <td>3.31</td>\n      <td>970</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>11.87</td>\n      <td>4.31</td>\n      <td>2.39</td>\n      <td>21.0</td>\n      <td>82</td>\n      <td>2.86</td>\n      <td>3.03</td>\n      <td>0.21</td>\n      <td>2.91</td>\n      <td>2.80</td>\n      <td>0.75</td>\n      <td>3.64</td>\n      <td>380</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>12.42</td>\n      <td>4.43</td>\n      <td>2.73</td>\n      <td>26.5</td>\n      <td>102</td>\n      <td>2.20</td>\n      <td>2.13</td>\n      <td>0.43</td>\n      <td>1.71</td>\n      <td>2.08</td>\n      <td>0.92</td>\n      <td>3.12</td>\n      <td>365</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>14.83</td>\n      <td>1.64</td>\n      <td>2.17</td>\n      <td>14.0</td>\n      <td>97</td>\n      <td>2.80</td>\n      <td>2.98</td>\n      <td>0.29</td>\n      <td>1.98</td>\n      <td>5.20</td>\n      <td>1.08</td>\n      <td>2.85</td>\n      <td>1045</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>12.36</td>\n      <td>3.83</td>\n      <td>2.38</td>\n      <td>21.0</td>\n      <td>88</td>\n      <td>2.30</td>\n      <td>0.92</td>\n      <td>0.50</td>\n      <td>1.04</td>\n      <td>7.65</td>\n      <td>0.56</td>\n      <td>1.58</td>\n      <td>520</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>12.72</td>\n      <td>1.75</td>\n      <td>2.28</td>\n      <td>22.5</td>\n      <td>84</td>\n      <td>1.38</td>\n      <td>1.76</td>\n      <td>0.48</td>\n      <td>1.63</td>\n      <td>3.30</td>\n      <td>0.88</td>\n      <td>2.42</td>\n      <td>488</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>12.00</td>\n      <td>1.51</td>\n      <td>2.42</td>\n      <td>22.0</td>\n      <td>86</td>\n      <td>1.45</td>\n      <td>1.25</td>\n      <td>0.50</td>\n      <td>1.63</td>\n      <td>3.60</td>\n      <td>1.05</td>\n      <td>2.65</td>\n      <td>450</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>13.45</td>\n      <td>3.70</td>\n      <td>2.60</td>\n      <td>23.0</td>\n      <td>111</td>\n      <td>1.70</td>\n      <td>0.92</td>\n      <td>0.43</td>\n      <td>1.46</td>\n      <td>10.68</td>\n      <td>0.85</td>\n      <td>1.56</td>\n      <td>695</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>13.88</td>\n      <td>5.04</td>\n      <td>2.23</td>\n      <td>20.0</td>\n      <td>80</td>\n      <td>0.98</td>\n      <td>0.34</td>\n      <td>0.40</td>\n      <td>0.68</td>\n      <td>4.90</td>\n      <td>0.58</td>\n      <td>1.33</td>\n      <td>415</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>14.39</td>\n      <td>1.87</td>\n      <td>2.45</td>\n      <td>14.6</td>\n      <td>96</td>\n      <td>2.50</td>\n      <td>2.52</td>\n      <td>0.30</td>\n      <td>1.98</td>\n      <td>5.25</td>\n      <td>1.02</td>\n      <td>3.58</td>\n      <td>1290</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>12.29</td>\n      <td>3.17</td>\n      <td>2.21</td>\n      <td>18.0</td>\n      <td>88</td>\n      <td>2.85</td>\n      <td>2.99</td>\n      <td>0.45</td>\n      <td>2.81</td>\n      <td>2.30</td>\n      <td>1.42</td>\n      <td>2.83</td>\n      <td>406</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>13.58</td>\n      <td>2.58</td>\n      <td>2.69</td>\n      <td>24.5</td>\n      <td>105</td>\n      <td>1.55</td>\n      <td>0.84</td>\n      <td>0.39</td>\n      <td>1.54</td>\n      <td>8.66</td>\n      <td>0.74</td>\n      <td>1.80</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n      <td>12.72</td>\n      <td>1.81</td>\n      <td>2.20</td>\n      <td>18.8</td>\n      <td>86</td>\n      <td>2.20</td>\n      <td>2.53</td>\n      <td>0.26</td>\n      <td>1.77</td>\n      <td>3.90</td>\n      <td>1.16</td>\n      <td>3.14</td>\n      <td>714</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>13.51</td>\n      <td>1.80</td>\n      <td>2.65</td>\n      <td>19.0</td>\n      <td>110</td>\n      <td>2.35</td>\n      <td>2.53</td>\n      <td>0.29</td>\n      <td>1.54</td>\n      <td>4.20</td>\n      <td>1.10</td>\n      <td>2.87</td>\n      <td>1095</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>13.64</td>\n      <td>3.10</td>\n      <td>2.56</td>\n      <td>15.2</td>\n      <td>116</td>\n      <td>2.70</td>\n      <td>3.03</td>\n      <td>0.17</td>\n      <td>1.66</td>\n      <td>5.10</td>\n      <td>0.96</td>\n      <td>3.36</td>\n      <td>845</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>13.67</td>\n      <td>1.25</td>\n      <td>1.92</td>\n      <td>18.0</td>\n      <td>94</td>\n      <td>2.10</td>\n      <td>1.79</td>\n      <td>0.32</td>\n      <td>0.73</td>\n      <td>3.80</td>\n      <td>1.23</td>\n      <td>2.46</td>\n      <td>630</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>13.82</td>\n      <td>1.75</td>\n      <td>2.42</td>\n      <td>14.0</td>\n      <td>111</td>\n      <td>3.88</td>\n      <td>3.74</td>\n      <td>0.32</td>\n      <td>1.87</td>\n      <td>7.05</td>\n      <td>1.01</td>\n      <td>3.26</td>\n      <td>1190</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2</td>\n      <td>12.37</td>\n      <td>1.17</td>\n      <td>1.92</td>\n      <td>19.6</td>\n      <td>78</td>\n      <td>2.11</td>\n      <td>2.00</td>\n      <td>0.27</td>\n      <td>1.04</td>\n      <td>4.68</td>\n      <td>1.12</td>\n      <td>3.48</td>\n      <td>510</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n      <td>13.07</td>\n      <td>1.50</td>\n      <td>2.10</td>\n      <td>15.5</td>\n      <td>98</td>\n      <td>2.40</td>\n      <td>2.64</td>\n      <td>0.28</td>\n      <td>1.37</td>\n      <td>3.70</td>\n      <td>1.18</td>\n      <td>2.69</td>\n      <td>1020</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1</td>\n      <td>14.20</td>\n      <td>1.76</td>\n      <td>2.45</td>\n      <td>15.2</td>\n      <td>112</td>\n      <td>3.27</td>\n      <td>3.39</td>\n      <td>0.34</td>\n      <td>1.97</td>\n      <td>6.75</td>\n      <td>1.05</td>\n      <td>2.85</td>\n      <td>1450</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n      <td>12.42</td>\n      <td>2.55</td>\n      <td>2.27</td>\n      <td>22.0</td>\n      <td>90</td>\n      <td>1.68</td>\n      <td>1.84</td>\n      <td>0.66</td>\n      <td>1.42</td>\n      <td>2.70</td>\n      <td>0.86</td>\n      <td>3.30</td>\n      <td>315</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2</td>\n      <td>12.64</td>\n      <td>1.36</td>\n      <td>2.02</td>\n      <td>16.8</td>\n      <td>100</td>\n      <td>2.02</td>\n      <td>1.41</td>\n      <td>0.53</td>\n      <td>0.62</td>\n      <td>5.75</td>\n      <td>0.98</td>\n      <td>1.59</td>\n      <td>450</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2</td>\n      <td>11.76</td>\n      <td>2.68</td>\n      <td>2.92</td>\n      <td>20.0</td>\n      <td>103</td>\n      <td>1.75</td>\n      <td>2.03</td>\n      <td>0.60</td>\n      <td>1.05</td>\n      <td>3.80</td>\n      <td>1.23</td>\n      <td>2.50</td>\n      <td>607</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2</td>\n      <td>11.79</td>\n      <td>2.13</td>\n      <td>2.78</td>\n      <td>28.5</td>\n      <td>92</td>\n      <td>2.13</td>\n      <td>2.24</td>\n      <td>0.58</td>\n      <td>1.76</td>\n      <td>3.00</td>\n      <td>0.97</td>\n      <td>2.44</td>\n      <td>466</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the wine dataset\n",
    "train_wine = pd.read_csv('train_wine.csv')\n",
    "test_wine = pd.read_csv('test_wine.csv')\n",
    "\n",
    "display(train_wine)\n",
    "display(test_wine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:36:04.226419Z",
     "start_time": "2023-10-12T22:36:04.167594Z"
    }
   },
   "id": "37196ef4493105a9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "     1     14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n0    2 -1.670125 -0.586254  0.923595  1.981185 -0.810781 -0.612897 -0.384151   \n1    2 -1.970387 -1.408850  0.494238  0.454064 -0.810781  0.289686  0.007108   \n2    3  0.994702  0.377863 -0.221358  0.759488 -0.673205 -1.531598 -1.293826   \n3    1  0.394178 -0.542029 -0.793834 -0.767633 -0.398053  0.160746  0.183174   \n4    1  0.381667 -0.320901  1.138274 -0.828718  0.152251  1.127799  1.200446   \n..  ..       ...       ...       ...       ...       ...       ...       ...   \n145  2 -0.594185 -0.533184 -1.366310  0.301352 -1.017145 -0.161605 -0.080925   \n146  2  0.444221 -1.231948 -0.006679 -0.767633  0.702554  0.370274 -0.687376   \n147  2 -0.268901  0.970486 -1.366310 -1.073057 -1.361085 -1.080306 -0.736283   \n148  1  1.395051 -0.276675  0.136440 -0.217869  0.221039  0.724860  0.897221   \n149  3  0.907125  2.934102  0.315339  0.301352 -0.329265 -0.999718 -1.362297   \n\n          .28      2.29      5.64      1.04      3.92      1065  \n0    0.324537 -0.430475 -1.052111  1.781424  0.864654 -0.608422  \n1    0.486267 -0.255722 -0.845966  0.645535 -0.405789 -1.017432  \n2    0.405402 -0.972209  1.945577 -1.101987 -1.295098 -0.439067  \n3   -0.726705 -0.413000 -0.472329  0.296030  0.243549  1.701840  \n4   -0.403246  0.128735  0.408081  0.514470  0.342361  1.653909  \n..        ...       ...       ...       ...       ...       ...  \n145 -0.322381 -0.185821 -0.906092  0.383406  1.372831 -0.256930  \n146  1.537509 -2.038203 -0.807314  0.296030 -0.942198 -0.007690  \n147  0.567132 -1.321715 -0.708537 -1.101987 -0.673993 -1.215545  \n148 -0.322381  1.386956  0.493974  0.514470  0.215316  0.982879  \n149  1.294914 -0.919783  1.138177 -1.364116 -1.210402 -0.039644  \n\n[150 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>-1.670125</td>\n      <td>-0.586254</td>\n      <td>0.923595</td>\n      <td>1.981185</td>\n      <td>-0.810781</td>\n      <td>-0.612897</td>\n      <td>-0.384151</td>\n      <td>0.324537</td>\n      <td>-0.430475</td>\n      <td>-1.052111</td>\n      <td>1.781424</td>\n      <td>0.864654</td>\n      <td>-0.608422</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>-1.970387</td>\n      <td>-1.408850</td>\n      <td>0.494238</td>\n      <td>0.454064</td>\n      <td>-0.810781</td>\n      <td>0.289686</td>\n      <td>0.007108</td>\n      <td>0.486267</td>\n      <td>-0.255722</td>\n      <td>-0.845966</td>\n      <td>0.645535</td>\n      <td>-0.405789</td>\n      <td>-1.017432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.994702</td>\n      <td>0.377863</td>\n      <td>-0.221358</td>\n      <td>0.759488</td>\n      <td>-0.673205</td>\n      <td>-1.531598</td>\n      <td>-1.293826</td>\n      <td>0.405402</td>\n      <td>-0.972209</td>\n      <td>1.945577</td>\n      <td>-1.101987</td>\n      <td>-1.295098</td>\n      <td>-0.439067</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.394178</td>\n      <td>-0.542029</td>\n      <td>-0.793834</td>\n      <td>-0.767633</td>\n      <td>-0.398053</td>\n      <td>0.160746</td>\n      <td>0.183174</td>\n      <td>-0.726705</td>\n      <td>-0.413000</td>\n      <td>-0.472329</td>\n      <td>0.296030</td>\n      <td>0.243549</td>\n      <td>1.701840</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.381667</td>\n      <td>-0.320901</td>\n      <td>1.138274</td>\n      <td>-0.828718</td>\n      <td>0.152251</td>\n      <td>1.127799</td>\n      <td>1.200446</td>\n      <td>-0.403246</td>\n      <td>0.128735</td>\n      <td>0.408081</td>\n      <td>0.514470</td>\n      <td>0.342361</td>\n      <td>1.653909</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2</td>\n      <td>-0.594185</td>\n      <td>-0.533184</td>\n      <td>-1.366310</td>\n      <td>0.301352</td>\n      <td>-1.017145</td>\n      <td>-0.161605</td>\n      <td>-0.080925</td>\n      <td>-0.322381</td>\n      <td>-0.185821</td>\n      <td>-0.906092</td>\n      <td>0.383406</td>\n      <td>1.372831</td>\n      <td>-0.256930</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>2</td>\n      <td>0.444221</td>\n      <td>-1.231948</td>\n      <td>-0.006679</td>\n      <td>-0.767633</td>\n      <td>0.702554</td>\n      <td>0.370274</td>\n      <td>-0.687376</td>\n      <td>1.537509</td>\n      <td>-2.038203</td>\n      <td>-0.807314</td>\n      <td>0.296030</td>\n      <td>-0.942198</td>\n      <td>-0.007690</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2</td>\n      <td>-0.268901</td>\n      <td>0.970486</td>\n      <td>-1.366310</td>\n      <td>-1.073057</td>\n      <td>-1.361085</td>\n      <td>-1.080306</td>\n      <td>-0.736283</td>\n      <td>0.567132</td>\n      <td>-1.321715</td>\n      <td>-0.708537</td>\n      <td>-1.101987</td>\n      <td>-0.673993</td>\n      <td>-1.215545</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>1</td>\n      <td>1.395051</td>\n      <td>-0.276675</td>\n      <td>0.136440</td>\n      <td>-0.217869</td>\n      <td>0.221039</td>\n      <td>0.724860</td>\n      <td>0.897221</td>\n      <td>-0.322381</td>\n      <td>1.386956</td>\n      <td>0.493974</td>\n      <td>0.514470</td>\n      <td>0.215316</td>\n      <td>0.982879</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>3</td>\n      <td>0.907125</td>\n      <td>2.934102</td>\n      <td>0.315339</td>\n      <td>0.301352</td>\n      <td>-0.329265</td>\n      <td>-0.999718</td>\n      <td>-1.362297</td>\n      <td>1.294914</td>\n      <td>-0.919783</td>\n      <td>1.138177</td>\n      <td>-1.364116</td>\n      <td>-1.210402</td>\n      <td>-0.039644</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    1     14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n0   1  0.719462 -0.533184  0.351119  0.301352  1.115282  1.063329  0.760280   \n1   3 -0.243880  0.298257  0.422678  0.759488  0.840130 -1.322069 -0.628687   \n2   2 -1.019557 -0.780847  0.601577 -0.156784  0.289827 -0.661250 -0.247210   \n3   1  1.545182 -0.559719 -0.221358 -0.981430  1.252858  1.450150  0.975472   \n4   2 -1.394885  1.748856  0.100660  0.454064 -1.223509  0.902153  1.004817   \n5   2 -0.706784  1.854998  1.317173  2.133897  0.152251 -0.161605  0.124485   \n6   1  2.308349 -0.612790 -0.686495 -1.683906 -0.191689  0.805448  0.955909   \n7   3 -0.781849  1.324291  0.064881  0.454064 -0.810781 -0.000430 -1.059071   \n8   2 -0.331456 -0.515493 -0.292917  0.912200 -1.085933 -1.483245 -0.237429   \n9   2 -1.232243 -0.727776  0.208000  0.759488 -0.948357 -1.370422 -0.736283   \n10  3  0.581841  1.209304  0.852036  1.064913  0.771342 -0.967483 -1.059071   \n11  3  1.119811  2.394550 -0.471816  0.148640 -1.361085 -2.127947 -1.626396   \n12  1  1.757868 -0.409352  0.315339 -1.500651 -0.260477  0.321921  0.505962   \n13  2 -0.869426  0.740513 -0.543375 -0.462209 -0.810781  0.886036  0.965691   \n14  3  0.744483  0.218651  1.174054  1.523049  0.358615 -1.209247 -1.137323   \n15  2 -0.331456 -0.462423 -0.579155 -0.217869 -0.948357 -0.161605  0.515744   \n16  1  0.656907 -0.471268  1.030935 -0.156784  0.702554  0.080158  0.515744   \n17  1  0.819549  0.678597  0.708917 -1.317397  1.115282  0.644272  1.004817   \n18  2  0.857082 -0.957749 -1.580989 -0.462209 -0.398053 -0.322781 -0.208084   \n19  1  1.044746 -0.515493  0.208000 -1.683906  0.771342  2.546144  1.699300   \n20  2 -0.769338 -1.028510 -1.580989  0.026470 -1.498661 -0.306663 -0.002674   \n21  1  0.106426 -0.736621 -0.936953 -1.225769 -0.122901  0.160746  0.623340   \n22  1  1.520161 -0.506648  0.315339 -1.317397  0.840130  1.562973  1.356949   \n23  2 -0.706784  0.192116 -0.328697  0.759488 -0.673205 -0.999718 -0.159177   \n24  2 -0.431543 -0.860453 -1.223191 -0.828718  0.014675 -0.451721 -0.579780   \n25  2 -1.532505  0.307102  1.996988  0.148640  0.221039 -0.886895  0.026671   \n26  2 -1.494972 -0.179379  1.496072  2.744746 -0.535629 -0.274428  0.232081   \n\n         .28      2.29      5.64      1.04      3.92      1065  \n0  -1.292758  1.509284  0.515448  0.121278  0.610565  1.174602  \n1  -0.969299 -0.570277  2.469528 -2.063124 -1.591535 -0.870444  \n2   0.728861 -0.972209 -0.901797  2.174616 -0.518717 -1.269867  \n3  -0.807569  0.775321  0.571279 -0.053474  1.005814  0.695294  \n4  -1.211893  2.313147 -0.966218 -0.883547  1.471643 -1.189982  \n5   0.567132  0.216111 -1.275435 -0.140850  0.737609 -1.237913  \n6  -0.564975  0.687944  0.064506  0.558158  0.356477  0.934948  \n7   1.133185 -0.954734  1.116703 -1.713620 -1.436259 -0.742629  \n8   0.971455  0.076309 -0.751483 -0.315602 -0.250512 -0.844881  \n9   1.133185  0.076309 -0.622643  0.427094  0.074156 -0.966305  \n10  0.567132 -0.220771  2.417992 -0.446667 -1.464491 -0.183436  \n11  0.324537 -1.583845 -0.064334 -1.626244 -1.789159 -1.078144  \n12 -0.484110  0.687944  0.085980  0.296030  1.386947  1.717817  \n13  0.728861  2.138394 -1.180952  2.043552  0.328245 -1.106902  \n14  0.243672 -0.080969  1.550466 -0.927235 -1.125706 -0.007690  \n15 -0.807569  0.320963 -0.493803  0.907663  0.765842 -0.122724  \n16 -0.564975 -0.080969 -0.364962  0.645535  0.384709  1.094717  \n17 -1.535352  0.128735  0.021559  0.033902  1.076394  0.295871  \n18 -0.322381 -1.496468 -0.536749  1.213479 -0.194048 -0.391136  \n19 -0.322381  0.495716  0.859022  0.252342  0.935234  1.398279  \n20 -0.726705 -0.954734 -0.158817  0.732911  1.245786 -0.774582  \n21 -0.645840 -0.378049 -0.579696  0.995039  0.130620  0.855063  \n22 -0.160651  0.670469  0.730182  0.427094  0.356477  2.229079  \n23  2.427021 -0.290673 -1.009164 -0.402979  0.991698 -1.397682  \n24  1.375779 -1.688697  0.300714  0.121278 -1.422143 -0.966305  \n25  1.941833 -0.937259 -0.536749  1.213479 -0.137584 -0.464630  \n26  1.780103  0.303488 -0.880324  0.077590 -0.222280 -0.915179  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.719462</td>\n      <td>-0.533184</td>\n      <td>0.351119</td>\n      <td>0.301352</td>\n      <td>1.115282</td>\n      <td>1.063329</td>\n      <td>0.760280</td>\n      <td>-1.292758</td>\n      <td>1.509284</td>\n      <td>0.515448</td>\n      <td>0.121278</td>\n      <td>0.610565</td>\n      <td>1.174602</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>-0.243880</td>\n      <td>0.298257</td>\n      <td>0.422678</td>\n      <td>0.759488</td>\n      <td>0.840130</td>\n      <td>-1.322069</td>\n      <td>-0.628687</td>\n      <td>-0.969299</td>\n      <td>-0.570277</td>\n      <td>2.469528</td>\n      <td>-2.063124</td>\n      <td>-1.591535</td>\n      <td>-0.870444</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-1.019557</td>\n      <td>-0.780847</td>\n      <td>0.601577</td>\n      <td>-0.156784</td>\n      <td>0.289827</td>\n      <td>-0.661250</td>\n      <td>-0.247210</td>\n      <td>0.728861</td>\n      <td>-0.972209</td>\n      <td>-0.901797</td>\n      <td>2.174616</td>\n      <td>-0.518717</td>\n      <td>-1.269867</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1.545182</td>\n      <td>-0.559719</td>\n      <td>-0.221358</td>\n      <td>-0.981430</td>\n      <td>1.252858</td>\n      <td>1.450150</td>\n      <td>0.975472</td>\n      <td>-0.807569</td>\n      <td>0.775321</td>\n      <td>0.571279</td>\n      <td>-0.053474</td>\n      <td>1.005814</td>\n      <td>0.695294</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>-1.394885</td>\n      <td>1.748856</td>\n      <td>0.100660</td>\n      <td>0.454064</td>\n      <td>-1.223509</td>\n      <td>0.902153</td>\n      <td>1.004817</td>\n      <td>-1.211893</td>\n      <td>2.313147</td>\n      <td>-0.966218</td>\n      <td>-0.883547</td>\n      <td>1.471643</td>\n      <td>-1.189982</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>-0.706784</td>\n      <td>1.854998</td>\n      <td>1.317173</td>\n      <td>2.133897</td>\n      <td>0.152251</td>\n      <td>-0.161605</td>\n      <td>0.124485</td>\n      <td>0.567132</td>\n      <td>0.216111</td>\n      <td>-1.275435</td>\n      <td>-0.140850</td>\n      <td>0.737609</td>\n      <td>-1.237913</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>2.308349</td>\n      <td>-0.612790</td>\n      <td>-0.686495</td>\n      <td>-1.683906</td>\n      <td>-0.191689</td>\n      <td>0.805448</td>\n      <td>0.955909</td>\n      <td>-0.564975</td>\n      <td>0.687944</td>\n      <td>0.064506</td>\n      <td>0.558158</td>\n      <td>0.356477</td>\n      <td>0.934948</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>-0.781849</td>\n      <td>1.324291</td>\n      <td>0.064881</td>\n      <td>0.454064</td>\n      <td>-0.810781</td>\n      <td>-0.000430</td>\n      <td>-1.059071</td>\n      <td>1.133185</td>\n      <td>-0.954734</td>\n      <td>1.116703</td>\n      <td>-1.713620</td>\n      <td>-1.436259</td>\n      <td>-0.742629</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>-0.331456</td>\n      <td>-0.515493</td>\n      <td>-0.292917</td>\n      <td>0.912200</td>\n      <td>-1.085933</td>\n      <td>-1.483245</td>\n      <td>-0.237429</td>\n      <td>0.971455</td>\n      <td>0.076309</td>\n      <td>-0.751483</td>\n      <td>-0.315602</td>\n      <td>-0.250512</td>\n      <td>-0.844881</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>-1.232243</td>\n      <td>-0.727776</td>\n      <td>0.208000</td>\n      <td>0.759488</td>\n      <td>-0.948357</td>\n      <td>-1.370422</td>\n      <td>-0.736283</td>\n      <td>1.133185</td>\n      <td>0.076309</td>\n      <td>-0.622643</td>\n      <td>0.427094</td>\n      <td>0.074156</td>\n      <td>-0.966305</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>0.581841</td>\n      <td>1.209304</td>\n      <td>0.852036</td>\n      <td>1.064913</td>\n      <td>0.771342</td>\n      <td>-0.967483</td>\n      <td>-1.059071</td>\n      <td>0.567132</td>\n      <td>-0.220771</td>\n      <td>2.417992</td>\n      <td>-0.446667</td>\n      <td>-1.464491</td>\n      <td>-0.183436</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>1.119811</td>\n      <td>2.394550</td>\n      <td>-0.471816</td>\n      <td>0.148640</td>\n      <td>-1.361085</td>\n      <td>-2.127947</td>\n      <td>-1.626396</td>\n      <td>0.324537</td>\n      <td>-1.583845</td>\n      <td>-0.064334</td>\n      <td>-1.626244</td>\n      <td>-1.789159</td>\n      <td>-1.078144</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>1.757868</td>\n      <td>-0.409352</td>\n      <td>0.315339</td>\n      <td>-1.500651</td>\n      <td>-0.260477</td>\n      <td>0.321921</td>\n      <td>0.505962</td>\n      <td>-0.484110</td>\n      <td>0.687944</td>\n      <td>0.085980</td>\n      <td>0.296030</td>\n      <td>1.386947</td>\n      <td>1.717817</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>-0.869426</td>\n      <td>0.740513</td>\n      <td>-0.543375</td>\n      <td>-0.462209</td>\n      <td>-0.810781</td>\n      <td>0.886036</td>\n      <td>0.965691</td>\n      <td>0.728861</td>\n      <td>2.138394</td>\n      <td>-1.180952</td>\n      <td>2.043552</td>\n      <td>0.328245</td>\n      <td>-1.106902</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>0.744483</td>\n      <td>0.218651</td>\n      <td>1.174054</td>\n      <td>1.523049</td>\n      <td>0.358615</td>\n      <td>-1.209247</td>\n      <td>-1.137323</td>\n      <td>0.243672</td>\n      <td>-0.080969</td>\n      <td>1.550466</td>\n      <td>-0.927235</td>\n      <td>-1.125706</td>\n      <td>-0.007690</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n      <td>-0.331456</td>\n      <td>-0.462423</td>\n      <td>-0.579155</td>\n      <td>-0.217869</td>\n      <td>-0.948357</td>\n      <td>-0.161605</td>\n      <td>0.515744</td>\n      <td>-0.807569</td>\n      <td>0.320963</td>\n      <td>-0.493803</td>\n      <td>0.907663</td>\n      <td>0.765842</td>\n      <td>-0.122724</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>0.656907</td>\n      <td>-0.471268</td>\n      <td>1.030935</td>\n      <td>-0.156784</td>\n      <td>0.702554</td>\n      <td>0.080158</td>\n      <td>0.515744</td>\n      <td>-0.564975</td>\n      <td>-0.080969</td>\n      <td>-0.364962</td>\n      <td>0.645535</td>\n      <td>0.384709</td>\n      <td>1.094717</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>0.819549</td>\n      <td>0.678597</td>\n      <td>0.708917</td>\n      <td>-1.317397</td>\n      <td>1.115282</td>\n      <td>0.644272</td>\n      <td>1.004817</td>\n      <td>-1.535352</td>\n      <td>0.128735</td>\n      <td>0.021559</td>\n      <td>0.033902</td>\n      <td>1.076394</td>\n      <td>0.295871</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>0.857082</td>\n      <td>-0.957749</td>\n      <td>-1.580989</td>\n      <td>-0.462209</td>\n      <td>-0.398053</td>\n      <td>-0.322781</td>\n      <td>-0.208084</td>\n      <td>-0.322381</td>\n      <td>-1.496468</td>\n      <td>-0.536749</td>\n      <td>1.213479</td>\n      <td>-0.194048</td>\n      <td>-0.391136</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>1.044746</td>\n      <td>-0.515493</td>\n      <td>0.208000</td>\n      <td>-1.683906</td>\n      <td>0.771342</td>\n      <td>2.546144</td>\n      <td>1.699300</td>\n      <td>-0.322381</td>\n      <td>0.495716</td>\n      <td>0.859022</td>\n      <td>0.252342</td>\n      <td>0.935234</td>\n      <td>1.398279</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2</td>\n      <td>-0.769338</td>\n      <td>-1.028510</td>\n      <td>-1.580989</td>\n      <td>0.026470</td>\n      <td>-1.498661</td>\n      <td>-0.306663</td>\n      <td>-0.002674</td>\n      <td>-0.726705</td>\n      <td>-0.954734</td>\n      <td>-0.158817</td>\n      <td>0.732911</td>\n      <td>1.245786</td>\n      <td>-0.774582</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n      <td>0.106426</td>\n      <td>-0.736621</td>\n      <td>-0.936953</td>\n      <td>-1.225769</td>\n      <td>-0.122901</td>\n      <td>0.160746</td>\n      <td>0.623340</td>\n      <td>-0.645840</td>\n      <td>-0.378049</td>\n      <td>-0.579696</td>\n      <td>0.995039</td>\n      <td>0.130620</td>\n      <td>0.855063</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1</td>\n      <td>1.520161</td>\n      <td>-0.506648</td>\n      <td>0.315339</td>\n      <td>-1.317397</td>\n      <td>0.840130</td>\n      <td>1.562973</td>\n      <td>1.356949</td>\n      <td>-0.160651</td>\n      <td>0.670469</td>\n      <td>0.730182</td>\n      <td>0.427094</td>\n      <td>0.356477</td>\n      <td>2.229079</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n      <td>-0.706784</td>\n      <td>0.192116</td>\n      <td>-0.328697</td>\n      <td>0.759488</td>\n      <td>-0.673205</td>\n      <td>-0.999718</td>\n      <td>-0.159177</td>\n      <td>2.427021</td>\n      <td>-0.290673</td>\n      <td>-1.009164</td>\n      <td>-0.402979</td>\n      <td>0.991698</td>\n      <td>-1.397682</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2</td>\n      <td>-0.431543</td>\n      <td>-0.860453</td>\n      <td>-1.223191</td>\n      <td>-0.828718</td>\n      <td>0.014675</td>\n      <td>-0.451721</td>\n      <td>-0.579780</td>\n      <td>1.375779</td>\n      <td>-1.688697</td>\n      <td>0.300714</td>\n      <td>0.121278</td>\n      <td>-1.422143</td>\n      <td>-0.966305</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2</td>\n      <td>-1.532505</td>\n      <td>0.307102</td>\n      <td>1.996988</td>\n      <td>0.148640</td>\n      <td>0.221039</td>\n      <td>-0.886895</td>\n      <td>0.026671</td>\n      <td>1.941833</td>\n      <td>-0.937259</td>\n      <td>-0.536749</td>\n      <td>1.213479</td>\n      <td>-0.137584</td>\n      <td>-0.464630</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2</td>\n      <td>-1.494972</td>\n      <td>-0.179379</td>\n      <td>1.496072</td>\n      <td>2.744746</td>\n      <td>-0.535629</td>\n      <td>-0.274428</td>\n      <td>0.232081</td>\n      <td>1.780103</td>\n      <td>0.303488</td>\n      <td>-0.880324</td>\n      <td>0.077590</td>\n      <td>-0.222280</td>\n      <td>-0.915179</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "train_wine.iloc[:, 1:], train_avg, train_stdev = ml.normalize(train_wine.iloc[:, 1:])\n",
    "test_wine.iloc[:, 1:] = (test_wine.iloc[:, 1:] - train_avg) / train_stdev\n",
    "\n",
    "display(train_wine)\n",
    "display(test_wine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:36:04.229870Z",
     "start_time": "2023-10-12T22:36:04.204335Z"
    }
   },
   "id": "23c8f7420ee23e5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "       14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n",
      "0   0.719462 -0.533184  0.351119  0.301352  1.115282  1.063329  0.760280   \n",
      "1  -0.243880  0.298257  0.422678  0.759488  0.840130 -1.322069 -0.628687   \n",
      "2  -1.019557 -0.780847  0.601577 -0.156784  0.289827 -0.661250 -0.247210   \n",
      "3   1.545182 -0.559719 -0.221358 -0.981430  1.252858  1.450150  0.975472   \n",
      "4  -1.394885  1.748856  0.100660  0.454064 -1.223509  0.902153  1.004817   \n",
      "5  -0.706784  1.854998  1.317173  2.133897  0.152251 -0.161605  0.124485   \n",
      "6   2.308349 -0.612790 -0.686495 -1.683906 -0.191689  0.805448  0.955909   \n",
      "7  -0.781849  1.324291  0.064881  0.454064 -0.810781 -0.000430 -1.059071   \n",
      "8  -0.331456 -0.515493 -0.292917  0.912200 -1.085933 -1.483245 -0.237429   \n",
      "9  -1.232243 -0.727776  0.208000  0.759488 -0.948357 -1.370422 -0.736283   \n",
      "10  0.581841  1.209304  0.852036  1.064913  0.771342 -0.967483 -1.059071   \n",
      "11  1.119811  2.394550 -0.471816  0.148640 -1.361085 -2.127947 -1.626396   \n",
      "12  1.757868 -0.409352  0.315339 -1.500651 -0.260477  0.321921  0.505962   \n",
      "13 -0.869426  0.740513 -0.543375 -0.462209 -0.810781  0.886036  0.965691   \n",
      "14  0.744483  0.218651  1.174054  1.523049  0.358615 -1.209247 -1.137323   \n",
      "15 -0.331456 -0.462423 -0.579155 -0.217869 -0.948357 -0.161605  0.515744   \n",
      "16  0.656907 -0.471268  1.030935 -0.156784  0.702554  0.080158  0.515744   \n",
      "17  0.819549  0.678597  0.708917 -1.317397  1.115282  0.644272  1.004817   \n",
      "18  0.857082 -0.957749 -1.580989 -0.462209 -0.398053 -0.322781 -0.208084   \n",
      "19  1.044746 -0.515493  0.208000 -1.683906  0.771342  2.546144  1.699300   \n",
      "20 -0.769338 -1.028510 -1.580989  0.026470 -1.498661 -0.306663 -0.002674   \n",
      "21  0.106426 -0.736621 -0.936953 -1.225769 -0.122901  0.160746  0.623340   \n",
      "22  1.520161 -0.506648  0.315339 -1.317397  0.840130  1.562973  1.356949   \n",
      "23 -0.706784  0.192116 -0.328697  0.759488 -0.673205 -0.999718 -0.159177   \n",
      "24 -0.431543 -0.860453 -1.223191 -0.828718  0.014675 -0.451721 -0.579780   \n",
      "25 -1.532505  0.307102  1.996988  0.148640  0.221039 -0.886895  0.026671   \n",
      "26 -1.494972 -0.179379  1.496072  2.744746 -0.535629 -0.274428  0.232081   \n",
      "\n",
      "         .28      2.29      5.64      1.04      3.92      1065  \n",
      "0  -1.292758  1.509284  0.515448  0.121278  0.610565  1.174602  \n",
      "1  -0.969299 -0.570277  2.469528 -2.063124 -1.591535 -0.870444  \n",
      "2   0.728861 -0.972209 -0.901797  2.174616 -0.518717 -1.269867  \n",
      "3  -0.807569  0.775321  0.571279 -0.053474  1.005814  0.695294  \n",
      "4  -1.211893  2.313147 -0.966218 -0.883547  1.471643 -1.189982  \n",
      "5   0.567132  0.216111 -1.275435 -0.140850  0.737609 -1.237913  \n",
      "6  -0.564975  0.687944  0.064506  0.558158  0.356477  0.934948  \n",
      "7   1.133185 -0.954734  1.116703 -1.713620 -1.436259 -0.742629  \n",
      "8   0.971455  0.076309 -0.751483 -0.315602 -0.250512 -0.844881  \n",
      "9   1.133185  0.076309 -0.622643  0.427094  0.074156 -0.966305  \n",
      "10  0.567132 -0.220771  2.417992 -0.446667 -1.464491 -0.183436  \n",
      "11  0.324537 -1.583845 -0.064334 -1.626244 -1.789159 -1.078144  \n",
      "12 -0.484110  0.687944  0.085980  0.296030  1.386947  1.717817  \n",
      "13  0.728861  2.138394 -1.180952  2.043552  0.328245 -1.106902  \n",
      "14  0.243672 -0.080969  1.550466 -0.927235 -1.125706 -0.007690  \n",
      "15 -0.807569  0.320963 -0.493803  0.907663  0.765842 -0.122724  \n",
      "16 -0.564975 -0.080969 -0.364962  0.645535  0.384709  1.094717  \n",
      "17 -1.535352  0.128735  0.021559  0.033902  1.076394  0.295871  \n",
      "18 -0.322381 -1.496468 -0.536749  1.213479 -0.194048 -0.391136  \n",
      "19 -0.322381  0.495716  0.859022  0.252342  0.935234  1.398279  \n",
      "20 -0.726705 -0.954734 -0.158817  0.732911  1.245786 -0.774582  \n",
      "21 -0.645840 -0.378049 -0.579696  0.995039  0.130620  0.855063  \n",
      "22 -0.160651  0.670469  0.730182  0.427094  0.356477  2.229079  \n",
      "23  2.427021 -0.290673 -1.009164 -0.402979  0.991698 -1.397682  \n",
      "24  1.375779 -1.688697  0.300714  0.121278 -1.422143 -0.966305  \n",
      "25  1.941833 -0.937259 -0.536749  1.213479 -0.137584 -0.464630  \n",
      "26  1.780103  0.303488 -0.880324  0.077590 -0.222280 -0.915179  \n",
      "Hidden Layer: \n",
      "              0         1         2         3         4         5         6  \\\n",
      "14.23 -1.480083 -0.478042  0.168223 -0.749854  0.978503 -1.483072  1.129722   \n",
      "1.71  -0.103360  1.217184  0.318092 -1.070889  0.174361  1.485543 -0.032715   \n",
      "2.43  -0.955197  0.589428 -1.621141  1.167019  1.326341  0.667957  1.279303   \n",
      "15.6   0.795196 -0.230752  1.425571 -1.230179  0.055548 -0.690380 -0.126637   \n",
      "127   -1.799502 -0.504788 -0.107264  1.271140  0.909480 -1.436993 -0.036409   \n",
      "2.8    0.016836  0.913795  0.758558 -0.872457 -0.231441  0.052443  0.040462   \n",
      "3.06  -0.558925 -0.864083  1.198953  0.093943  0.724469  0.211875 -0.695081   \n",
      ".28   -0.947582  0.200086 -0.330995 -0.647710  0.183551  2.890509 -0.173849   \n",
      "2.29  -1.206030  0.531531  0.795257 -0.113118 -1.323080  0.005619 -1.596426   \n",
      "5.64  -0.271523  0.863883 -0.207677  1.009591  0.225859  0.390871  2.985123   \n",
      "1.04   1.045413  0.123085  1.756019  1.245890  0.102826 -0.807977 -1.131755   \n",
      "3.92   0.812316 -0.348492  2.142110  1.893564 -1.573712 -0.535655  1.482545   \n",
      "1065  -0.021481  0.150173  0.717350  0.064108  0.222252 -2.329616  0.192711   \n",
      "\n",
      "              7  \n",
      "14.23 -1.480914  \n",
      "1.71  -0.916899  \n",
      "2.43   0.393942  \n",
      "15.6   1.222607  \n",
      "127    0.813399  \n",
      "2.8   -1.006355  \n",
      "3.06  -0.132174  \n",
      ".28    1.026976  \n",
      "2.29  -0.129117  \n",
      "5.64   0.670370  \n",
      "1.04   1.139177  \n",
      "3.92  -0.745487  \n",
      "1065  -2.074475  \n",
      "Output: \n",
      "[[0.95156253 0.06512037 0.05216323]\n",
      " [0.08837293 0.05705229 0.84606562]\n",
      " [0.00643019 0.81906805 0.16776304]\n",
      " [0.97130314 0.03920623 0.06405717]\n",
      " [0.04734227 0.77657339 0.0331742 ]\n",
      " [0.00672559 0.80950535 0.12476363]\n",
      " [0.94157975 0.09712006 0.07452164]\n",
      " [0.03088196 0.11881767 0.82892483]\n",
      " [0.00246028 0.91435121 0.29459702]\n",
      " [0.00284246 0.90284939 0.14006076]\n",
      " [0.06593342 0.05064894 0.87624289]\n",
      " [0.1126078  0.09001865 0.79616111]\n",
      " [0.96651947 0.04479428 0.05146369]\n",
      " [0.00639393 0.87520173 0.03420693]\n",
      " [0.12655779 0.0522786  0.88273711]\n",
      " [0.30598991 0.80352067 0.01206899]\n",
      " [0.94491518 0.06577703 0.11147094]\n",
      " [0.97227873 0.02783906 0.10682525]\n",
      " [0.24093771 0.7566627  0.06821962]\n",
      " [0.97480674 0.02318174 0.08572168]\n",
      " [0.10652223 0.80402141 0.01538091]\n",
      " [0.62255441 0.67713837 0.01671083]\n",
      " [0.97677689 0.02267466 0.10969114]\n",
      " [0.00347096 0.91512937 0.11972557]\n",
      " [0.00736881 0.64071412 0.64296593]\n",
      " [0.01140765 0.49852888 0.36848695]\n",
      " [0.00399821 0.85399022 0.24438922]]\n",
      "[[0.95156253 0.06512037 0.05216323]\n",
      " [0.08837293 0.05705229 0.84606562]\n",
      " [0.00643019 0.81906805 0.16776304]\n",
      " [0.97130314 0.03920623 0.06405717]\n",
      " [0.04734227 0.77657339 0.0331742 ]\n",
      " [0.00672559 0.80950535 0.12476363]\n",
      " [0.94157975 0.09712006 0.07452164]\n",
      " [0.03088196 0.11881767 0.82892483]\n",
      " [0.00246028 0.91435121 0.29459702]\n",
      " [0.00284246 0.90284939 0.14006076]\n",
      " [0.06593342 0.05064894 0.87624289]\n",
      " [0.1126078  0.09001865 0.79616111]\n",
      " [0.96651947 0.04479428 0.05146369]\n",
      " [0.00639393 0.87520173 0.03420693]\n",
      " [0.12655779 0.0522786  0.88273711]\n",
      " [0.30598991 0.80352067 0.01206899]\n",
      " [0.94491518 0.06577703 0.11147094]\n",
      " [0.97227873 0.02783906 0.10682525]\n",
      " [0.24093771 0.7566627  0.06821962]\n",
      " [0.97480674 0.02318174 0.08572168]\n",
      " [0.10652223 0.80402141 0.01538091]\n",
      " [0.62255441 0.67713837 0.01671083]\n",
      " [0.97677689 0.02267466 0.10969114]\n",
      " [0.00347096 0.91512937 0.11972557]\n",
      " [0.00736881 0.64071412 0.64296593]\n",
      " [0.01140765 0.49852888 0.36848695]\n",
      " [0.00399821 0.85399022 0.24438922]]\n"
     ]
    }
   ],
   "source": [
    "# Train the multi-class neural network on the wine dataset\n",
    "train_x = train_wine.iloc[:, 1:]\n",
    "train_y = train_wine.iloc[:, 0]\n",
    "\n",
    "# convert the labels to one-hot labels\n",
    "# subtract 1 from the labels to make them 0, 1, 2 instead of 1, 2, 3\n",
    "train_y = train_y - 1\n",
    "train_y = np.eye(3)[train_y]\n",
    "\n",
    "test_x = test_wine.iloc[:, 1:]\n",
    "test_y = test_wine.iloc[:, 0]\n",
    "\n",
    "mnn = ml.NeuralNetwork(13, 8, 3)\n",
    "mnn.train(train_x, train_y)\n",
    "mnn_pred = mnn.predict(test_x)\n",
    "print(mnn_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:36:04.395503Z",
     "start_time": "2023-10-12T22:36:04.221722Z"
    }
   },
   "id": "bffdbc58851a8306"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Neural Network Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Test Labels: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Accuracy: 0.9506172839506172\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the multi-class neural network and the test labels\n",
    "mnm_pred_idx = np.argmax(mnn_pred, axis=1)\n",
    "mnm_pred_idx = np.eye(3)[mnm_pred_idx]\n",
    "\n",
    "test_y_idx = test_y - 1\n",
    "test_y_idx = np.eye(3)[test_y_idx]\n",
    "\n",
    "print(\"Multi-class Neural Network Output: \\n\" + str(mnm_pred_idx))\n",
    "print(\"Test Labels: \\n\" + str(test_y_idx))\n",
    "\n",
    "accuracy = np.sum(mnm_pred_idx == test_y_idx) / len(test_y_idx) / 3\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:36:04.398690Z",
     "start_time": "2023-10-12T22:36:04.396035Z"
    }
   },
   "id": "42b3fb162df31812"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d900294fdf284d79"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Input Data:        14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n",
      "0   0.719462 -0.533184  0.351119  0.301352  1.115282  1.063329  0.760280   \n",
      "1  -0.243880  0.298257  0.422678  0.759488  0.840130 -1.322069 -0.628687   \n",
      "2  -1.019557 -0.780847  0.601577 -0.156784  0.289827 -0.661250 -0.247210   \n",
      "3   1.545182 -0.559719 -0.221358 -0.981430  1.252858  1.450150  0.975472   \n",
      "4  -1.394885  1.748856  0.100660  0.454064 -1.223509  0.902153  1.004817   \n",
      "5  -0.706784  1.854998  1.317173  2.133897  0.152251 -0.161605  0.124485   \n",
      "6   2.308349 -0.612790 -0.686495 -1.683906 -0.191689  0.805448  0.955909   \n",
      "7  -0.781849  1.324291  0.064881  0.454064 -0.810781 -0.000430 -1.059071   \n",
      "8  -0.331456 -0.515493 -0.292917  0.912200 -1.085933 -1.483245 -0.237429   \n",
      "9  -1.232243 -0.727776  0.208000  0.759488 -0.948357 -1.370422 -0.736283   \n",
      "10  0.581841  1.209304  0.852036  1.064913  0.771342 -0.967483 -1.059071   \n",
      "11  1.119811  2.394550 -0.471816  0.148640 -1.361085 -2.127947 -1.626396   \n",
      "12  1.757868 -0.409352  0.315339 -1.500651 -0.260477  0.321921  0.505962   \n",
      "13 -0.869426  0.740513 -0.543375 -0.462209 -0.810781  0.886036  0.965691   \n",
      "14  0.744483  0.218651  1.174054  1.523049  0.358615 -1.209247 -1.137323   \n",
      "15 -0.331456 -0.462423 -0.579155 -0.217869 -0.948357 -0.161605  0.515744   \n",
      "16  0.656907 -0.471268  1.030935 -0.156784  0.702554  0.080158  0.515744   \n",
      "17  0.819549  0.678597  0.708917 -1.317397  1.115282  0.644272  1.004817   \n",
      "18  0.857082 -0.957749 -1.580989 -0.462209 -0.398053 -0.322781 -0.208084   \n",
      "19  1.044746 -0.515493  0.208000 -1.683906  0.771342  2.546144  1.699300   \n",
      "20 -0.769338 -1.028510 -1.580989  0.026470 -1.498661 -0.306663 -0.002674   \n",
      "21  0.106426 -0.736621 -0.936953 -1.225769 -0.122901  0.160746  0.623340   \n",
      "22  1.520161 -0.506648  0.315339 -1.317397  0.840130  1.562973  1.356949   \n",
      "23 -0.706784  0.192116 -0.328697  0.759488 -0.673205 -0.999718 -0.159177   \n",
      "24 -0.431543 -0.860453 -1.223191 -0.828718  0.014675 -0.451721 -0.579780   \n",
      "25 -1.532505  0.307102  1.996988  0.148640  0.221039 -0.886895  0.026671   \n",
      "26 -1.494972 -0.179379  1.496072  2.744746 -0.535629 -0.274428  0.232081   \n",
      "\n",
      "         .28      2.29      5.64      1.04      3.92      1065  \n",
      "0  -1.292758  1.509284  0.515448  0.121278  0.610565  1.174602  \n",
      "1  -0.969299 -0.570277  2.469528 -2.063124 -1.591535 -0.870444  \n",
      "2   0.728861 -0.972209 -0.901797  2.174616 -0.518717 -1.269867  \n",
      "3  -0.807569  0.775321  0.571279 -0.053474  1.005814  0.695294  \n",
      "4  -1.211893  2.313147 -0.966218 -0.883547  1.471643 -1.189982  \n",
      "5   0.567132  0.216111 -1.275435 -0.140850  0.737609 -1.237913  \n",
      "6  -0.564975  0.687944  0.064506  0.558158  0.356477  0.934948  \n",
      "7   1.133185 -0.954734  1.116703 -1.713620 -1.436259 -0.742629  \n",
      "8   0.971455  0.076309 -0.751483 -0.315602 -0.250512 -0.844881  \n",
      "9   1.133185  0.076309 -0.622643  0.427094  0.074156 -0.966305  \n",
      "10  0.567132 -0.220771  2.417992 -0.446667 -1.464491 -0.183436  \n",
      "11  0.324537 -1.583845 -0.064334 -1.626244 -1.789159 -1.078144  \n",
      "12 -0.484110  0.687944  0.085980  0.296030  1.386947  1.717817  \n",
      "13  0.728861  2.138394 -1.180952  2.043552  0.328245 -1.106902  \n",
      "14  0.243672 -0.080969  1.550466 -0.927235 -1.125706 -0.007690  \n",
      "15 -0.807569  0.320963 -0.493803  0.907663  0.765842 -0.122724  \n",
      "16 -0.564975 -0.080969 -0.364962  0.645535  0.384709  1.094717  \n",
      "17 -1.535352  0.128735  0.021559  0.033902  1.076394  0.295871  \n",
      "18 -0.322381 -1.496468 -0.536749  1.213479 -0.194048 -0.391136  \n",
      "19 -0.322381  0.495716  0.859022  0.252342  0.935234  1.398279  \n",
      "20 -0.726705 -0.954734 -0.158817  0.732911  1.245786 -0.774582  \n",
      "21 -0.645840 -0.378049 -0.579696  0.995039  0.130620  0.855063  \n",
      "22 -0.160651  0.670469  0.730182  0.427094  0.356477  2.229079  \n",
      "23  2.427021 -0.290673 -1.009164 -0.402979  0.991698 -1.397682  \n",
      "24  1.375779 -1.688697  0.300714  0.121278 -1.422143 -0.966305  \n",
      "25  1.941833 -0.937259 -0.536749  1.213479 -0.137584 -0.464630  \n",
      "26  1.780103  0.303488 -0.880324  0.077590 -0.222280 -0.915179  \n",
      "Predicted data based on trained weights: [[0.94077796 0.06614231 0.06001142]\n",
      " [0.00323053 0.01712052 0.9735731 ]\n",
      " [0.01170022 0.9776048  0.03730616]\n",
      " [0.9698753  0.02856778 0.02962198]\n",
      " [0.11859466 0.96278703 0.01073743]\n",
      " [0.03811225 0.871604   0.19802241]\n",
      " [0.98596525 0.02946885 0.02943512]\n",
      " [0.00740961 0.16698396 0.9484001 ]\n",
      " [0.01304091 0.8177121  0.23357138]\n",
      " [0.00941305 0.9324539  0.14220065]\n",
      " [0.05735328 0.0057451  0.9687742 ]\n",
      " [0.02030024 0.04586095 0.98696905]\n",
      " [0.9975829  0.00653908 0.07678934]\n",
      " [0.14861545 0.9933191  0.00101021]\n",
      " [0.10573055 0.00924365 0.9604686 ]\n",
      " [0.17858559 0.8651638  0.02315792]\n",
      " [0.9127341  0.08263215 0.15018357]\n",
      " [0.9376526  0.04767548 0.06251058]\n",
      " [0.07930149 0.7891816  0.08059123]\n",
      " [0.98940134 0.02332347 0.02156803]\n",
      " [0.01016359 0.96774715 0.03148519]\n",
      " [0.6191921  0.5618034  0.0378476 ]\n",
      " [0.9967184  0.00512356 0.08085269]\n",
      " [0.01137274 0.8876582  0.09902211]\n",
      " [0.00451825 0.73175347 0.35370365]\n",
      " [0.04415454 0.72181493 0.25124744]\n",
      " [0.0044357  0.96125376 0.21821173]]\n",
      "Weights: [<tf.Variable 'autoencoder_tf_1/dense_2/kernel:0' shape=(13, 8) dtype=float32, numpy=\n",
      "array([[ 0.4079198 , -0.34194317, -0.31130096,  0.21401645,  0.36101267,\n",
      "         0.2062036 , -0.8700866 , -0.17021158],\n",
      "       [-0.12882203,  0.1166409 , -0.35245907,  0.0054848 ,  0.41453743,\n",
      "         0.02446196, -0.13778819,  0.09332938],\n",
      "       [ 0.4055446 ,  0.32565567, -0.05055913, -0.28409958,  0.07850517,\n",
      "        -0.36701477, -0.5134213 ,  0.35325113],\n",
      "       [-0.39052263, -0.00290168, -0.45787278,  0.40376604,  0.01382189,\n",
      "         0.63147867,  0.5474439 , -0.07090343],\n",
      "       [ 0.02888879, -0.01550576,  0.12519391,  0.10833456,  0.21513785,\n",
      "        -0.21429145, -0.25777704,  0.41936347],\n",
      "       [ 0.212521  , -0.35291326,  0.25856823, -0.26119754,  0.5929103 ,\n",
      "         0.48349828,  0.29638895,  0.18774733],\n",
      "       [-0.2280323 , -0.12024523,  0.48142964, -0.13658434, -0.22865409,\n",
      "        -0.18698345,  0.05100664,  0.30632108],\n",
      "       [-0.08386073,  0.35695627,  0.13973212,  0.12800148,  0.00210727,\n",
      "        -0.37527457, -0.17688654, -0.25869682],\n",
      "       [ 0.12214828, -0.37590146,  0.39259854,  0.0893683 ,  0.1323398 ,\n",
      "        -0.29931992, -0.16932543, -0.48151144],\n",
      "       [-0.22565612,  0.22536886, -0.42459056,  0.12599452, -0.44260445,\n",
      "         0.2500788 , -0.5647375 , -0.09747155],\n",
      "       [ 0.39301825, -0.248974  ,  0.48379353,  0.37770066, -0.13833128,\n",
      "         0.41997382,  0.4308633 , -0.42563194],\n",
      "       [ 0.6080126 , -0.1514187 ,  0.47027633,  0.4837973 ,  0.34277278,\n",
      "         0.1982114 , -0.24196406,  0.3065113 ],\n",
      "       [ 0.5914096 ,  0.13715184, -0.5094994 ,  0.24260423,  0.30225784,\n",
      "        -0.40348583, -0.86005676, -0.60822   ]], dtype=float32)>, <tf.Variable 'autoencoder_tf_1/dense_2/bias:0' shape=(8,) dtype=float32, numpy=\n",
      "array([-0.4559438 , -0.15897992,  0.5674288 ,  0.14598863, -0.22250126,\n",
      "        0.17823997, -0.30301186,  0.02961378], dtype=float32)>, <tf.Variable 'autoencoder_tf_1/dense_3/kernel:0' shape=(8, 3) dtype=float32, numpy=\n",
      "array([[ 1.1091976 ,  0.1636072 ,  0.12890513],\n",
      "       [-0.459174  , -0.21670668,  0.5927038 ],\n",
      "       [-0.3013532 ,  0.21094818, -1.0907711 ],\n",
      "       [-0.23436353,  0.03644962, -0.198399  ],\n",
      "       [ 0.73422056, -0.00454588, -0.01221853],\n",
      "       [-0.4758723 , -0.2838934 ,  0.01694352],\n",
      "       [-0.30302307,  1.4084105 , -0.02258333],\n",
      "       [-0.32309544, -0.06835011,  0.07184685]], dtype=float32)>, <tf.Variable 'autoencoder_tf_1/dense_3/bias:0' shape=(3,) dtype=float32, numpy=array([-0.30101925, -0.25325382, -0.598261  ], dtype=float32)>]\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "[[0.94077796 0.06614231 0.06001142]\n",
      " [0.00323053 0.01712052 0.9735731 ]\n",
      " [0.01170022 0.9776048  0.03730616]\n",
      " [0.9698753  0.02856778 0.02962198]\n",
      " [0.11859466 0.96278703 0.01073743]\n",
      " [0.03811225 0.871604   0.19802241]\n",
      " [0.98596525 0.02946885 0.02943512]\n",
      " [0.00740961 0.16698396 0.9484001 ]\n",
      " [0.01304091 0.8177121  0.23357138]\n",
      " [0.00941305 0.9324539  0.14220065]\n",
      " [0.05735328 0.0057451  0.9687742 ]\n",
      " [0.02030024 0.04586095 0.98696905]\n",
      " [0.9975829  0.00653908 0.07678934]\n",
      " [0.14861545 0.9933191  0.00101021]\n",
      " [0.10573055 0.00924365 0.9604686 ]\n",
      " [0.17858559 0.8651638  0.02315792]\n",
      " [0.9127341  0.08263215 0.15018357]\n",
      " [0.9376526  0.04767548 0.06251058]\n",
      " [0.07930149 0.7891816  0.08059123]\n",
      " [0.98940134 0.02332347 0.02156803]\n",
      " [0.01016359 0.96774715 0.03148519]\n",
      " [0.6191921  0.5618034  0.0378476 ]\n",
      " [0.9967184  0.00512356 0.08085269]\n",
      " [0.01137274 0.8876582  0.09902211]\n",
      " [0.00451825 0.73175347 0.35370365]\n",
      " [0.04415454 0.72181493 0.25124744]\n",
      " [0.0044357  0.96125376 0.21821173]]\n"
     ]
    }
   ],
   "source": [
    "# Train the multi-class neural network using Tensorflow on the wine dataset\n",
    "mnn_tf = ml.AutoencoderTF(8, 3)\n",
    "mnn_tf.train(train_x, train_y)\n",
    "mnn_tf_pred = mnn_tf.test(test_x)\n",
    "print(mnn_tf_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:36:16.423707Z",
     "start_time": "2023-10-12T22:36:04.399734Z"
    }
   },
   "id": "61245605bd0b5f91"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Neural Network Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Multi-class Neural Network using Tensorflow Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Test Labels: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the multi-class neural network, test labels, and the multi-class neural network using Tensorflow\n",
    "mnn_tf_pred_idx = np.argmax(mnn_tf_pred, axis=1)\n",
    "mnn_tf_pred_idx = np.eye(3)[mnn_tf_pred_idx]\n",
    "\n",
    "print(\"Multi-class Neural Network Output: \\n\" + str(mnm_pred_idx))\n",
    "print(\"Multi-class Neural Network using Tensorflow Output: \\n\" + str(mnn_tf_pred_idx))\n",
    "print(\"Test Labels: \\n\" + str(test_y_idx))\n",
    "\n",
    "accuracy = np.sum(mnn_tf_pred_idx == test_y_idx) / len(test_y_idx) / 3\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T22:36:16.429737Z",
     "start_time": "2023-10-12T22:36:16.407330Z"
    }
   },
   "id": "f958f869991e4d0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
