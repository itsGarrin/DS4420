{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:55.181329Z",
     "start_time": "2023-10-11T19:43:53.192606Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import MLutils as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "y = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 1]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:55.185010Z",
     "start_time": "2023-10-11T19:43:55.183220Z"
    }
   },
   "id": "8880148f815df2a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d09354f5fcbb3c74"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Output: \n",
      "[[9.08115078e-01 7.09610185e-07 1.16293328e-01 7.01495813e-02\n",
      "  3.93638181e-02 1.65797439e-06 1.60868369e-02 1.77630937e-02]\n",
      " [1.13894317e-05 9.20512072e-01 6.38728122e-02 8.52532933e-02\n",
      "  6.43902604e-06 2.50793659e-02 4.60507760e-02 1.05289259e-01]\n",
      " [1.23776316e-01 4.08939013e-02 2.46465208e-01 2.37123002e-01\n",
      "  4.26129509e-02 1.95246854e-02 1.19209304e-01 1.91804629e-01]\n",
      " [7.74564025e-02 6.44805278e-02 2.42682485e-01 2.47635393e-01\n",
      "  6.45384553e-02 5.15664820e-02 7.54216473e-02 2.26537711e-01]\n",
      " [4.54912478e-02 7.09495030e-06 6.64894946e-02 8.61900148e-02\n",
      "  9.21801045e-01 2.73083102e-02 9.70406744e-06 9.96117722e-02]\n",
      " [1.27875172e-07 1.70648773e-02 1.07602095e-02 3.21640403e-02\n",
      "  1.96058976e-02 9.06019743e-01 9.20851897e-08 1.37303237e-01]\n",
      " [1.65715575e-02 3.92808137e-02 1.14028928e-01 7.08617318e-02\n",
      "  7.78322827e-07 1.82141044e-06 9.10442413e-01 1.93577570e-02]\n",
      " [1.54834779e-02 8.37873788e-02 1.87088975e-01 2.17605792e-01\n",
      "  7.73785128e-02 1.62333066e-01 1.53909623e-02 2.46148189e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Implement the Neural Network\n",
    "nn = ml.NeuralNetwork()\n",
    "nn.train(X, y)\n",
    "nn_pred = nn.predict()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:55.200147Z",
     "start_time": "2023-10-11T19:43:55.185819Z"
    }
   },
   "id": "1becf6db31023bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b2d0f6dd3250344"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 15:43:55.233807: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "Input Data: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Predicted data based on trained weights: [[9.8388857e-01 3.6956730e-16 9.2944531e-03 1.0628666e-02 3.6805080e-08\n",
      "  2.5238989e-10 1.4210517e-25 3.7951982e-11]\n",
      " [2.2983993e-14 9.8031718e-01 1.4410327e-08 1.5373686e-16 8.9398425e-05\n",
      "  1.3856635e-02 1.0654442e-02 5.0627403e-08]\n",
      " [2.0602651e-02 2.2442707e-04 9.7618687e-01 9.7372949e-06 1.7013070e-03\n",
      "  2.2542845e-02 1.1782032e-10 1.0496455e-07]\n",
      " [1.3570023e-02 4.8551175e-13 4.0078954e-10 9.8376608e-01 1.3112938e-02\n",
      "  2.0293558e-14 3.2849758e-12 1.0184794e-02]\n",
      " [1.7103937e-03 2.1587593e-02 1.7924411e-02 2.8746622e-02 9.3513995e-01\n",
      "  1.7098295e-04 2.5414912e-02 3.7331387e-02]\n",
      " [2.5693152e-12 1.1546965e-02 1.1157674e-02 1.8387488e-22 1.2242703e-11\n",
      "  9.8376554e-01 1.9669536e-17 9.7396522e-18]\n",
      " [7.8878659e-17 1.1749091e-02 3.1597714e-18 1.3065435e-12 8.9739626e-03\n",
      "  6.6822090e-11 9.8662376e-01 1.1565626e-02]\n",
      " [2.3995125e-14 2.4129808e-12 4.7377535e-26 1.1330275e-04 1.8543271e-02\n",
      "  7.6182920e-22 1.2627057e-03 9.8228353e-01]]\n",
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create an Autoencoder using Tensorflow\n",
    "autoencoder = ml.Autoencoder(8, 3)\n",
    "autoencoder.train(X, y)\n",
    "ae_pred = autoencoder.test(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:59.625882Z",
     "start_time": "2023-10-11T19:43:55.200737Z"
    }
   },
   "id": "9f2f59b0676efa32"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Output: \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Autoencoder Output: \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the Autoencoder and the Neural Network using mean squared error (MSE)\n",
    "\n",
    "# use argmax to select the only maximum as 1 and convert the results to one-hot labels\n",
    "nn_pred_idx = np.argmax(nn_pred, axis=1)\n",
    "nn_pred_idx = np.eye(8)[nn_pred_idx]\n",
    "\n",
    "ae_pred_idx = np.argmax(ae_pred, axis=1)\n",
    "ae_pred_idx = np.eye(8)[ae_pred_idx]\n",
    "\n",
    "print(\"Neural Network Output: \\n\" + str(nn_pred_idx))\n",
    "print(\"Autoencoder Output: \\n\" + str(ae_pred_idx))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:59.643701Z",
     "start_time": "2023-10-11T19:43:59.626929Z"
    }
   },
   "id": "735da874ef6b998d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc899744cf7484b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "     1  14.23  1.71  2.43  15.6  127   2.8  3.06   .28  2.29  5.64  1.04  \\\n0    2  11.65  1.67  2.62  26.0   88  1.92  1.61  0.40  1.34  2.60  1.36   \n1    2  11.41  0.74  2.50  21.0   88  2.48  2.01  0.42  1.44  3.08  1.10   \n2    3  13.78  2.76  2.30  22.0   90  1.35  0.68  0.41  1.03  9.58  0.70   \n3    1  13.30  1.72  2.14  17.0   94  2.40  2.19  0.27  1.35  3.95  1.02   \n4    1  13.29  1.97  2.68  16.8  102  3.00  3.23  0.31  1.66  6.00  1.07   \n..  ..    ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   \n145  2  12.51  1.73  1.98  20.5   85  2.20  1.92  0.32  1.48  2.94  1.04   \n146  2  13.34  0.94  2.36  17.0  110  2.53  1.30  0.55  0.42  3.17  1.02   \n147  2  12.77  3.43  1.98  16.0   80  1.63  1.25  0.43  0.83  3.40  0.70   \n148  1  14.10  2.02  2.40  18.8  103  2.75  2.92  0.32  2.38  6.20  1.07   \n149  3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06  7.70  0.64   \n\n     3.92  1065  \n0    3.21   562  \n1    2.31   434  \n2    1.68   615  \n3    2.77  1285  \n4    2.84  1270  \n..    ...   ...  \n145  3.57   672  \n146  1.93   750  \n147  2.12   372  \n148  2.75  1060  \n149  1.74   740  \n\n[150 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>11.65</td>\n      <td>1.67</td>\n      <td>2.62</td>\n      <td>26.0</td>\n      <td>88</td>\n      <td>1.92</td>\n      <td>1.61</td>\n      <td>0.40</td>\n      <td>1.34</td>\n      <td>2.60</td>\n      <td>1.36</td>\n      <td>3.21</td>\n      <td>562</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>11.41</td>\n      <td>0.74</td>\n      <td>2.50</td>\n      <td>21.0</td>\n      <td>88</td>\n      <td>2.48</td>\n      <td>2.01</td>\n      <td>0.42</td>\n      <td>1.44</td>\n      <td>3.08</td>\n      <td>1.10</td>\n      <td>2.31</td>\n      <td>434</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>13.78</td>\n      <td>2.76</td>\n      <td>2.30</td>\n      <td>22.0</td>\n      <td>90</td>\n      <td>1.35</td>\n      <td>0.68</td>\n      <td>0.41</td>\n      <td>1.03</td>\n      <td>9.58</td>\n      <td>0.70</td>\n      <td>1.68</td>\n      <td>615</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>13.30</td>\n      <td>1.72</td>\n      <td>2.14</td>\n      <td>17.0</td>\n      <td>94</td>\n      <td>2.40</td>\n      <td>2.19</td>\n      <td>0.27</td>\n      <td>1.35</td>\n      <td>3.95</td>\n      <td>1.02</td>\n      <td>2.77</td>\n      <td>1285</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.29</td>\n      <td>1.97</td>\n      <td>2.68</td>\n      <td>16.8</td>\n      <td>102</td>\n      <td>3.00</td>\n      <td>3.23</td>\n      <td>0.31</td>\n      <td>1.66</td>\n      <td>6.00</td>\n      <td>1.07</td>\n      <td>2.84</td>\n      <td>1270</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2</td>\n      <td>12.51</td>\n      <td>1.73</td>\n      <td>1.98</td>\n      <td>20.5</td>\n      <td>85</td>\n      <td>2.20</td>\n      <td>1.92</td>\n      <td>0.32</td>\n      <td>1.48</td>\n      <td>2.94</td>\n      <td>1.04</td>\n      <td>3.57</td>\n      <td>672</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>2</td>\n      <td>13.34</td>\n      <td>0.94</td>\n      <td>2.36</td>\n      <td>17.0</td>\n      <td>110</td>\n      <td>2.53</td>\n      <td>1.30</td>\n      <td>0.55</td>\n      <td>0.42</td>\n      <td>3.17</td>\n      <td>1.02</td>\n      <td>1.93</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2</td>\n      <td>12.77</td>\n      <td>3.43</td>\n      <td>1.98</td>\n      <td>16.0</td>\n      <td>80</td>\n      <td>1.63</td>\n      <td>1.25</td>\n      <td>0.43</td>\n      <td>0.83</td>\n      <td>3.40</td>\n      <td>0.70</td>\n      <td>2.12</td>\n      <td>372</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>1</td>\n      <td>14.10</td>\n      <td>2.02</td>\n      <td>2.40</td>\n      <td>18.8</td>\n      <td>103</td>\n      <td>2.75</td>\n      <td>2.92</td>\n      <td>0.32</td>\n      <td>2.38</td>\n      <td>6.20</td>\n      <td>1.07</td>\n      <td>2.75</td>\n      <td>1060</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>3</td>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    1  14.23  1.71  2.43  15.6  127   2.8  3.06   .28  2.29   5.64  1.04  \\\n0   1  13.56  1.73  2.46  20.5  116  2.96  2.78  0.20  2.45   6.25  0.98   \n1   3  12.79  2.67  2.48  22.0  112  1.48  1.36  0.24  1.26  10.80  0.48   \n2   2  12.17  1.45  2.53  19.0  104  1.89  1.75  0.45  1.03   2.95  1.45   \n3   1  14.22  1.70  2.30  16.3  118  3.20  3.00  0.26  2.03   6.38  0.94   \n4   2  11.87  4.31  2.39  21.0   82  2.86  3.03  0.21  2.91   2.80  0.75   \n5   2  12.42  4.43  2.73  26.5  102  2.20  2.13  0.43  1.71   2.08  0.92   \n6   1  14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98   5.20  1.08   \n7   3  12.36  3.83  2.38  21.0   88  2.30  0.92  0.50  1.04   7.65  0.56   \n8   2  12.72  1.75  2.28  22.5   84  1.38  1.76  0.48  1.63   3.30  0.88   \n9   2  12.00  1.51  2.42  22.0   86  1.45  1.25  0.50  1.63   3.60  1.05   \n10  3  13.45  3.70  2.60  23.0  111  1.70  0.92  0.43  1.46  10.68  0.85   \n11  3  13.88  5.04  2.23  20.0   80  0.98  0.34  0.40  0.68   4.90  0.58   \n12  1  14.39  1.87  2.45  14.6   96  2.50  2.52  0.30  1.98   5.25  1.02   \n13  2  12.29  3.17  2.21  18.0   88  2.85  2.99  0.45  2.81   2.30  1.42   \n14  3  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54   8.66  0.74   \n15  2  12.72  1.81  2.20  18.8   86  2.20  2.53  0.26  1.77   3.90  1.16   \n16  1  13.51  1.80  2.65  19.0  110  2.35  2.53  0.29  1.54   4.20  1.10   \n17  1  13.64  3.10  2.56  15.2  116  2.70  3.03  0.17  1.66   5.10  0.96   \n18  2  13.67  1.25  1.92  18.0   94  2.10  1.79  0.32  0.73   3.80  1.23   \n19  1  13.82  1.75  2.42  14.0  111  3.88  3.74  0.32  1.87   7.05  1.01   \n20  2  12.37  1.17  1.92  19.6   78  2.11  2.00  0.27  1.04   4.68  1.12   \n21  1  13.07  1.50  2.10  15.5   98  2.40  2.64  0.28  1.37   3.70  1.18   \n22  1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97   6.75  1.05   \n23  2  12.42  2.55  2.27  22.0   90  1.68  1.84  0.66  1.42   2.70  0.86   \n24  2  12.64  1.36  2.02  16.8  100  2.02  1.41  0.53  0.62   5.75  0.98   \n25  2  11.76  2.68  2.92  20.0  103  1.75  2.03  0.60  1.05   3.80  1.23   \n26  2  11.79  2.13  2.78  28.5   92  2.13  2.24  0.58  1.76   3.00  0.97   \n\n    3.92  1065  \n0   3.03  1120  \n1   1.47   480  \n2   2.23   355  \n3   3.31   970  \n4   3.64   380  \n5   3.12   365  \n6   2.85  1045  \n7   1.58   520  \n8   2.42   488  \n9   2.65   450  \n10  1.56   695  \n11  1.33   415  \n12  3.58  1290  \n13  2.83   406  \n14  1.80   750  \n15  3.14   714  \n16  2.87  1095  \n17  3.36   845  \n18  2.46   630  \n19  3.26  1190  \n20  3.48   510  \n21  2.69  1020  \n22  2.85  1450  \n23  3.30   315  \n24  1.59   450  \n25  2.50   607  \n26  2.44   466  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>13.56</td>\n      <td>1.73</td>\n      <td>2.46</td>\n      <td>20.5</td>\n      <td>116</td>\n      <td>2.96</td>\n      <td>2.78</td>\n      <td>0.20</td>\n      <td>2.45</td>\n      <td>6.25</td>\n      <td>0.98</td>\n      <td>3.03</td>\n      <td>1120</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>12.79</td>\n      <td>2.67</td>\n      <td>2.48</td>\n      <td>22.0</td>\n      <td>112</td>\n      <td>1.48</td>\n      <td>1.36</td>\n      <td>0.24</td>\n      <td>1.26</td>\n      <td>10.80</td>\n      <td>0.48</td>\n      <td>1.47</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>12.17</td>\n      <td>1.45</td>\n      <td>2.53</td>\n      <td>19.0</td>\n      <td>104</td>\n      <td>1.89</td>\n      <td>1.75</td>\n      <td>0.45</td>\n      <td>1.03</td>\n      <td>2.95</td>\n      <td>1.45</td>\n      <td>2.23</td>\n      <td>355</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.22</td>\n      <td>1.70</td>\n      <td>2.30</td>\n      <td>16.3</td>\n      <td>118</td>\n      <td>3.20</td>\n      <td>3.00</td>\n      <td>0.26</td>\n      <td>2.03</td>\n      <td>6.38</td>\n      <td>0.94</td>\n      <td>3.31</td>\n      <td>970</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>11.87</td>\n      <td>4.31</td>\n      <td>2.39</td>\n      <td>21.0</td>\n      <td>82</td>\n      <td>2.86</td>\n      <td>3.03</td>\n      <td>0.21</td>\n      <td>2.91</td>\n      <td>2.80</td>\n      <td>0.75</td>\n      <td>3.64</td>\n      <td>380</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>12.42</td>\n      <td>4.43</td>\n      <td>2.73</td>\n      <td>26.5</td>\n      <td>102</td>\n      <td>2.20</td>\n      <td>2.13</td>\n      <td>0.43</td>\n      <td>1.71</td>\n      <td>2.08</td>\n      <td>0.92</td>\n      <td>3.12</td>\n      <td>365</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>14.83</td>\n      <td>1.64</td>\n      <td>2.17</td>\n      <td>14.0</td>\n      <td>97</td>\n      <td>2.80</td>\n      <td>2.98</td>\n      <td>0.29</td>\n      <td>1.98</td>\n      <td>5.20</td>\n      <td>1.08</td>\n      <td>2.85</td>\n      <td>1045</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>12.36</td>\n      <td>3.83</td>\n      <td>2.38</td>\n      <td>21.0</td>\n      <td>88</td>\n      <td>2.30</td>\n      <td>0.92</td>\n      <td>0.50</td>\n      <td>1.04</td>\n      <td>7.65</td>\n      <td>0.56</td>\n      <td>1.58</td>\n      <td>520</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>12.72</td>\n      <td>1.75</td>\n      <td>2.28</td>\n      <td>22.5</td>\n      <td>84</td>\n      <td>1.38</td>\n      <td>1.76</td>\n      <td>0.48</td>\n      <td>1.63</td>\n      <td>3.30</td>\n      <td>0.88</td>\n      <td>2.42</td>\n      <td>488</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>12.00</td>\n      <td>1.51</td>\n      <td>2.42</td>\n      <td>22.0</td>\n      <td>86</td>\n      <td>1.45</td>\n      <td>1.25</td>\n      <td>0.50</td>\n      <td>1.63</td>\n      <td>3.60</td>\n      <td>1.05</td>\n      <td>2.65</td>\n      <td>450</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>13.45</td>\n      <td>3.70</td>\n      <td>2.60</td>\n      <td>23.0</td>\n      <td>111</td>\n      <td>1.70</td>\n      <td>0.92</td>\n      <td>0.43</td>\n      <td>1.46</td>\n      <td>10.68</td>\n      <td>0.85</td>\n      <td>1.56</td>\n      <td>695</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>13.88</td>\n      <td>5.04</td>\n      <td>2.23</td>\n      <td>20.0</td>\n      <td>80</td>\n      <td>0.98</td>\n      <td>0.34</td>\n      <td>0.40</td>\n      <td>0.68</td>\n      <td>4.90</td>\n      <td>0.58</td>\n      <td>1.33</td>\n      <td>415</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>14.39</td>\n      <td>1.87</td>\n      <td>2.45</td>\n      <td>14.6</td>\n      <td>96</td>\n      <td>2.50</td>\n      <td>2.52</td>\n      <td>0.30</td>\n      <td>1.98</td>\n      <td>5.25</td>\n      <td>1.02</td>\n      <td>3.58</td>\n      <td>1290</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>12.29</td>\n      <td>3.17</td>\n      <td>2.21</td>\n      <td>18.0</td>\n      <td>88</td>\n      <td>2.85</td>\n      <td>2.99</td>\n      <td>0.45</td>\n      <td>2.81</td>\n      <td>2.30</td>\n      <td>1.42</td>\n      <td>2.83</td>\n      <td>406</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>13.58</td>\n      <td>2.58</td>\n      <td>2.69</td>\n      <td>24.5</td>\n      <td>105</td>\n      <td>1.55</td>\n      <td>0.84</td>\n      <td>0.39</td>\n      <td>1.54</td>\n      <td>8.66</td>\n      <td>0.74</td>\n      <td>1.80</td>\n      <td>750</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n      <td>12.72</td>\n      <td>1.81</td>\n      <td>2.20</td>\n      <td>18.8</td>\n      <td>86</td>\n      <td>2.20</td>\n      <td>2.53</td>\n      <td>0.26</td>\n      <td>1.77</td>\n      <td>3.90</td>\n      <td>1.16</td>\n      <td>3.14</td>\n      <td>714</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>13.51</td>\n      <td>1.80</td>\n      <td>2.65</td>\n      <td>19.0</td>\n      <td>110</td>\n      <td>2.35</td>\n      <td>2.53</td>\n      <td>0.29</td>\n      <td>1.54</td>\n      <td>4.20</td>\n      <td>1.10</td>\n      <td>2.87</td>\n      <td>1095</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>13.64</td>\n      <td>3.10</td>\n      <td>2.56</td>\n      <td>15.2</td>\n      <td>116</td>\n      <td>2.70</td>\n      <td>3.03</td>\n      <td>0.17</td>\n      <td>1.66</td>\n      <td>5.10</td>\n      <td>0.96</td>\n      <td>3.36</td>\n      <td>845</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>13.67</td>\n      <td>1.25</td>\n      <td>1.92</td>\n      <td>18.0</td>\n      <td>94</td>\n      <td>2.10</td>\n      <td>1.79</td>\n      <td>0.32</td>\n      <td>0.73</td>\n      <td>3.80</td>\n      <td>1.23</td>\n      <td>2.46</td>\n      <td>630</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>13.82</td>\n      <td>1.75</td>\n      <td>2.42</td>\n      <td>14.0</td>\n      <td>111</td>\n      <td>3.88</td>\n      <td>3.74</td>\n      <td>0.32</td>\n      <td>1.87</td>\n      <td>7.05</td>\n      <td>1.01</td>\n      <td>3.26</td>\n      <td>1190</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2</td>\n      <td>12.37</td>\n      <td>1.17</td>\n      <td>1.92</td>\n      <td>19.6</td>\n      <td>78</td>\n      <td>2.11</td>\n      <td>2.00</td>\n      <td>0.27</td>\n      <td>1.04</td>\n      <td>4.68</td>\n      <td>1.12</td>\n      <td>3.48</td>\n      <td>510</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n      <td>13.07</td>\n      <td>1.50</td>\n      <td>2.10</td>\n      <td>15.5</td>\n      <td>98</td>\n      <td>2.40</td>\n      <td>2.64</td>\n      <td>0.28</td>\n      <td>1.37</td>\n      <td>3.70</td>\n      <td>1.18</td>\n      <td>2.69</td>\n      <td>1020</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1</td>\n      <td>14.20</td>\n      <td>1.76</td>\n      <td>2.45</td>\n      <td>15.2</td>\n      <td>112</td>\n      <td>3.27</td>\n      <td>3.39</td>\n      <td>0.34</td>\n      <td>1.97</td>\n      <td>6.75</td>\n      <td>1.05</td>\n      <td>2.85</td>\n      <td>1450</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n      <td>12.42</td>\n      <td>2.55</td>\n      <td>2.27</td>\n      <td>22.0</td>\n      <td>90</td>\n      <td>1.68</td>\n      <td>1.84</td>\n      <td>0.66</td>\n      <td>1.42</td>\n      <td>2.70</td>\n      <td>0.86</td>\n      <td>3.30</td>\n      <td>315</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2</td>\n      <td>12.64</td>\n      <td>1.36</td>\n      <td>2.02</td>\n      <td>16.8</td>\n      <td>100</td>\n      <td>2.02</td>\n      <td>1.41</td>\n      <td>0.53</td>\n      <td>0.62</td>\n      <td>5.75</td>\n      <td>0.98</td>\n      <td>1.59</td>\n      <td>450</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2</td>\n      <td>11.76</td>\n      <td>2.68</td>\n      <td>2.92</td>\n      <td>20.0</td>\n      <td>103</td>\n      <td>1.75</td>\n      <td>2.03</td>\n      <td>0.60</td>\n      <td>1.05</td>\n      <td>3.80</td>\n      <td>1.23</td>\n      <td>2.50</td>\n      <td>607</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2</td>\n      <td>11.79</td>\n      <td>2.13</td>\n      <td>2.78</td>\n      <td>28.5</td>\n      <td>92</td>\n      <td>2.13</td>\n      <td>2.24</td>\n      <td>0.58</td>\n      <td>1.76</td>\n      <td>3.00</td>\n      <td>0.97</td>\n      <td>2.44</td>\n      <td>466</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the wine dataset\n",
    "train_wine = pd.read_csv('train_wine.csv')\n",
    "test_wine = pd.read_csv('test_wine.csv')\n",
    "\n",
    "display(train_wine)\n",
    "display(test_wine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:59.649321Z",
     "start_time": "2023-10-11T19:43:59.629291Z"
    }
   },
   "id": "37196ef4493105a9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "     1     14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n0    2 -1.670125 -0.586254  0.923595  1.981185 -0.810781 -0.612897 -0.384151   \n1    2 -1.970387 -1.408850  0.494238  0.454064 -0.810781  0.289686  0.007108   \n2    3  0.994702  0.377863 -0.221358  0.759488 -0.673205 -1.531598 -1.293826   \n3    1  0.394178 -0.542029 -0.793834 -0.767633 -0.398053  0.160746  0.183174   \n4    1  0.381667 -0.320901  1.138274 -0.828718  0.152251  1.127799  1.200446   \n..  ..       ...       ...       ...       ...       ...       ...       ...   \n145  2 -0.594185 -0.533184 -1.366310  0.301352 -1.017145 -0.161605 -0.080925   \n146  2  0.444221 -1.231948 -0.006679 -0.767633  0.702554  0.370274 -0.687376   \n147  2 -0.268901  0.970486 -1.366310 -1.073057 -1.361085 -1.080306 -0.736283   \n148  1  1.395051 -0.276675  0.136440 -0.217869  0.221039  0.724860  0.897221   \n149  3  0.907125  2.934102  0.315339  0.301352 -0.329265 -0.999718 -1.362297   \n\n          .28      2.29      5.64      1.04      3.92      1065  \n0    0.324537 -0.430475 -1.052111  1.781424  0.864654 -0.608422  \n1    0.486267 -0.255722 -0.845966  0.645535 -0.405789 -1.017432  \n2    0.405402 -0.972209  1.945577 -1.101987 -1.295098 -0.439067  \n3   -0.726705 -0.413000 -0.472329  0.296030  0.243549  1.701840  \n4   -0.403246  0.128735  0.408081  0.514470  0.342361  1.653909  \n..        ...       ...       ...       ...       ...       ...  \n145 -0.322381 -0.185821 -0.906092  0.383406  1.372831 -0.256930  \n146  1.537509 -2.038203 -0.807314  0.296030 -0.942198 -0.007690  \n147  0.567132 -1.321715 -0.708537 -1.101987 -0.673993 -1.215545  \n148 -0.322381  1.386956  0.493974  0.514470  0.215316  0.982879  \n149  1.294914 -0.919783  1.138177 -1.364116 -1.210402 -0.039644  \n\n[150 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>-1.670125</td>\n      <td>-0.586254</td>\n      <td>0.923595</td>\n      <td>1.981185</td>\n      <td>-0.810781</td>\n      <td>-0.612897</td>\n      <td>-0.384151</td>\n      <td>0.324537</td>\n      <td>-0.430475</td>\n      <td>-1.052111</td>\n      <td>1.781424</td>\n      <td>0.864654</td>\n      <td>-0.608422</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>-1.970387</td>\n      <td>-1.408850</td>\n      <td>0.494238</td>\n      <td>0.454064</td>\n      <td>-0.810781</td>\n      <td>0.289686</td>\n      <td>0.007108</td>\n      <td>0.486267</td>\n      <td>-0.255722</td>\n      <td>-0.845966</td>\n      <td>0.645535</td>\n      <td>-0.405789</td>\n      <td>-1.017432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0.994702</td>\n      <td>0.377863</td>\n      <td>-0.221358</td>\n      <td>0.759488</td>\n      <td>-0.673205</td>\n      <td>-1.531598</td>\n      <td>-1.293826</td>\n      <td>0.405402</td>\n      <td>-0.972209</td>\n      <td>1.945577</td>\n      <td>-1.101987</td>\n      <td>-1.295098</td>\n      <td>-0.439067</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.394178</td>\n      <td>-0.542029</td>\n      <td>-0.793834</td>\n      <td>-0.767633</td>\n      <td>-0.398053</td>\n      <td>0.160746</td>\n      <td>0.183174</td>\n      <td>-0.726705</td>\n      <td>-0.413000</td>\n      <td>-0.472329</td>\n      <td>0.296030</td>\n      <td>0.243549</td>\n      <td>1.701840</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.381667</td>\n      <td>-0.320901</td>\n      <td>1.138274</td>\n      <td>-0.828718</td>\n      <td>0.152251</td>\n      <td>1.127799</td>\n      <td>1.200446</td>\n      <td>-0.403246</td>\n      <td>0.128735</td>\n      <td>0.408081</td>\n      <td>0.514470</td>\n      <td>0.342361</td>\n      <td>1.653909</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>2</td>\n      <td>-0.594185</td>\n      <td>-0.533184</td>\n      <td>-1.366310</td>\n      <td>0.301352</td>\n      <td>-1.017145</td>\n      <td>-0.161605</td>\n      <td>-0.080925</td>\n      <td>-0.322381</td>\n      <td>-0.185821</td>\n      <td>-0.906092</td>\n      <td>0.383406</td>\n      <td>1.372831</td>\n      <td>-0.256930</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>2</td>\n      <td>0.444221</td>\n      <td>-1.231948</td>\n      <td>-0.006679</td>\n      <td>-0.767633</td>\n      <td>0.702554</td>\n      <td>0.370274</td>\n      <td>-0.687376</td>\n      <td>1.537509</td>\n      <td>-2.038203</td>\n      <td>-0.807314</td>\n      <td>0.296030</td>\n      <td>-0.942198</td>\n      <td>-0.007690</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>2</td>\n      <td>-0.268901</td>\n      <td>0.970486</td>\n      <td>-1.366310</td>\n      <td>-1.073057</td>\n      <td>-1.361085</td>\n      <td>-1.080306</td>\n      <td>-0.736283</td>\n      <td>0.567132</td>\n      <td>-1.321715</td>\n      <td>-0.708537</td>\n      <td>-1.101987</td>\n      <td>-0.673993</td>\n      <td>-1.215545</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>1</td>\n      <td>1.395051</td>\n      <td>-0.276675</td>\n      <td>0.136440</td>\n      <td>-0.217869</td>\n      <td>0.221039</td>\n      <td>0.724860</td>\n      <td>0.897221</td>\n      <td>-0.322381</td>\n      <td>1.386956</td>\n      <td>0.493974</td>\n      <td>0.514470</td>\n      <td>0.215316</td>\n      <td>0.982879</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>3</td>\n      <td>0.907125</td>\n      <td>2.934102</td>\n      <td>0.315339</td>\n      <td>0.301352</td>\n      <td>-0.329265</td>\n      <td>-0.999718</td>\n      <td>-1.362297</td>\n      <td>1.294914</td>\n      <td>-0.919783</td>\n      <td>1.138177</td>\n      <td>-1.364116</td>\n      <td>-1.210402</td>\n      <td>-0.039644</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    1     14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n0   1  0.719462 -0.533184  0.351119  0.301352  1.115282  1.063329  0.760280   \n1   3 -0.243880  0.298257  0.422678  0.759488  0.840130 -1.322069 -0.628687   \n2   2 -1.019557 -0.780847  0.601577 -0.156784  0.289827 -0.661250 -0.247210   \n3   1  1.545182 -0.559719 -0.221358 -0.981430  1.252858  1.450150  0.975472   \n4   2 -1.394885  1.748856  0.100660  0.454064 -1.223509  0.902153  1.004817   \n5   2 -0.706784  1.854998  1.317173  2.133897  0.152251 -0.161605  0.124485   \n6   1  2.308349 -0.612790 -0.686495 -1.683906 -0.191689  0.805448  0.955909   \n7   3 -0.781849  1.324291  0.064881  0.454064 -0.810781 -0.000430 -1.059071   \n8   2 -0.331456 -0.515493 -0.292917  0.912200 -1.085933 -1.483245 -0.237429   \n9   2 -1.232243 -0.727776  0.208000  0.759488 -0.948357 -1.370422 -0.736283   \n10  3  0.581841  1.209304  0.852036  1.064913  0.771342 -0.967483 -1.059071   \n11  3  1.119811  2.394550 -0.471816  0.148640 -1.361085 -2.127947 -1.626396   \n12  1  1.757868 -0.409352  0.315339 -1.500651 -0.260477  0.321921  0.505962   \n13  2 -0.869426  0.740513 -0.543375 -0.462209 -0.810781  0.886036  0.965691   \n14  3  0.744483  0.218651  1.174054  1.523049  0.358615 -1.209247 -1.137323   \n15  2 -0.331456 -0.462423 -0.579155 -0.217869 -0.948357 -0.161605  0.515744   \n16  1  0.656907 -0.471268  1.030935 -0.156784  0.702554  0.080158  0.515744   \n17  1  0.819549  0.678597  0.708917 -1.317397  1.115282  0.644272  1.004817   \n18  2  0.857082 -0.957749 -1.580989 -0.462209 -0.398053 -0.322781 -0.208084   \n19  1  1.044746 -0.515493  0.208000 -1.683906  0.771342  2.546144  1.699300   \n20  2 -0.769338 -1.028510 -1.580989  0.026470 -1.498661 -0.306663 -0.002674   \n21  1  0.106426 -0.736621 -0.936953 -1.225769 -0.122901  0.160746  0.623340   \n22  1  1.520161 -0.506648  0.315339 -1.317397  0.840130  1.562973  1.356949   \n23  2 -0.706784  0.192116 -0.328697  0.759488 -0.673205 -0.999718 -0.159177   \n24  2 -0.431543 -0.860453 -1.223191 -0.828718  0.014675 -0.451721 -0.579780   \n25  2 -1.532505  0.307102  1.996988  0.148640  0.221039 -0.886895  0.026671   \n26  2 -1.494972 -0.179379  1.496072  2.744746 -0.535629 -0.274428  0.232081   \n\n         .28      2.29      5.64      1.04      3.92      1065  \n0  -1.292758  1.509284  0.515448  0.121278  0.610565  1.174602  \n1  -0.969299 -0.570277  2.469528 -2.063124 -1.591535 -0.870444  \n2   0.728861 -0.972209 -0.901797  2.174616 -0.518717 -1.269867  \n3  -0.807569  0.775321  0.571279 -0.053474  1.005814  0.695294  \n4  -1.211893  2.313147 -0.966218 -0.883547  1.471643 -1.189982  \n5   0.567132  0.216111 -1.275435 -0.140850  0.737609 -1.237913  \n6  -0.564975  0.687944  0.064506  0.558158  0.356477  0.934948  \n7   1.133185 -0.954734  1.116703 -1.713620 -1.436259 -0.742629  \n8   0.971455  0.076309 -0.751483 -0.315602 -0.250512 -0.844881  \n9   1.133185  0.076309 -0.622643  0.427094  0.074156 -0.966305  \n10  0.567132 -0.220771  2.417992 -0.446667 -1.464491 -0.183436  \n11  0.324537 -1.583845 -0.064334 -1.626244 -1.789159 -1.078144  \n12 -0.484110  0.687944  0.085980  0.296030  1.386947  1.717817  \n13  0.728861  2.138394 -1.180952  2.043552  0.328245 -1.106902  \n14  0.243672 -0.080969  1.550466 -0.927235 -1.125706 -0.007690  \n15 -0.807569  0.320963 -0.493803  0.907663  0.765842 -0.122724  \n16 -0.564975 -0.080969 -0.364962  0.645535  0.384709  1.094717  \n17 -1.535352  0.128735  0.021559  0.033902  1.076394  0.295871  \n18 -0.322381 -1.496468 -0.536749  1.213479 -0.194048 -0.391136  \n19 -0.322381  0.495716  0.859022  0.252342  0.935234  1.398279  \n20 -0.726705 -0.954734 -0.158817  0.732911  1.245786 -0.774582  \n21 -0.645840 -0.378049 -0.579696  0.995039  0.130620  0.855063  \n22 -0.160651  0.670469  0.730182  0.427094  0.356477  2.229079  \n23  2.427021 -0.290673 -1.009164 -0.402979  0.991698 -1.397682  \n24  1.375779 -1.688697  0.300714  0.121278 -1.422143 -0.966305  \n25  1.941833 -0.937259 -0.536749  1.213479 -0.137584 -0.464630  \n26  1.780103  0.303488 -0.880324  0.077590 -0.222280 -0.915179  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>14.23</th>\n      <th>1.71</th>\n      <th>2.43</th>\n      <th>15.6</th>\n      <th>127</th>\n      <th>2.8</th>\n      <th>3.06</th>\n      <th>.28</th>\n      <th>2.29</th>\n      <th>5.64</th>\n      <th>1.04</th>\n      <th>3.92</th>\n      <th>1065</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.719462</td>\n      <td>-0.533184</td>\n      <td>0.351119</td>\n      <td>0.301352</td>\n      <td>1.115282</td>\n      <td>1.063329</td>\n      <td>0.760280</td>\n      <td>-1.292758</td>\n      <td>1.509284</td>\n      <td>0.515448</td>\n      <td>0.121278</td>\n      <td>0.610565</td>\n      <td>1.174602</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>-0.243880</td>\n      <td>0.298257</td>\n      <td>0.422678</td>\n      <td>0.759488</td>\n      <td>0.840130</td>\n      <td>-1.322069</td>\n      <td>-0.628687</td>\n      <td>-0.969299</td>\n      <td>-0.570277</td>\n      <td>2.469528</td>\n      <td>-2.063124</td>\n      <td>-1.591535</td>\n      <td>-0.870444</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-1.019557</td>\n      <td>-0.780847</td>\n      <td>0.601577</td>\n      <td>-0.156784</td>\n      <td>0.289827</td>\n      <td>-0.661250</td>\n      <td>-0.247210</td>\n      <td>0.728861</td>\n      <td>-0.972209</td>\n      <td>-0.901797</td>\n      <td>2.174616</td>\n      <td>-0.518717</td>\n      <td>-1.269867</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1.545182</td>\n      <td>-0.559719</td>\n      <td>-0.221358</td>\n      <td>-0.981430</td>\n      <td>1.252858</td>\n      <td>1.450150</td>\n      <td>0.975472</td>\n      <td>-0.807569</td>\n      <td>0.775321</td>\n      <td>0.571279</td>\n      <td>-0.053474</td>\n      <td>1.005814</td>\n      <td>0.695294</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>-1.394885</td>\n      <td>1.748856</td>\n      <td>0.100660</td>\n      <td>0.454064</td>\n      <td>-1.223509</td>\n      <td>0.902153</td>\n      <td>1.004817</td>\n      <td>-1.211893</td>\n      <td>2.313147</td>\n      <td>-0.966218</td>\n      <td>-0.883547</td>\n      <td>1.471643</td>\n      <td>-1.189982</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>-0.706784</td>\n      <td>1.854998</td>\n      <td>1.317173</td>\n      <td>2.133897</td>\n      <td>0.152251</td>\n      <td>-0.161605</td>\n      <td>0.124485</td>\n      <td>0.567132</td>\n      <td>0.216111</td>\n      <td>-1.275435</td>\n      <td>-0.140850</td>\n      <td>0.737609</td>\n      <td>-1.237913</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>2.308349</td>\n      <td>-0.612790</td>\n      <td>-0.686495</td>\n      <td>-1.683906</td>\n      <td>-0.191689</td>\n      <td>0.805448</td>\n      <td>0.955909</td>\n      <td>-0.564975</td>\n      <td>0.687944</td>\n      <td>0.064506</td>\n      <td>0.558158</td>\n      <td>0.356477</td>\n      <td>0.934948</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>-0.781849</td>\n      <td>1.324291</td>\n      <td>0.064881</td>\n      <td>0.454064</td>\n      <td>-0.810781</td>\n      <td>-0.000430</td>\n      <td>-1.059071</td>\n      <td>1.133185</td>\n      <td>-0.954734</td>\n      <td>1.116703</td>\n      <td>-1.713620</td>\n      <td>-1.436259</td>\n      <td>-0.742629</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>-0.331456</td>\n      <td>-0.515493</td>\n      <td>-0.292917</td>\n      <td>0.912200</td>\n      <td>-1.085933</td>\n      <td>-1.483245</td>\n      <td>-0.237429</td>\n      <td>0.971455</td>\n      <td>0.076309</td>\n      <td>-0.751483</td>\n      <td>-0.315602</td>\n      <td>-0.250512</td>\n      <td>-0.844881</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>-1.232243</td>\n      <td>-0.727776</td>\n      <td>0.208000</td>\n      <td>0.759488</td>\n      <td>-0.948357</td>\n      <td>-1.370422</td>\n      <td>-0.736283</td>\n      <td>1.133185</td>\n      <td>0.076309</td>\n      <td>-0.622643</td>\n      <td>0.427094</td>\n      <td>0.074156</td>\n      <td>-0.966305</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3</td>\n      <td>0.581841</td>\n      <td>1.209304</td>\n      <td>0.852036</td>\n      <td>1.064913</td>\n      <td>0.771342</td>\n      <td>-0.967483</td>\n      <td>-1.059071</td>\n      <td>0.567132</td>\n      <td>-0.220771</td>\n      <td>2.417992</td>\n      <td>-0.446667</td>\n      <td>-1.464491</td>\n      <td>-0.183436</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3</td>\n      <td>1.119811</td>\n      <td>2.394550</td>\n      <td>-0.471816</td>\n      <td>0.148640</td>\n      <td>-1.361085</td>\n      <td>-2.127947</td>\n      <td>-1.626396</td>\n      <td>0.324537</td>\n      <td>-1.583845</td>\n      <td>-0.064334</td>\n      <td>-1.626244</td>\n      <td>-1.789159</td>\n      <td>-1.078144</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1</td>\n      <td>1.757868</td>\n      <td>-0.409352</td>\n      <td>0.315339</td>\n      <td>-1.500651</td>\n      <td>-0.260477</td>\n      <td>0.321921</td>\n      <td>0.505962</td>\n      <td>-0.484110</td>\n      <td>0.687944</td>\n      <td>0.085980</td>\n      <td>0.296030</td>\n      <td>1.386947</td>\n      <td>1.717817</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2</td>\n      <td>-0.869426</td>\n      <td>0.740513</td>\n      <td>-0.543375</td>\n      <td>-0.462209</td>\n      <td>-0.810781</td>\n      <td>0.886036</td>\n      <td>0.965691</td>\n      <td>0.728861</td>\n      <td>2.138394</td>\n      <td>-1.180952</td>\n      <td>2.043552</td>\n      <td>0.328245</td>\n      <td>-1.106902</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3</td>\n      <td>0.744483</td>\n      <td>0.218651</td>\n      <td>1.174054</td>\n      <td>1.523049</td>\n      <td>0.358615</td>\n      <td>-1.209247</td>\n      <td>-1.137323</td>\n      <td>0.243672</td>\n      <td>-0.080969</td>\n      <td>1.550466</td>\n      <td>-0.927235</td>\n      <td>-1.125706</td>\n      <td>-0.007690</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2</td>\n      <td>-0.331456</td>\n      <td>-0.462423</td>\n      <td>-0.579155</td>\n      <td>-0.217869</td>\n      <td>-0.948357</td>\n      <td>-0.161605</td>\n      <td>0.515744</td>\n      <td>-0.807569</td>\n      <td>0.320963</td>\n      <td>-0.493803</td>\n      <td>0.907663</td>\n      <td>0.765842</td>\n      <td>-0.122724</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1</td>\n      <td>0.656907</td>\n      <td>-0.471268</td>\n      <td>1.030935</td>\n      <td>-0.156784</td>\n      <td>0.702554</td>\n      <td>0.080158</td>\n      <td>0.515744</td>\n      <td>-0.564975</td>\n      <td>-0.080969</td>\n      <td>-0.364962</td>\n      <td>0.645535</td>\n      <td>0.384709</td>\n      <td>1.094717</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>0.819549</td>\n      <td>0.678597</td>\n      <td>0.708917</td>\n      <td>-1.317397</td>\n      <td>1.115282</td>\n      <td>0.644272</td>\n      <td>1.004817</td>\n      <td>-1.535352</td>\n      <td>0.128735</td>\n      <td>0.021559</td>\n      <td>0.033902</td>\n      <td>1.076394</td>\n      <td>0.295871</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>0.857082</td>\n      <td>-0.957749</td>\n      <td>-1.580989</td>\n      <td>-0.462209</td>\n      <td>-0.398053</td>\n      <td>-0.322781</td>\n      <td>-0.208084</td>\n      <td>-0.322381</td>\n      <td>-1.496468</td>\n      <td>-0.536749</td>\n      <td>1.213479</td>\n      <td>-0.194048</td>\n      <td>-0.391136</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>1.044746</td>\n      <td>-0.515493</td>\n      <td>0.208000</td>\n      <td>-1.683906</td>\n      <td>0.771342</td>\n      <td>2.546144</td>\n      <td>1.699300</td>\n      <td>-0.322381</td>\n      <td>0.495716</td>\n      <td>0.859022</td>\n      <td>0.252342</td>\n      <td>0.935234</td>\n      <td>1.398279</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>2</td>\n      <td>-0.769338</td>\n      <td>-1.028510</td>\n      <td>-1.580989</td>\n      <td>0.026470</td>\n      <td>-1.498661</td>\n      <td>-0.306663</td>\n      <td>-0.002674</td>\n      <td>-0.726705</td>\n      <td>-0.954734</td>\n      <td>-0.158817</td>\n      <td>0.732911</td>\n      <td>1.245786</td>\n      <td>-0.774582</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n      <td>0.106426</td>\n      <td>-0.736621</td>\n      <td>-0.936953</td>\n      <td>-1.225769</td>\n      <td>-0.122901</td>\n      <td>0.160746</td>\n      <td>0.623340</td>\n      <td>-0.645840</td>\n      <td>-0.378049</td>\n      <td>-0.579696</td>\n      <td>0.995039</td>\n      <td>0.130620</td>\n      <td>0.855063</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>1</td>\n      <td>1.520161</td>\n      <td>-0.506648</td>\n      <td>0.315339</td>\n      <td>-1.317397</td>\n      <td>0.840130</td>\n      <td>1.562973</td>\n      <td>1.356949</td>\n      <td>-0.160651</td>\n      <td>0.670469</td>\n      <td>0.730182</td>\n      <td>0.427094</td>\n      <td>0.356477</td>\n      <td>2.229079</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>2</td>\n      <td>-0.706784</td>\n      <td>0.192116</td>\n      <td>-0.328697</td>\n      <td>0.759488</td>\n      <td>-0.673205</td>\n      <td>-0.999718</td>\n      <td>-0.159177</td>\n      <td>2.427021</td>\n      <td>-0.290673</td>\n      <td>-1.009164</td>\n      <td>-0.402979</td>\n      <td>0.991698</td>\n      <td>-1.397682</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>2</td>\n      <td>-0.431543</td>\n      <td>-0.860453</td>\n      <td>-1.223191</td>\n      <td>-0.828718</td>\n      <td>0.014675</td>\n      <td>-0.451721</td>\n      <td>-0.579780</td>\n      <td>1.375779</td>\n      <td>-1.688697</td>\n      <td>0.300714</td>\n      <td>0.121278</td>\n      <td>-1.422143</td>\n      <td>-0.966305</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>2</td>\n      <td>-1.532505</td>\n      <td>0.307102</td>\n      <td>1.996988</td>\n      <td>0.148640</td>\n      <td>0.221039</td>\n      <td>-0.886895</td>\n      <td>0.026671</td>\n      <td>1.941833</td>\n      <td>-0.937259</td>\n      <td>-0.536749</td>\n      <td>1.213479</td>\n      <td>-0.137584</td>\n      <td>-0.464630</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>2</td>\n      <td>-1.494972</td>\n      <td>-0.179379</td>\n      <td>1.496072</td>\n      <td>2.744746</td>\n      <td>-0.535629</td>\n      <td>-0.274428</td>\n      <td>0.232081</td>\n      <td>1.780103</td>\n      <td>0.303488</td>\n      <td>-0.880324</td>\n      <td>0.077590</td>\n      <td>-0.222280</td>\n      <td>-0.915179</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "train_wine.iloc[:, 1:], train_avg, train_stdev = ml.normalize(train_wine.iloc[:, 1:])\n",
    "test_wine.iloc[:, 1:] = (test_wine.iloc[:, 1:] - train_avg) / train_stdev\n",
    "\n",
    "display(train_wine)\n",
    "display(test_wine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:59.663303Z",
     "start_time": "2023-10-11T19:43:59.648630Z"
    }
   },
   "id": "23c8f7420ee23e5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.36989855e-01 2.03118535e-05 5.87906627e-02]\n",
      " [7.57063697e-04 1.34099378e-03 9.98867403e-01]\n",
      " [3.12237050e-03 9.97536587e-01 4.82719356e-04]\n",
      " [9.96475098e-01 6.51057853e-05 8.32817859e-03]\n",
      " [3.40600039e-03 9.55015122e-01 1.80464024e-04]\n",
      " [1.09342940e-04 9.93477232e-01 4.44638559e-03]\n",
      " [9.96675777e-01 8.29264585e-05 6.33843025e-03]\n",
      " [1.90427808e-04 2.97495921e-03 9.95907719e-01]\n",
      " [2.26048909e-04 9.95222132e-01 1.03808487e-02]\n",
      " [4.67747583e-05 9.94091713e-01 3.05558038e-02]\n",
      " [1.18689472e-02 1.77466275e-05 9.95508693e-01]\n",
      " [7.17655655e-04 1.63219187e-03 9.98525343e-01]\n",
      " [9.96729444e-01 3.26085213e-05 8.54832256e-03]\n",
      " [8.38814504e-03 9.93482710e-01 8.25997106e-05]\n",
      " [8.91589930e-03 3.15570388e-05 9.95928429e-01]\n",
      " [4.38886003e-02 9.87532985e-01 2.43877604e-05]\n",
      " [9.95050189e-01 1.08557915e-04 5.98725409e-03]\n",
      " [9.96501442e-01 8.49862725e-05 6.57701382e-03]\n",
      " [9.93115888e-03 9.95631866e-01 1.10441745e-04]\n",
      " [9.96754758e-01 4.29141999e-05 8.05298060e-03]\n",
      " [2.43053833e-03 9.95099283e-01 1.84179336e-04]\n",
      " [8.70794366e-01 5.98401276e-01 2.90435398e-05]\n",
      " [9.96624468e-01 3.14090506e-05 9.39069914e-03]\n",
      " [5.65523605e-04 9.97044850e-01 5.64523799e-03]\n",
      " [5.44545018e-05 9.77188625e-01 1.99888440e-01]\n",
      " [3.22472394e-03 9.95822934e-01 2.34268132e-03]\n",
      " [1.61309696e-05 9.92305417e-01 3.43218858e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Train the multi-class neural network on the wine dataset\n",
    "train_x = train_wine.iloc[:, 1:]\n",
    "train_y = train_wine.iloc[:, 0]\n",
    "\n",
    "# convert the labels to one-hot labels\n",
    "# subtract 1 from the labels to make them 0, 1, 2 instead of 1, 2, 3\n",
    "train_y = train_y - 1\n",
    "train_y = np.eye(3)[train_y]\n",
    "\n",
    "test_x = test_wine.iloc[:, 1:]\n",
    "test_y = test_wine.iloc[:, 0]\n",
    "\n",
    "mnn = ml.MultiNeuralNetwork()\n",
    "mnn.train(train_x, train_y)\n",
    "mnn_pred = mnn.predict(test_x)\n",
    "print(mnn_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:59.812549Z",
     "start_time": "2023-10-11T19:43:59.663083Z"
    }
   },
   "id": "bffdbc58851a8306"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Neural Network Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Test Labels: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the multi-class neural network and the test labels\n",
    "mnm_pred_idx = np.argmax(mnn_pred, axis=1)\n",
    "mnm_pred_idx = np.eye(3)[mnm_pred_idx]\n",
    "\n",
    "test_y_idx = test_y - 1\n",
    "test_y_idx = np.eye(3)[test_y_idx]\n",
    "\n",
    "print(\"Multi-class Neural Network Output: \\n\" + str(mnm_pred_idx))\n",
    "print(\"Test Labels: \\n\" + str(test_y_idx))\n",
    "\n",
    "accuracy = np.sum(mnm_pred_idx == test_y_idx) / len(test_y_idx) / 3\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:43:59.815007Z",
     "start_time": "2023-10-11T19:43:59.812923Z"
    }
   },
   "id": "42b3fb162df31812"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d900294fdf284d79"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Input Data:        14.23      1.71      2.43      15.6       127       2.8      3.06  \\\n",
      "0   0.719462 -0.533184  0.351119  0.301352  1.115282  1.063329  0.760280   \n",
      "1  -0.243880  0.298257  0.422678  0.759488  0.840130 -1.322069 -0.628687   \n",
      "2  -1.019557 -0.780847  0.601577 -0.156784  0.289827 -0.661250 -0.247210   \n",
      "3   1.545182 -0.559719 -0.221358 -0.981430  1.252858  1.450150  0.975472   \n",
      "4  -1.394885  1.748856  0.100660  0.454064 -1.223509  0.902153  1.004817   \n",
      "5  -0.706784  1.854998  1.317173  2.133897  0.152251 -0.161605  0.124485   \n",
      "6   2.308349 -0.612790 -0.686495 -1.683906 -0.191689  0.805448  0.955909   \n",
      "7  -0.781849  1.324291  0.064881  0.454064 -0.810781 -0.000430 -1.059071   \n",
      "8  -0.331456 -0.515493 -0.292917  0.912200 -1.085933 -1.483245 -0.237429   \n",
      "9  -1.232243 -0.727776  0.208000  0.759488 -0.948357 -1.370422 -0.736283   \n",
      "10  0.581841  1.209304  0.852036  1.064913  0.771342 -0.967483 -1.059071   \n",
      "11  1.119811  2.394550 -0.471816  0.148640 -1.361085 -2.127947 -1.626396   \n",
      "12  1.757868 -0.409352  0.315339 -1.500651 -0.260477  0.321921  0.505962   \n",
      "13 -0.869426  0.740513 -0.543375 -0.462209 -0.810781  0.886036  0.965691   \n",
      "14  0.744483  0.218651  1.174054  1.523049  0.358615 -1.209247 -1.137323   \n",
      "15 -0.331456 -0.462423 -0.579155 -0.217869 -0.948357 -0.161605  0.515744   \n",
      "16  0.656907 -0.471268  1.030935 -0.156784  0.702554  0.080158  0.515744   \n",
      "17  0.819549  0.678597  0.708917 -1.317397  1.115282  0.644272  1.004817   \n",
      "18  0.857082 -0.957749 -1.580989 -0.462209 -0.398053 -0.322781 -0.208084   \n",
      "19  1.044746 -0.515493  0.208000 -1.683906  0.771342  2.546144  1.699300   \n",
      "20 -0.769338 -1.028510 -1.580989  0.026470 -1.498661 -0.306663 -0.002674   \n",
      "21  0.106426 -0.736621 -0.936953 -1.225769 -0.122901  0.160746  0.623340   \n",
      "22  1.520161 -0.506648  0.315339 -1.317397  0.840130  1.562973  1.356949   \n",
      "23 -0.706784  0.192116 -0.328697  0.759488 -0.673205 -0.999718 -0.159177   \n",
      "24 -0.431543 -0.860453 -1.223191 -0.828718  0.014675 -0.451721 -0.579780   \n",
      "25 -1.532505  0.307102  1.996988  0.148640  0.221039 -0.886895  0.026671   \n",
      "26 -1.494972 -0.179379  1.496072  2.744746 -0.535629 -0.274428  0.232081   \n",
      "\n",
      "         .28      2.29      5.64      1.04      3.92      1065  \n",
      "0  -1.292758  1.509284  0.515448  0.121278  0.610565  1.174602  \n",
      "1  -0.969299 -0.570277  2.469528 -2.063124 -1.591535 -0.870444  \n",
      "2   0.728861 -0.972209 -0.901797  2.174616 -0.518717 -1.269867  \n",
      "3  -0.807569  0.775321  0.571279 -0.053474  1.005814  0.695294  \n",
      "4  -1.211893  2.313147 -0.966218 -0.883547  1.471643 -1.189982  \n",
      "5   0.567132  0.216111 -1.275435 -0.140850  0.737609 -1.237913  \n",
      "6  -0.564975  0.687944  0.064506  0.558158  0.356477  0.934948  \n",
      "7   1.133185 -0.954734  1.116703 -1.713620 -1.436259 -0.742629  \n",
      "8   0.971455  0.076309 -0.751483 -0.315602 -0.250512 -0.844881  \n",
      "9   1.133185  0.076309 -0.622643  0.427094  0.074156 -0.966305  \n",
      "10  0.567132 -0.220771  2.417992 -0.446667 -1.464491 -0.183436  \n",
      "11  0.324537 -1.583845 -0.064334 -1.626244 -1.789159 -1.078144  \n",
      "12 -0.484110  0.687944  0.085980  0.296030  1.386947  1.717817  \n",
      "13  0.728861  2.138394 -1.180952  2.043552  0.328245 -1.106902  \n",
      "14  0.243672 -0.080969  1.550466 -0.927235 -1.125706 -0.007690  \n",
      "15 -0.807569  0.320963 -0.493803  0.907663  0.765842 -0.122724  \n",
      "16 -0.564975 -0.080969 -0.364962  0.645535  0.384709  1.094717  \n",
      "17 -1.535352  0.128735  0.021559  0.033902  1.076394  0.295871  \n",
      "18 -0.322381 -1.496468 -0.536749  1.213479 -0.194048 -0.391136  \n",
      "19 -0.322381  0.495716  0.859022  0.252342  0.935234  1.398279  \n",
      "20 -0.726705 -0.954734 -0.158817  0.732911  1.245786 -0.774582  \n",
      "21 -0.645840 -0.378049 -0.579696  0.995039  0.130620  0.855063  \n",
      "22 -0.160651  0.670469  0.730182  0.427094  0.356477  2.229079  \n",
      "23  2.427021 -0.290673 -1.009164 -0.402979  0.991698 -1.397682  \n",
      "24  1.375779 -1.688697  0.300714  0.121278 -1.422143 -0.966305  \n",
      "25  1.941833 -0.937259 -0.536749  1.213479 -0.137584 -0.464630  \n",
      "26  1.780103  0.303488 -0.880324  0.077590 -0.222280 -0.915179  \n",
      "Predicted data based on trained weights: [[9.9563938e-01 4.0301614e-02 5.0784512e-03]\n",
      " [1.3775783e-04 2.5490290e-05 1.0000000e+00]\n",
      " [2.5027050e-03 9.9999624e-01 7.5213914e-03]\n",
      " [9.9869663e-01 3.8543036e-03 1.5463895e-02]\n",
      " [2.8219065e-01 9.9979848e-01 1.1030625e-04]\n",
      " [1.9861614e-02 9.9996239e-01 1.5119812e-03]\n",
      " [9.9885595e-01 3.4312485e-03 1.4319010e-02]\n",
      " [9.2020805e-04 1.0732725e-03 9.9999654e-01]\n",
      " [2.9144194e-02 9.9974340e-01 8.8799037e-03]\n",
      " [9.7166719e-03 9.9993414e-01 8.3700260e-03]\n",
      " [3.6534769e-04 8.7559172e-05 9.9999994e-01]\n",
      " [1.9114531e-04 3.6541168e-03 9.9999297e-01]\n",
      " [9.9985462e-01 5.4383144e-04 4.2091995e-03]\n",
      " [1.0527204e-02 9.9999738e-01 1.6823004e-04]\n",
      " [1.0130045e-03 3.1407367e-04 9.9999928e-01]\n",
      " [3.6270231e-01 9.9562001e-01 4.4189533e-03]\n",
      " [9.9743557e-01 7.4844128e-03 2.1170169e-02]\n",
      " [9.9966002e-01 1.1709041e-03 6.8933684e-03]\n",
      " [3.3159453e-02 9.9933255e-01 5.5953082e-02]\n",
      " [9.9986571e-01 5.0664850e-04 4.0150629e-03]\n",
      " [1.7534390e-02 9.9994797e-01 7.6095923e-03]\n",
      " [9.1196287e-01 5.5123371e-01 2.3689218e-02]\n",
      " [9.9985814e-01 3.3890543e-04 7.1134591e-03]\n",
      " [1.8321514e-02 9.9985880e-01 4.3866681e-03]\n",
      " [1.3189204e-02 9.9258041e-01 4.5399699e-01]\n",
      " [4.5537636e-02 9.9694550e-01 3.7504923e-02]\n",
      " [3.7765147e-03 9.9999654e-01 7.2339282e-04]]\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "[[9.9563938e-01 4.0301614e-02 5.0784512e-03]\n",
      " [1.3775783e-04 2.5490290e-05 1.0000000e+00]\n",
      " [2.5027050e-03 9.9999624e-01 7.5213914e-03]\n",
      " [9.9869663e-01 3.8543036e-03 1.5463895e-02]\n",
      " [2.8219065e-01 9.9979848e-01 1.1030625e-04]\n",
      " [1.9861614e-02 9.9996239e-01 1.5119812e-03]\n",
      " [9.9885595e-01 3.4312485e-03 1.4319010e-02]\n",
      " [9.2020805e-04 1.0732725e-03 9.9999654e-01]\n",
      " [2.9144194e-02 9.9974340e-01 8.8799037e-03]\n",
      " [9.7166719e-03 9.9993414e-01 8.3700260e-03]\n",
      " [3.6534769e-04 8.7559172e-05 9.9999994e-01]\n",
      " [1.9114531e-04 3.6541168e-03 9.9999297e-01]\n",
      " [9.9985462e-01 5.4383144e-04 4.2091995e-03]\n",
      " [1.0527204e-02 9.9999738e-01 1.6823004e-04]\n",
      " [1.0130045e-03 3.1407367e-04 9.9999928e-01]\n",
      " [3.6270231e-01 9.9562001e-01 4.4189533e-03]\n",
      " [9.9743557e-01 7.4844128e-03 2.1170169e-02]\n",
      " [9.9966002e-01 1.1709041e-03 6.8933684e-03]\n",
      " [3.3159453e-02 9.9933255e-01 5.5953082e-02]\n",
      " [9.9986571e-01 5.0664850e-04 4.0150629e-03]\n",
      " [1.7534390e-02 9.9994797e-01 7.6095923e-03]\n",
      " [9.1196287e-01 5.5123371e-01 2.3689218e-02]\n",
      " [9.9985814e-01 3.3890543e-04 7.1134591e-03]\n",
      " [1.8321514e-02 9.9985880e-01 4.3866681e-03]\n",
      " [1.3189204e-02 9.9258041e-01 4.5399699e-01]\n",
      " [4.5537636e-02 9.9694550e-01 3.7504923e-02]\n",
      " [3.7765147e-03 9.9999654e-01 7.2339282e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Train the multi-class neural network using Tensorflow on the wine dataset\n",
    "train_x = train_wine.iloc[:, 1:]\n",
    "train_y = train_wine.iloc[:, 0]\n",
    "\n",
    "train_y = train_y - 1\n",
    "\n",
    "mnn_tf = ml.MultiNeuralNetworkTF()\n",
    "mnn_tf.train(train_x, train_y)\n",
    "mnn_tf_pred = mnn_tf.predict(test_x)\n",
    "print(mnn_tf_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:44:01.620417Z",
     "start_time": "2023-10-11T19:43:59.814950Z"
    }
   },
   "id": "61245605bd0b5f91"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class Neural Network Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Multi-class Neural Network using Tensorflow Output: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Test Labels: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of the multi-class neural network, test labels, and the multi-class neural network using Tensorflow\n",
    "mnn_tf_pred_idx = np.argmax(mnn_tf_pred, axis=1)\n",
    "mnn_tf_pred_idx = np.eye(3)[mnn_tf_pred_idx]\n",
    "\n",
    "print(\"Multi-class Neural Network Output: \\n\" + str(mnm_pred_idx))\n",
    "print(\"Multi-class Neural Network using Tensorflow Output: \\n\" + str(mnn_tf_pred_idx))\n",
    "print(\"Test Labels: \\n\" + str(test_y_idx))\n",
    "\n",
    "accuracy = np.sum(mnn_tf_pred_idx == test_y_idx) / len(test_y_idx) / 3\n",
    "print(\"Accuracy: \" + str(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T19:44:20.294277Z",
     "start_time": "2023-10-11T19:44:20.289389Z"
    }
   },
   "id": "f958f869991e4d0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
